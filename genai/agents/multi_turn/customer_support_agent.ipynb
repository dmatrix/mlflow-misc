{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge Tutorial: Multi-Turn Conversation Evaluation with MLflow 3.7\n",
    "\n",
    "This interactive notebook demonstrates how to use MLflow 3.7's session-level evaluation features to assess multi-turn customer support conversations.\n",
    "\n",
    "## Tutorial Goals\n",
    "\n",
    "1. Track multi-turn conversations with session IDs using MLflow tracing\n",
    "2. Create session-level judges with `{{ conversation }}` template\n",
    "3. Evaluate conversation quality (coherence and context retention)\n",
    "4. Demonstrate MLflow 3.7's new session tracking features\n",
    "\n",
    "## Scenario\n",
    "\n",
    "A customer support agent handles multi-turn conversations. Session-level judges evaluate:\n",
    "- **Coherence**: Does the conversation flow logically across turns?\n",
    "- **Context Retention**: Does the agent remember information from earlier messages?\n",
    "\n",
    "**Key MLflow 3.7 Feature:**\n",
    "\n",
    "The `{{ conversation }}` template variable in judge instructions automatically makes judges session-level aware. MLflow aggregates all traces with the same session ID into a single conversation view for evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "Based on the [customer_support_agent.py](customer_support_agent.py) CLI pattern, adapted for interactive notebook tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Dependencies\n",
    "\n",
    "First, let's import all necessary libraries and load environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai.common.config import AgentConfig\n",
    "from genai.agents.multi_turn.customer_support_agent_cls import CustomerSupportAgent\n",
    "from genai.agents.multi_turn.scenarios import (\n",
    "    get_scenario_printer_troubleshooting,\n",
    "    get_scenario_account_access\n",
    ")\n",
    "from genai.agents.multi_turn.prompts import (\n",
    "    get_coherence_judge_instructions,\n",
    "    get_context_retention_judge_instructions,\n",
    "    map_retention_to_score\n",
    ")\n",
    "from genai.common.mlflow_config import setup_mlflow_tracking\n",
    "import mlflow\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables from .env file if it exists\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        load_dotenv(env_file)\n",
    "        print(f\"âœ“ Loaded environment variables from {env_file.absolute()}\")\n",
    "    else:\n",
    "        print(f\"â„¹ï¸  No .env file found at {env_file.absolute()}\")\n",
    "        print(\"   You can create one with your credentials or set environment variables manually\")\n",
    "except ImportError:\n",
    "    print(\"â„¹ï¸  python-dotenv not installed. Install with: pip install python-dotenv\")\n",
    "    print(\"   Or set environment variables manually in the cells below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration: Set Up Environment\n",
    "\n",
    "**Two ways to configure credentials:**\n",
    "\n",
    "1. **Recommended**: Create a `.env` file in this directory with your credentials\n",
    "2. **Alternative**: Uncomment and set credentials in the cells below\n",
    "\n",
    "### Create a `.env` file (Recommended)\n",
    "\n",
    "Create a file named `.env` in the same directory as this notebook:\n",
    "\n",
    "**For Databricks:**\n",
    "```\n",
    "DATABRICKS_TOKEN=your-token-here\n",
    "DATABRICKS_HOST=https://your-workspace.cloud.databricks.com\n",
    "```\n",
    "\n",
    "**For OpenAI:**\n",
    "```\n",
    "OPENAI_API_KEY=sk-your-key-here\n",
    "```\n",
    "\n",
    "The cell above will automatically load these credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION: Choose your provider and models\n",
    "# ============================================================================\n",
    "\n",
    "# Option 1: Databricks (default)\n",
    "PROVIDER = \"databricks\"\n",
    "AGENT_MODEL = \"databricks-gpt-5\"\n",
    "JUDGE_MODEL = \"databricks-gemini-2-5-flash\"\n",
    "\n",
    "# Option 2: OpenAI (uncomment to use)\n",
    "# PROVIDER = \"openai\"\n",
    "# AGENT_MODEL = \"gpt-4o-mini\"\n",
    "# JUDGE_MODEL = \"gpt-4o\"\n",
    "\n",
    "# Other settings\n",
    "TEMPERATURE = 1.0\n",
    "EXPERIMENT_NAME = \"customer-support-agent-notebook\"\n",
    "\n",
    "print(\"âœ“ Configuration set:\")\n",
    "print(f\"  Provider: {PROVIDER}\")\n",
    "print(f\"  Agent Model: {AGENT_MODEL}\")\n",
    "print(f\"  Judge Model: {JUDGE_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Credentials (Optional Manual Setup)\n",
    "\n",
    "If you didn't create a `.env` file, you can set credentials manually by uncommenting the appropriate lines below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MANUAL CREDENTIAL SETUP (if not using .env file)\n",
    "# ============================================================================\n",
    "\n",
    "# For Databricks - Uncomment and set these if you didn't create a .env file\n",
    "# os.environ[\"DATABRICKS_TOKEN\"] = \"your-token-here\"\n",
    "# os.environ[\"DATABRICKS_HOST\"] = \"https://your-workspace.cloud.databricks.com\"\n",
    "\n",
    "# For OpenAI - Uncomment and set this if you didn't create a .env file\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-your-key-here\"\n",
    "\n",
    "# Verify credentials are set\n",
    "if PROVIDER == \"databricks\":\n",
    "    if \"DATABRICKS_TOKEN\" in os.environ and \"DATABRICKS_HOST\" in os.environ:\n",
    "        print(\"âœ“ Databricks credentials found\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Missing Databricks credentials!\")\n",
    "        print(\"   Create a .env file or uncomment the lines above to set credentials\")\n",
    "elif PROVIDER == \"openai\":\n",
    "    if \"OPENAI_API_KEY\" in os.environ:\n",
    "        print(\"âœ“ OpenAI credentials found\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Missing OpenAI credentials!\")\n",
    "        print(\"   Create a .env file or uncomment the lines above to set credentials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup MLflow Tracing\n",
    "\n",
    "Enable MLflow tracing to capture all customer support conversations and evaluation results automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_mlflow_tracking(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    enable_autolog=True\n",
    ")\n",
    "\n",
    "print(\"âœ“ MLflow tracing enabled\")\n",
    "print(f\"  Experiment: {EXPERIMENT_NAME}\")\n",
    "print(\"  View traces: mlflow ui\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import the Agent Class\n",
    "\n",
    "Instead of redefining the class, we import it directly from the module to follow DRY principles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomerSupportAgent already imported at top of notebook\n",
    "print(\"âœ“ CustomerSupportAgent imported successfully\")\n",
    "print(\"\\nThe class provides:\")\n",
    "print(\"  - handle_message(): Process single message with session tracking\")\n",
    "print(\"  - run_conversation(): Execute complete multi-turn conversation\")\n",
    "print(\"  - evaluate_session(): Judge conversation quality using MLflow 3.7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 (continued): Initialize Agent and Judges\n",
    "\n",
    "Create the configuration and instantiate our customer support agent with session-level judges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent configuration\n",
    "config = AgentConfig(\n",
    "    model=AGENT_MODEL,\n",
    "    provider=PROVIDER,\n",
    "    temperature=TEMPERATURE,\n",
    "    mlflow_experiment=EXPERIMENT_NAME\n",
    ")\n",
    "\n",
    "# Initialize the agent with judges\n",
    "agent = CustomerSupportAgent(config, judge_model=JUDGE_MODEL, debug=False)\n",
    "\n",
    "print(\"âœ“ Agent and Judges initialized\")\n",
    "print(f\"  Provider: {config.provider}\")\n",
    "print(f\"  Agent Model: {config.model}\")\n",
    "print(f\"  Judge Model: {JUDGE_MODEL}\")\n",
    "print(f\"  Temperature: {config.temperature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Session Tracking\n",
    "\n",
    "Before running scenarios, let's understand how MLflow 3.7's session tracking works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Turn Session Tracking Flow\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    MULTI-TURN SESSION TRACKING FLOW                      â”‚\n",
    "â”‚                          (MLflow 3.7 Features)                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  CONVERSATION SESSION: \"session-printer-001\"                            â”‚\n",
    "â”‚  (Single logical conversation = Multiple MLflow traces)                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                    â”‚\n",
    "                                    â–¼\n",
    "        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "        â•‘  TURN 1: User sends first message                     â•‘\n",
    "        â•‘  \"My HP LaserJet 3000 won't turn on\"                  â•‘\n",
    "        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                                    â”‚\n",
    "                                    â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  @mlflow.trace() decorator creates Trace #1          â”‚\n",
    "        â”‚  trace_id: tr-abc123...                               â”‚\n",
    "        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "        â”‚  mlflow.update_current_trace(                         â”‚\n",
    "        â”‚      metadata={                                       â”‚\n",
    "        â”‚          \"mlflow.trace.session\": \"session-printer-001\"â”‚\n",
    "        â”‚      }                                                 â”‚\n",
    "        â”‚  )                                                     â”‚\n",
    "        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "        â”‚  TRACE DATA:                                          â”‚\n",
    "        â”‚    - Input: \"My HP LaserJet 3000 won't turn on\"       â”‚\n",
    "        â”‚    - Output: \"Let me help. Is power cable connected?\" â”‚\n",
    "        â”‚    - Session Tag: \"session-printer-001\" âœ“             â”‚\n",
    "        â”‚    - Timestamp: 2025-12-07 10:00:01                   â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                    â”‚\n",
    "                                    â–¼\n",
    "        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "        â•‘  TURN 2: User responds                                â•‘\n",
    "        â•‘  \"Yes, power cable is plugged in securely\"            â•‘\n",
    "        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                                    â”‚\n",
    "                                    â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  @mlflow.trace() creates NEW Trace #2                â”‚\n",
    "        â”‚  trace_id: tr-def456...                               â”‚\n",
    "        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "        â”‚  mlflow.update_current_trace(                         â”‚\n",
    "        â”‚      metadata={                                       â”‚\n",
    "        â”‚          \"mlflow.trace.session\": \"session-printer-001\"â”‚\n",
    "        â”‚      }                                                 â”‚\n",
    "        â”‚  )                                                     â”‚\n",
    "        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "        â”‚  TRACE DATA:                                          â”‚\n",
    "        â”‚    - Input: \"Yes, power cable is plugged in...\"       â”‚\n",
    "        â”‚    - Output: \"Try different outlet in another room\"   â”‚\n",
    "        â”‚    - Session Tag: \"session-printer-001\" âœ“             â”‚\n",
    "        â”‚    - Timestamp: 2025-12-07 10:00:45                   â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                    â”‚\n",
    "                                    â–¼\n",
    "        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "        â•‘  TURN 3: User tries solution                          â•‘\n",
    "        â•‘  \"Tried different outlet, still nothing\"              â•‘\n",
    "        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                                    â”‚\n",
    "                                    â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  @mlflow.trace() creates NEW Trace #3                â”‚\n",
    "        â”‚  trace_id: tr-ghi789...                               â”‚\n",
    "        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "        â”‚  mlflow.update_current_trace(                         â”‚\n",
    "        â”‚      metadata={                                       â”‚\n",
    "        â”‚          \"mlflow.trace.session\": \"session-printer-001\"â”‚\n",
    "        â”‚      }                                                 â”‚\n",
    "        â”‚  )                                                     â”‚\n",
    "        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "        â”‚  TRACE DATA:                                          â”‚\n",
    "        â”‚    - Input: \"Tried different outlet, still nothing\"   â”‚\n",
    "        â”‚    - Output: \"Contact HP for warranty replacement\"    â”‚\n",
    "        â”‚    - Session Tag: \"session-printer-001\" âœ“             â”‚\n",
    "        â”‚    - Timestamp: 2025-12-07 10:02:10                   â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                    â”‚\n",
    "                                    â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      SESSION-LEVEL EVALUATION                            â”‚\n",
    "â”‚                      (MLflow 3.7 Key Feature)                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                    â”‚\n",
    "                                    â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  STEP 1: Search for all traces in session            â”‚\n",
    "        â”‚                                                       â”‚\n",
    "        â”‚  mlflow.search_traces(                                â”‚\n",
    "        â”‚      experiment_ids=[exp_id],                         â”‚\n",
    "        â”‚      filter_string=\"run_id = 'run-xyz'\"               â”‚\n",
    "        â”‚  )                                                     â”‚\n",
    "        â”‚                                                       â”‚\n",
    "        â”‚  FINDS: [Trace #1, Trace #2, Trace #3]               â”‚\n",
    "        â”‚  All tagged with \"session-printer-001\"                â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                    â”‚\n",
    "                                    â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  STEP 2: Aggregate traces with {{ conversation }}    â”‚\n",
    "        â”‚                                                       â”‚\n",
    "        â”‚  Judge instructions contain:                          â”‚\n",
    "        â”‚  \"Evaluate this conversation: {{ conversation }}\"     â”‚\n",
    "        â”‚                                                       â”‚\n",
    "        â”‚  MLflow replaces {{ conversation }} with:             â”‚\n",
    "        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "        â”‚  â”‚ User: My HP LaserJet 3000 won't turn on       â”‚  â”‚\n",
    "        â”‚  â”‚ Assistant: Let me help. Is power cable...     â”‚  â”‚\n",
    "        â”‚  â”‚ User: Yes, power cable is plugged in...       â”‚  â”‚\n",
    "        â”‚  â”‚ Assistant: Try different outlet in another... â”‚  â”‚\n",
    "        â”‚  â”‚ User: Tried different outlet, still nothing   â”‚  â”‚\n",
    "        â”‚  â”‚ Assistant: Contact HP for warranty...         â”‚  â”‚\n",
    "        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                    â”‚\n",
    "                                    â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  STEP 3: Session-Level Judges Evaluate                â”‚\n",
    "        â”‚                                                       â”‚\n",
    "        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "        â”‚  â”‚ COHERENCE JUDGE (Boolean)                       â”‚ â”‚\n",
    "        â”‚  â”‚ - Reads entire conversation                     â”‚ â”‚\n",
    "        â”‚  â”‚ - Checks logical flow across turns              â”‚ â”‚\n",
    "        â”‚  â”‚ - Output: True (coherent)                       â”‚ â”‚\n",
    "        â”‚  â”‚ - Rationale: \"Conversation flows naturally...\"  â”‚ â”‚\n",
    "        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "        â”‚                                                       â”‚\n",
    "        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "        â”‚  â”‚ CONTEXT RETENTION JUDGE (Categorical)           â”‚ â”‚\n",
    "        â”‚  â”‚ - Reads entire conversation                     â”‚ â”‚\n",
    "        â”‚  â”‚ - Checks if agent remembers context             â”‚ â”‚\n",
    "        â”‚  â”‚ - Output: \"excellent\"                           â”‚ â”‚\n",
    "        â”‚  â”‚ - Rationale: \"Agent perfectly recalls HP...\"    â”‚ â”‚\n",
    "        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                    â”‚\n",
    "                                    â–¼\n",
    "        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "        â•‘  FINAL RESULTS                                        â•‘\n",
    "        â•‘  âœ“ Coherence: PASS (True)                            â•‘\n",
    "        â•‘  âœ“ Context Retention: EXCELLENT (4/4)                â•‘\n",
    "        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                           KEY DIFFERENCES                                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "  TURN-LEVEL EVALUATION          â”‚  SESSION-LEVEL EVALUATION\n",
    "  (Traditional approach)          â”‚  (MLflow 3.7 with {{ conversation }})\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  1 trace = 1 evaluation          â”‚  N traces = 1 evaluation\n",
    "  Evaluates individual responses  â”‚  Evaluates entire conversation\n",
    "  No context between turns        â”‚  Full conversation context\n",
    "  Can't assess coherence          â”‚  Can assess flow and coherence\n",
    "  Can't assess retention          â”‚  Can assess memory/retention\n",
    "  3 traces â†’ 3 judge calls        â”‚  3 traces â†’ 1 judge call (aggregated)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: First Scenario - Printer Troubleshooting\n",
    "\n",
    "Load the printer troubleshooting scenario from the pre-defined scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first scenario\n",
    "scenario1 = get_scenario_printer_troubleshooting()\n",
    "\n",
    "print(f\"Scenario: {scenario1['name']}\")\n",
    "print(f\"Description: {scenario1['description']}\")\n",
    "print(f\"Session ID: {scenario1['session_id']}\")\n",
    "print(f\"Number of turns: {len(scenario1['messages'])}\")\n",
    "print(f\"\\nMessages:\")\n",
    "for i, msg in enumerate(scenario1['messages'], 1):\n",
    "    print(f\"  Turn {i}: {msg[:60]}...\" if len(msg) > 60 else f\"  Turn {i}: {msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Multi-Turn Conversation\n",
    "\n",
    "Execute the conversation and see the agent respond to each turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLflow run for this conversation\n",
    "with mlflow.start_run(run_name=scenario1['name']) as run:\n",
    "    # Run the conversation\n",
    "    conv_result1 = agent.run_conversation(\n",
    "        messages=scenario1['messages'],\n",
    "        session_id=scenario1['session_id']\n",
    "    )\n",
    "    \n",
    "    # Store run ID for evaluation\n",
    "    run_id_1 = run.info.run_id\n",
    "    \n",
    "print(f\"\\nâœ“ Conversation completed with {conv_result1['turns']} turns\")\n",
    "print(f\"  Session ID: {conv_result1['session_id']}\")\n",
    "print(f\"  Run ID: {run_id_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate Session with Session-Level Judges\n",
    "\n",
    "Now evaluate the entire conversation using MLflow 3.7's session-level evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Evaluating session '{scenario1['session_id']}'...\\n\")\n",
    "\n",
    "try:\n",
    "    eval_result1 = agent.evaluate_session(scenario1['session_id'], run_id_1)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Evaluation Results: {scenario1['name']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Coherence results\n",
    "    coherence_symbol = \"âœ“\" if eval_result1['coherence']['passed'] else \"âœ—\"\n",
    "    coherence_label = \"PASS\" if eval_result1['coherence']['passed'] else \"FAIL\"\n",
    "    print(f\"\\nðŸ“Š Coherence: {coherence_symbol} {coherence_label}\")\n",
    "    print(f\"   Value: {eval_result1['coherence']['feedback_value']}\")\n",
    "    print(f\"   Rationale: {eval_result1['coherence']['rationale']}\")\n",
    "    \n",
    "    # Context retention results\n",
    "    retention_value = str(eval_result1['context_retention']['feedback_value']).upper()\n",
    "    retention_score = eval_result1['context_retention']['score']\n",
    "    print(f\"\\nðŸ§  Context Retention: {retention_value}\")\n",
    "    print(f\"   Score: {retention_score}/4\")\n",
    "    print(f\"   Rationale: {eval_result1['context_retention']['rationale']}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâœ— Evaluation failed: {e}\")\n",
    "    print(f\"Error details: {type(e).__name__}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nEnsure MLflow >= 3.7.0 for {{ conversation }} template support.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Second Scenario - Account Access\n",
    "\n",
    "Run the account access scenario to compare evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load second scenario\n",
    "scenario2 = get_scenario_account_access()\n",
    "\n",
    "print(f\"{'â”€'*70}\")\n",
    "print(f\"Scenario: {scenario2['name']}\")\n",
    "print(f\"Description: {scenario2['description']}\")\n",
    "print(f\"Session ID: {scenario2['session_id']}\")\n",
    "print(f\"{'â”€'*70}\\n\")\n",
    "\n",
    "# Start MLflow run for this conversation\n",
    "with mlflow.start_run(run_name=scenario2['name']) as run:\n",
    "    # Run conversation\n",
    "    conv_result2 = agent.run_conversation(\n",
    "        messages=scenario2['messages'],\n",
    "        session_id=scenario2['session_id']\n",
    "    )\n",
    "    \n",
    "    run_id_2 = run.info.run_id\n",
    "    \n",
    "    print(f\"\\nEvaluating session '{scenario2['session_id']}'...\\n\")\n",
    "    \n",
    "    try:\n",
    "        eval_result2 = agent.evaluate_session(scenario2['session_id'], run_id_2)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Evaluation Results: {scenario2['name']}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Coherence results\n",
    "        coherence_symbol = \"âœ“\" if eval_result2['coherence']['passed'] else \"âœ—\"\n",
    "        coherence_label = \"PASS\" if eval_result2['coherence']['passed'] else \"FAIL\"\n",
    "        print(f\"\\nðŸ“Š Coherence: {coherence_symbol} {coherence_label}\")\n",
    "        print(f\"   Value: {eval_result2['coherence']['feedback_value']}\")\n",
    "        print(f\"   Rationale: {eval_result2['coherence']['rationale']}\")\n",
    "        \n",
    "        # Context retention results\n",
    "        retention_value = str(eval_result2['context_retention']['feedback_value']).upper()\n",
    "        retention_score = eval_result2['context_retention']['score']\n",
    "        print(f\"\\nðŸ§  Context Retention: {retention_value}\")\n",
    "        print(f\"   Score: {retention_score}/4\")\n",
    "        print(f\"   Rationale: {eval_result2['context_retention']['rationale']}\")\n",
    "        \n",
    "        print(f\"\\n{'='*70}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâœ— Evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug Mode: Understanding DataFrame Structure\n",
    "\n",
    "Enable debug mode to see MLflow's internal DataFrame structure and how evaluation results are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize agent with debug mode\n",
    "agent_debug = CustomerSupportAgent(config, judge_model=JUDGE_MODEL, debug=True)\n",
    "\n",
    "print(\"âœ“ Agent re-initialized with debug=True\")\n",
    "print(\"\\nRe-running evaluation of Scenario 1 with debug output...\\n\")\n",
    "\n",
    "try:\n",
    "    eval_result_debug = agent_debug.evaluate_session(scenario1['session_id'], run_id_1)\n",
    "    print(\"\\nâœ“ Debug evaluation completed\")\n",
    "    print(\"\\nThe debug output above shows:\")\n",
    "    print(\"  - Available DataFrame columns\")\n",
    "    print(\"  - DataFrame shape (rows x columns)\")\n",
    "    print(\"  - Coherence and context retention column names\")\n",
    "    print(\"  - Raw values extracted from judges\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâœ— Debug evaluation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Scenarios: Billing Inquiry\n",
    "\n",
    "Let's test with a billing support conversation that should demonstrate excellent context retention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define billing inquiry scenario\n",
    "scenario_billing = {\n",
    "    \"name\": \"Billing Inquiry\",\n",
    "    \"session_id\": \"session-billing-003\",\n",
    "    \"description\": \"Customer questions unexpected charges, agent investigates and resolves\",\n",
    "    \"expected_coherence\": True,\n",
    "    \"expected_retention\": \"excellent\",\n",
    "    \"messages\": [\n",
    "        \"I just got charged $29.99 on my account but I don't recognize this charge. Can you help?\",\n",
    "        \"The charge date was December 1st. I checked and I didn't make any purchases that day.\",\n",
    "        \"Oh yes! I forgot about the premium subscription. That makes sense. But I thought it was only $19.99?\",\n",
    "        \"Okay, I see it now in my account settings. Thanks for clarifying. I was worried it was fraud!\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"{'â”€'*70}\")\n",
    "print(f\"Scenario: {scenario_billing['name']}\")\n",
    "print(f\"Description: {scenario_billing['description']}\")\n",
    "print(f\"Expected Retention: {scenario_billing['expected_retention']}\")\n",
    "print(f\"{'â”€'*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run billing scenario\n",
    "with mlflow.start_run(run_name=scenario_billing['name']) as run:\n",
    "    conv_result_billing = agent.run_conversation(\n",
    "        messages=scenario_billing['messages'],\n",
    "        session_id=scenario_billing['session_id']\n",
    "    )\n",
    "    \n",
    "    run_id_billing = run.info.run_id\n",
    "    \n",
    "    print(f\"\\nEvaluating session '{scenario_billing['session_id']}'...\\n\")\n",
    "    \n",
    "    try:\n",
    "        eval_result_billing = agent.evaluate_session(scenario_billing['session_id'], run_id_billing)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Evaluation Results: {scenario_billing['name']}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        coherence_symbol = \"âœ“\" if eval_result_billing['coherence']['passed'] else \"âœ—\"\n",
    "        coherence_label = \"PASS\" if eval_result_billing['coherence']['passed'] else \"FAIL\"\n",
    "        print(f\"\\nðŸ“Š Coherence: {coherence_symbol} {coherence_label}\")\n",
    "        print(f\"   Value: {eval_result_billing['coherence']['feedback_value']}\")\n",
    "        print(f\"   Rationale: {eval_result_billing['coherence']['rationale']}\")\n",
    "        \n",
    "        retention_value = str(eval_result_billing['context_retention']['feedback_value']).upper()\n",
    "        retention_score = eval_result_billing['context_retention']['score']\n",
    "        print(f\"\\nðŸ§  Context Retention: {retention_value}\")\n",
    "        print(f\"   Score: {retention_score}/4\")\n",
    "        print(f\"   Rationale: {eval_result_billing['context_retention']['rationale']}\")\n",
    "        \n",
    "        print(f\"\\n{'='*70}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâœ— Evaluation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Scenarios: Shipping Delay\n",
    "\n",
    "Test with a shipping support conversation that demonstrates good (but not perfect) context retention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define shipping delay scenario\n",
    "scenario_shipping = {\n",
    "    \"name\": \"Shipping Delay\",\n",
    "    \"session_id\": \"session-shipping-004\",\n",
    "    \"description\": \"Customer tracking delayed package, agent provides updates and resolution\",\n",
    "    \"expected_coherence\": True,\n",
    "    \"expected_retention\": \"good\",\n",
    "    \"messages\": [\n",
    "        \"My order #12345 was supposed to arrive yesterday but it still shows 'in transit'. What's going on?\",\n",
    "        \"Yes, I checked the tracking number TRK789456123 and it hasn't updated since Monday.\",\n",
    "        \"That would be helpful, thanks. The delivery address is 123 Main St, Apt 4B, Boston MA.\",\n",
    "        \"Great! I appreciate you expediting it. Will I get an email when it ships out tomorrow?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"{'â”€'*70}\")\n",
    "print(f\"Scenario: {scenario_shipping['name']}\")\n",
    "print(f\"Description: {scenario_shipping['description']}\")\n",
    "print(f\"Expected Retention: {scenario_shipping['expected_retention']}\")\n",
    "print(f\"{'â”€'*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run shipping scenario\n",
    "with mlflow.start_run(run_name=scenario_shipping['name']) as run:\n",
    "    conv_result_shipping = agent.run_conversation(\n",
    "        messages=scenario_shipping['messages'],\n",
    "        session_id=scenario_shipping['session_id']\n",
    "    )\n",
    "    \n",
    "    run_id_shipping = run.info.run_id\n",
    "    \n",
    "    print(f\"\\nEvaluating session '{scenario_shipping['session_id']}'...\\n\")\n",
    "    \n",
    "    try:\n",
    "        eval_result_shipping = agent.evaluate_session(scenario_shipping['session_id'], run_id_shipping)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Evaluation Results: {scenario_shipping['name']}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        coherence_symbol = \"âœ“\" if eval_result_shipping['coherence']['passed'] else \"âœ—\"\n",
    "        coherence_label = \"PASS\" if eval_result_shipping['coherence']['passed'] else \"FAIL\"\n",
    "        print(f\"\\nðŸ“Š Coherence: {coherence_symbol} {coherence_label}\")\n",
    "        print(f\"   Value: {eval_result_shipping['coherence']['feedback_value']}\")\n",
    "        print(f\"   Rationale: {eval_result_shipping['coherence']['rationale']}\")\n",
    "        \n",
    "        retention_value = str(eval_result_shipping['context_retention']['feedback_value']).upper()\n",
    "        retention_score = eval_result_shipping['context_retention']['score']\n",
    "        print(f\"\\nðŸ§  Context Retention: {retention_value}\")\n",
    "        print(f\"   Score: {retention_score}/4\")\n",
    "        print(f\"   Rationale: {eval_result_shipping['context_retention']['rationale']}\")\n",
    "        \n",
    "        print(f\"\\n{'='*70}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâœ— Evaluation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Try It Yourself!\n",
    "\n",
    "Create and run your own custom conversation scenario using the helper function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_custom_scenario(name: str, messages: List[str], session_id: str = None):\n",
    "    \"\"\"\n",
    "    Helper function to run a complete conversation scenario.\n",
    "    \n",
    "    Args:\n",
    "        name: Scenario name\n",
    "        messages: List of user messages (turns)\n",
    "        session_id: Optional session ID (auto-generated if not provided)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with conversation and evaluation results\n",
    "    \"\"\"\n",
    "    import uuid\n",
    "    \n",
    "    if session_id is None:\n",
    "        session_id = f\"session-custom-{uuid.uuid4().hex[:8]}\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Running Custom Scenario: {name}\")\n",
    "    print(f\"Session ID: {session_id}\")\n",
    "    print(f\"Turns: {len(messages)}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Run conversation\n",
    "    with mlflow.start_run(run_name=name) as run:\n",
    "        conv_result = agent.run_conversation(\n",
    "            messages=messages,\n",
    "            session_id=session_id\n",
    "        )\n",
    "        \n",
    "        run_id = run.info.run_id\n",
    "        \n",
    "        # Evaluate\n",
    "        print(f\"\\nEvaluating session...\\n\")\n",
    "        eval_result = agent.evaluate_session(session_id, run_id)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Evaluation Results: {name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        coherence_symbol = \"âœ“\" if eval_result['coherence']['passed'] else \"âœ—\"\n",
    "        coherence_label = \"PASS\" if eval_result['coherence']['passed'] else \"FAIL\"\n",
    "        print(f\"\\nðŸ“Š Coherence: {coherence_symbol} {coherence_label}\")\n",
    "        print(f\"   Rationale: {eval_result['coherence']['rationale']}\")\n",
    "        \n",
    "        retention_value = str(eval_result['context_retention']['feedback_value']).upper()\n",
    "        retention_score = eval_result['context_retention']['score']\n",
    "        print(f\"\\nðŸ§  Context Retention: {retention_value} ({retention_score}/4)\")\n",
    "        print(f\"   Rationale: {eval_result['context_retention']['rationale']}\")\n",
    "        \n",
    "        print(f\"\\n{'='*70}\\n\")\n",
    "        \n",
    "        return {\n",
    "            \"conversation\": conv_result,\n",
    "            \"evaluation\": eval_result\n",
    "        }\n",
    "\n",
    "# Example: Uncomment and customize!\n",
    "# result = run_custom_scenario(\n",
    "#     name=\"Product Return Request\",\n",
    "#     messages=[\n",
    "#         \"I want to return a laptop I bought last week. It's not working properly.\",\n",
    "#         \"The order number is ORD-9876. The screen keeps flickering.\",\n",
    "#         \"Yes, I have the original box and all accessories.\",\n",
    "#         \"Great! I'll ship it back tomorrow. Thanks for your help!\"\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "print(\"âœ“ Helper function defined: run_custom_scenario()\")\n",
    "print(\"  Uncomment the example above to try it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Multi-Scenario Experiment\n",
    "\n",
    "Run all scenarios in a batch and compare the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all scenario results\n",
    "all_results = []\n",
    "\n",
    "# Note: We already ran scenarios above, so we'll use those results\n",
    "# In a fresh run, you'd execute all scenarios here\n",
    "\n",
    "try:\n",
    "    all_results.append({\n",
    "        \"scenario\": scenario1['name'],\n",
    "        \"session_id\": scenario1['session_id'],\n",
    "        \"turns\": len(scenario1['messages']),\n",
    "        \"coherence\": \"PASS\" if eval_result1['coherence']['passed'] else \"FAIL\",\n",
    "        \"retention\": eval_result1['context_retention']['feedback_value'],\n",
    "        \"retention_score\": eval_result1['context_retention']['score']\n",
    "    })\n",
    "    \n",
    "    all_results.append({\n",
    "        \"scenario\": scenario2['name'],\n",
    "        \"session_id\": scenario2['session_id'],\n",
    "        \"turns\": len(scenario2['messages']),\n",
    "        \"coherence\": \"PASS\" if eval_result2['coherence']['passed'] else \"FAIL\",\n",
    "        \"retention\": eval_result2['context_retention']['feedback_value'],\n",
    "        \"retention_score\": eval_result2['context_retention']['score']\n",
    "    })\n",
    "    \n",
    "    all_results.append({\n",
    "        \"scenario\": scenario_billing['name'],\n",
    "        \"session_id\": scenario_billing['session_id'],\n",
    "        \"turns\": len(scenario_billing['messages']),\n",
    "        \"coherence\": \"PASS\" if eval_result_billing['coherence']['passed'] else \"FAIL\",\n",
    "        \"retention\": eval_result_billing['context_retention']['feedback_value'],\n",
    "        \"retention_score\": eval_result_billing['context_retention']['score']\n",
    "    })\n",
    "    \n",
    "    all_results.append({\n",
    "        \"scenario\": scenario_shipping['name'],\n",
    "        \"session_id\": scenario_shipping['session_id'],\n",
    "        \"turns\": len(scenario_shipping['messages']),\n",
    "        \"coherence\": \"PASS\" if eval_result_shipping['coherence']['passed'] else \"FAIL\",\n",
    "        \"retention\": eval_result_shipping['context_retention']['feedback_value'],\n",
    "        \"retention_score\": eval_result_shipping['context_retention']['score']\n",
    "    })\n",
    "    \n",
    "    print(\"âœ“ Collected results from all scenarios\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Some scenarios may not have run yet. Error: {e}\")\n",
    "    print(\"   Run all scenario cells above first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary table\n",
    "if len(all_results) > 0:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ“Š MULTI-SCENARIO EXPERIMENT SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nTotal Scenarios Run: {len(all_results)}\")\n",
    "    print(f\"\\n{results_df.to_string(index=False)}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_retention_score = results_df['retention_score'].mean()\n",
    "    coherence_pass_rate = (results_df['coherence'] == 'PASS').sum() / len(results_df) * 100\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ðŸ“ˆ STATISTICS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Average Context Retention Score: {avg_retention_score:.2f}/4\")\n",
    "    print(f\"Coherence Pass Rate: {coherence_pass_rate:.0f}%\")\n",
    "    \n",
    "    # Retention distribution\n",
    "    retention_counts = results_df['retention'].value_counts()\n",
    "    print(f\"\\nContext Retention Distribution:\")\n",
    "    for level, count in retention_counts.items():\n",
    "        print(f\"  {level}: {count} scenario(s)\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "else:\n",
    "    print(\"No results to display. Run scenario cells above first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Mechanics\n",
    "\n",
    "Let's examine how the session-level evaluation works under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the judge instructions with {{ conversation }} template\n",
    "print(\"Coherence Judge Instructions:\")\n",
    "print(\"=\"*70)\n",
    "print(get_coherence_judge_instructions())\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"Context Retention Judge Instructions:\")\n",
    "print(\"=\"*70)\n",
    "print(get_context_retention_judge_instructions())\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"ðŸ’¡ Key Observation:\")\n",
    "print(\"Both judge instructions contain {{ conversation }} template variable.\")\n",
    "print(\"This automatically makes them session-level judges in MLflow 3.7!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the session tracking pattern\n",
    "print(\"Session Tracking Code Pattern:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "@mlflow.trace(span_type=SpanType.CHAT_MODEL, name=\"handle_support_message\")\n",
    "def handle_message(self, message: str, session_id: str) -> str:\n",
    "    # CRITICAL: Update trace with session metadata\n",
    "    mlflow.update_current_trace(\n",
    "        metadata={\"mlflow.trace.session\": session_id}\n",
    "    )\n",
    "    \n",
    "    # ... rest of message handling ...\n",
    "    \n",
    "    return assistant_message\n",
    "\"\"\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nðŸ’¡ This simple metadata update enables all of MLflow 3.7's session-level features!\")\n",
    "print(\"   - Groups multiple traces into one session\")\n",
    "print(\"   - Enables {{ conversation }} template substitution\")\n",
    "print(\"   - Allows session-level evaluation instead of turn-level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Key Concepts Summary\n",
    "\n",
    "### MLflow 3.7 Session-Level Evaluation Features\n",
    "\n",
    "1. **Session Tracking**: `mlflow.update_current_trace(metadata={'mlflow.trace.session': session_id})`\n",
    "   - Tags each trace with a session ID\n",
    "   - Enables grouping of related traces\n",
    "   - Critical for multi-turn conversation tracking\n",
    "\n",
    "2. **{{ conversation }} Template**: Automatically aggregates session traces\n",
    "   - Used in judge instructions\n",
    "   - Makes judges automatically session-level aware\n",
    "   - MLflow substitutes with formatted conversation transcript\n",
    "\n",
    "3. **Session-Level Judges**: Two types demonstrated\n",
    "   - **Boolean Judge** (Coherence): Returns True/False\n",
    "   - **Categorical Judge** (Context Retention): Returns excellent/good/fair/poor\n",
    "\n",
    "4. **Session-Level Evaluation**: `mlflow.genai.evaluate()` with session-level judges\n",
    "   - Evaluates entire conversation, not individual turns\n",
    "   - Returns one result per session (not per trace)\n",
    "   - Enables assessment of coherence, context retention, and flow\n",
    "\n",
    "### Workflow Summary\n",
    "\n",
    "```\n",
    "1. Setup MLflow tracking\n",
    "2. Initialize CustomerSupportAgent with judges\n",
    "3. Run multi-turn conversation (each turn creates a trace)\n",
    "4. Each trace tagged with session ID via mlflow.update_current_trace()\n",
    "5. Evaluate session with mlflow.genai.evaluate()\n",
    "6. Judges receive {{ conversation }} with full transcript\n",
    "7. Get coherence and context retention results\n",
    "```\n",
    "\n",
    "### Judge Configuration\n",
    "\n",
    "- **Coherence Judge**: `feedback_value_type=bool`\n",
    "- **Context Retention Judge**: `feedback_value_type=Literal[\"excellent\", \"good\", \"fair\", \"poor\"]`\n",
    "- Both use `{{ conversation }}` template â†’ automatically session-level\n",
    "- Both return feedback_value + rationale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Context Retention Levels Explained\n",
    "\n",
    "The context retention judge evaluates conversations on a 4-level scale:\n",
    "\n",
    "| Level | Score | Description |\n",
    "|-------|-------|-------------|\n",
    "| **Excellent** | 4 | Perfectly recalls all relevant prior context; seamlessly references earlier information; builds on previous exchanges without repetition |\n",
    "| **Good** | 3 | Recalls most relevant context; occasionally references earlier info; minor lapses but overall maintains context |\n",
    "| **Fair** | 2 | Some context retention issues; asks redundant questions; doesn't fully leverage earlier information |\n",
    "| **Poor** | 1 | Frequently forgets prior context; treats each turn independently; significant repetition or contradictions |\n",
    "\n",
    "### Examples from Our Scenarios:\n",
    "\n",
    "- **Printer Troubleshooting**: Expected \"excellent\" - Agent remembers HP LaserJet 3000, power cable checks, outlet testing\n",
    "- **Account Access**: Expected \"good\" - Agent tracks password issue, email address, but may have minor inefficiencies\n",
    "- **Billing Inquiry**: Expected \"excellent\" - Agent remembers $29.99 charge, Dec 1st date, premium subscription, price change\n",
    "- **Shipping Delay**: Expected \"good\" - Agent tracks order #12345, tracking number, address, but might ask for info already in system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Next Steps\n",
    "\n",
    "1. **Explore MLflow UI**: Run `mlflow ui` to see detailed traces\n",
    "   - View session groupings\n",
    "   - Explore trace hierarchies\n",
    "   - Compare evaluation results across sessions\n",
    "\n",
    "2. **Customize Judge Instructions**: Edit `prompts.py` to change evaluation criteria\n",
    "   - Modify coherence requirements\n",
    "   - Adjust context retention levels\n",
    "   - Add new evaluation dimensions\n",
    "\n",
    "3. **Try Different Models**: Experiment with different agent and judge models\n",
    "   - Compare OpenAI vs Databricks models\n",
    "   - Test various temperature settings\n",
    "   - Evaluate model performance on context retention\n",
    "\n",
    "4. **Create Custom Scenarios**: Design your own conversation tests\n",
    "   - Test edge cases (very long conversations, context switching)\n",
    "   - Domain-specific scenarios (e.g., healthcare, finance)\n",
    "   - Multi-language support\n",
    "\n",
    "5. **Apply to Your Use Case**: Adapt this pattern for your own applications\n",
    "   - Customer support bots\n",
    "   - Virtual assistants\n",
    "   - Educational tutors\n",
    "   - Any multi-turn conversational AI\n",
    "\n",
    "## ðŸ“– Resources\n",
    "\n",
    "- [MLflow GenAI Judges Documentation](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html)\n",
    "- [MLflow 3.7 Release Notes](https://github.com/mlflow/mlflow/releases/tag/v3.7.0)\n",
    "- [Session-Level Evaluation Guide](https://mlflow.org/docs/latest/llms/tracing/index.html)\n",
    "- [Customer Support Agent README](README.md)\n",
    "- [Agent Planning Judge Tutorial](../agent_planning/agent_planning_judge.ipynb) - Related pattern"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
