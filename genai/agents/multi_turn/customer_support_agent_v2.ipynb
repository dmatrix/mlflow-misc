{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow Evaluation Demo: Multi-Turn Conversation with Session-Level Judges\n",
    "\n",
    "![Multi-turn evaluationjudges](images/llm_as_judge.png)\n",
    "\n",
    "This notebook demonstrates MLflow 3.7's evaluation capabilities by showing how to:\n",
    "1. Set up session-level judges directly in the notebook\n",
    "2. Use `mlflow.genai.evaluate()` API to evaluate conversations\n",
    "3. Extract and interpret evaluation results\n",
    "\n",
    "**Key Difference from v1**: This notebook showcases MLflow's evaluation methods (`make_judge`, `search_traces`, `genai.evaluate`) by keeping evaluation logic in the notebook rather than hidden in the agent class.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to create session-level judges with `{{ conversation }}` template\n",
    "- How to call `mlflow.genai.evaluate()` directly\n",
    "- How to extract results from evaluation DataFrames\n",
    "- Best practices for multi-turn conversation evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai.common.config import AgentConfig\n",
    "from genai.agents.multi_turn.customer_support_agent_simple import CustomerSupportAgentSimple\n",
    "from genai.agents.multi_turn.scenarios import get_scenario_printer_troubleshooting\n",
    "from genai.agents.multi_turn.prompts import (\n",
    "    get_coherence_judge_instructions,\n",
    "    get_context_retention_judge_instructions,\n",
    ")\n",
    "import mlflow\n",
    "from mlflow.genai.judges import make_judge\n",
    "from typing_extensions import Literal\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        load_dotenv(env_file)\n",
    "        print(\"‚úì Loaded environment variables from {env_file.absolute()}\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  No .env file found. Set environment variables manually.\")\n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è  python-dotenv not installed. Set environment variables manually.\")\n",
    "\n",
    "## Convenience for display\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provider and model configuration\n",
    "PROVIDER = \"databricks\"  # or \"openai\"\n",
    "AGENT_MODEL = \"databricks-gpt-5\"\n",
    "JUDGE_MODEL = \"databricks-gemini-2-5-flash\"\n",
    "TEMPERATURE = 1.0\n",
    "EXPERIMENT_NAME = \"customer-support-v2-demo\"\n",
    "\n",
    "print(\"‚úì Configuration:\")\n",
    "print(f\"  Provider: {PROVIDER}\")\n",
    "print(f\"  Agent Model: {AGENT_MODEL}\")\n",
    "print(f\"  Judge Model: {JUDGE_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup MLflow Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.openai.autolog()\n",
    "using_databricks_mlflow = False\n",
    "if using_databricks_mlflow:\n",
    "    mlflow.set_tracking_uri(\"databricks\")\n",
    "    EXPERIMENT_NAME = f\"/Users/danny.chiao@databricks.com/{EXPERIMENT_NAME}\"\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "else:\n",
    "    mlflow.set_tracking_uri(None)\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Agent (Conversation-Only)\n",
    "\n",
    "Note: This agent handles conversations only. We'll set up evaluation separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AgentConfig(\n",
    "    model=AGENT_MODEL,\n",
    "    provider=PROVIDER,\n",
    "    temperature=TEMPERATURE,\n",
    "    mlflow_experiment=EXPERIMENT_NAME\n",
    ")\n",
    "\n",
    "agent = CustomerSupportAgentSimple(config)\n",
    "\n",
    "print(\"‚úì Agent initialized (conversation-only)\")\n",
    "print(f\"  Provider: {config.provider}\")\n",
    "print(f\"  Model: {config.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Conversation Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load printer troubleshooting scenario\n",
    "scenario = get_scenario_printer_troubleshooting()\n",
    "\n",
    "print(f\"Scenario: {scenario['name']}\")\n",
    "print(f\"Turns: {len(scenario['messages'])}\")\n",
    "print(f\"Session ID: {scenario['session_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the conversation\n",
    "# Under the hood, it tags messages with a session id (mlflow.update_current_trace(metadata={\"mlflow.trace.session\": session_id}))\n",
    "# Then it calls into OpenAI for responses, but sends simulated pre-canned user responses\n",
    "conv_result = agent.run_conversation(\n",
    "    messages=scenario['messages'],\n",
    "    session_id=scenario['session_id']\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Conversation completed\")\n",
    "print(\"  Turns: {conv_result['turns']}\")\n",
    "print(\"Now run `mlflow ui` from this directory to see the experiment in http://127.0.0.1:5000/#/experiments/1/chat-sessions/{scenario['session_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ Evaluation Showcase: MLflow Methods\n",
    "\n",
    "Now we'll demonstrate MLflow's evaluation capabilities step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Session-Level Judges with `make_judge()`\n",
    "\n",
    "This is where we showcase MLflow's judge creation API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's see what the judges will evaluate\n",
    "print(\"=\"*70)\n",
    "print(\"COHERENCE JUDGE INSTRUCTIONS\")\n",
    "print(\"=\"*70)\n",
    "print(get_coherence_judge_instructions())\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONTEXT RETENTION JUDGE INSTRUCTIONS\")\n",
    "print(\"=\"*70)\n",
    "print(get_context_retention_judge_instructions())\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüí° Both instructions contain {{ conversation }} - this makes them session-level!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure judge model URI\n",
    "if PROVIDER == \"databricks\":\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"DATABRICKS_TOKEN\", \"\")\n",
    "    os.environ[\"OPENAI_API_BASE\"] = f\"{config.databricks_host}/serving-endpoints\"\n",
    "    judge_model_uri = f\"openai:/{JUDGE_MODEL}\"\n",
    "else:\n",
    "    judge_model_uri = JUDGE_MODEL\n",
    "\n",
    "print(f\"Judge model URI: {judge_model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create judges using mlflow.genai.judges.make_judge()\n",
    "print(\"Creating judges...\\n\")\n",
    "\n",
    "coherence_judge = make_judge(\n",
    "    name=\"conversation_coherence\",\n",
    "    model=judge_model_uri,\n",
    "    instructions=get_coherence_judge_instructions(),\n",
    "    feedback_value_type=bool\n",
    ")\n",
    "\n",
    "context_judge = make_judge(\n",
    "    name=\"context_retention\",\n",
    "    model=judge_model_uri,\n",
    "    instructions=get_context_retention_judge_instructions(),\n",
    "    feedback_value_type=Literal[\"excellent\", \"good\", \"fair\", \"poor\"]\n",
    ")\n",
    "\n",
    "print(\"‚úì Judges created\")\n",
    "print(f\"  Coherence judge is session-level: {coherence_judge.is_session_level_scorer}\")\n",
    "print(f\"  Context judge is session-level: {context_judge.is_session_level_scorer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Search for Traces with `mlflow.search_traces()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get experiment and search for traces\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "session_traces = mlflow.search_traces(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    filter_string=f\"metadata.`mlflow.trace.session` = '{scenario['session_id']}'\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Found {len(session_traces)} traces\")\n",
    "print(f\"  Session ID: {scenario['session_id']}\")\n",
    "print(\"  Each trace = 1 conversation turn\")\n",
    "display(session_traces[[\"request_time\", \"request\", \"response\"]].sort_values(by=\"request_time\", ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate with `mlflow.genai.evaluate()`\n",
    "\n",
    "**Key MLflow API**: This is where the magic happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating conversation...\\n\")\n",
    "\n",
    "with mlflow.start_run(run_name=scenario['name']) as run:\n",
    "    eval_results = mlflow.genai.evaluate(\n",
    "        data=session_traces,\n",
    "        scorers=[coherence_judge, context_judge]\n",
    "    )\n",
    "\n",
    "print(\"‚úì Evaluation complete\")\n",
    "result_df = eval_results.result_df\n",
    "print(f\"  Result DataFrame shape: {result_df.shape}\")\n",
    "if not using_databricks_mlflow:\n",
    "    print(\"See results at http://localhost:5000/#/experiments/1/chat-sessions/session-printer-001\")\n",
    "    print(\"See results at http://localhost:5000/#/experiments/1/evaluation-runs?selectedRunUuid={eval_results.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_df[[\"assessments\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä What Just Happened?\n",
    "\n",
    "We just demonstrated the complete MLflow evaluation workflow:\n",
    "\n",
    "1. **`make_judge()`**: Created session-level judges with `{{ conversation }}` template\n",
    "2. **`mlflow.search_traces()`**: Found all traces for the conversation\n",
    "3. **`mlflow.genai.evaluate()`**: Evaluated the full conversation (not individual turns)\n",
    "\n",
    "**Key Insight**: Session-level evaluation assesses the entire conversation holistically, enabling evaluation of:\n",
    "- Conversation coherence and flow\n",
    "- Context retention across turns\n",
    "- Logical progression of the discussion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
