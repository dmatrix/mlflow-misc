{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge Tutorial: Tool Selection Evaluation with MLflow\n",
    "\n",
    "This interactive notebook demonstrates how to use MLflow's LLM-as-a-Judge pattern to evaluate AI agent decisions.\n",
    "\n",
    "## Tutorial Goals\n",
    "\n",
    "1. Use MLflow tracing to capture agent actions\n",
    "2. Create a judge using `mlflow.genai.judges.make_judge()`\n",
    "3. Evaluate agent decisions using the judge\n",
    "4. Integrate with MLflow experiments for reproducibility\n",
    "\n",
    "## Scenario\n",
    "\n",
    "An AI agent selects a tool to answer a user query. The judge evaluates whether the agent chose the appropriate tool.\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "- Does the selected tool match the user's intent?\n",
    "- Can this tool address the task requirements?\n",
    "- Are there more suitable tools available?\n",
    "\n",
    "\n",
    "![LLM-as-a-Judge tool selection evaluation with MLflow](images/tools_selection_notebook_diagram.png)\n",
    "\n",
    "---\n",
    "\n",
    "Based on: [Using LLM as a Judge](https://medium.com/@juanc.olamendy/using-llm-as-a-judge-to-evaluate-agent-outputs-a-comprehensive-tutorial-00b6f1f356cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Dependencies\n",
    "\n",
    "First, let's import all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai.common.config import AgentConfig\n",
    "from genai.agents.tools_selection.prompts import get_judge_instructions, get_tool_selection_prompt\n",
    "import mlflow\n",
    "from typing import List\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables from .env file if it exists\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        load_dotenv(env_file)\n",
    "        print(f\"‚úì Loaded environment variables from {env_file.absolute()}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è  No .env file found at {env_file.absolute()}\")\n",
    "        print(\"   You can create one with your credentials or set environment variables manually\")\n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è  python-dotenv not installed. Install with: pip install python-dotenv\")\n",
    "    print(\"   Or set environment variables manually in the cells below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration: Set Up Environment\n",
    "\n",
    "**Two ways to configure credentials:**\n",
    "\n",
    "1. **Recommended**: Create a `.env` file in this directory with your credentials\n",
    "2. **Alternative**: Uncomment and set credentials in the cells below\n",
    "\n",
    "### Create a `.env` file (Recommended)\n",
    "\n",
    "Create a file named `.env` in the same directory as this notebook:\n",
    "\n",
    "**For Databricks:**\n",
    "```\n",
    "DATABRICKS_TOKEN=your-token-here\n",
    "DATABRICKS_HOST=https://your-workspace.cloud.databricks.com\n",
    "```\n",
    "\n",
    "**For OpenAI:**\n",
    "```\n",
    "OPENAI_API_KEY=sk-your-key-here\n",
    "```\n",
    "\n",
    "The cell above will automatically load these credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration set:\n",
      "  Provider: databricks\n",
      "  Agent Model: databricks-gpt-5\n",
      "  Judge Model: databricks-gemini-2-5-flash\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION: Choose your provider and models\n",
    "# ============================================================================\n",
    "\n",
    "# Option 1: Databricks (default)\n",
    "PROVIDER = \"databricks\"\n",
    "AGENT_MODEL = \"databricks-gpt-5\"\n",
    "JUDGE_MODEL = \"databricks-gemini-2-5-flash\"\n",
    "\n",
    "# Option 2: OpenAI (uncomment to use)\n",
    "# PROVIDER = \"openai\"\n",
    "# AGENT_MODEL = \"gpt-4o-mini\"\n",
    "# JUDGE_MODEL = \"gpt-4o\"\n",
    "\n",
    "# Other settings\n",
    "TEMPERATURE = 1.0\n",
    "EXPERIMENT_NAME = \"tool-selection-judge-notebook\"\n",
    "\n",
    "print(f\"‚úì Configuration set:\")\n",
    "print(f\"  Provider: {PROVIDER}\")\n",
    "print(f\"  Agent Model: {AGENT_MODEL}\")\n",
    "print(f\"  Judge Model: {JUDGE_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Credentials (Optional Manual Setup)\n",
    "\n",
    "If you didn't create a `.env` file, you can set credentials manually by uncommenting the appropriate lines below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Databricks credentials found\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MANUAL CREDENTIAL SETUP (if not using .env file)\n",
    "# ============================================================================\n",
    "\n",
    "# For Databricks - Uncomment and set these if you didn't create a .env file\n",
    "# os.environ[\"DATABRICKS_TOKEN\"] = \"your-token-here\"\n",
    "# os.environ[\"DATABRICKS_HOST\"] = \"https://your-workspace.cloud.databricks.com\"\n",
    "\n",
    "# For OpenAI - Uncomment and set this if you didn't create a .env file\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-your-key-here\"\n",
    "\n",
    "# Verify credentials are set\n",
    "if PROVIDER == \"databricks\":\n",
    "    if \"DATABRICKS_TOKEN\" in os.environ and \"DATABRICKS_HOST\" in os.environ:\n",
    "        print(\"‚úì Databricks credentials found\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Missing Databricks credentials!\")\n",
    "        print(\"   Create a .env file or uncomment the lines above to set credentials\")\n",
    "elif PROVIDER == \"openai\":\n",
    "    if \"OPENAI_API_KEY\" in os.environ:\n",
    "        print(\"‚úì OpenAI credentials found\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Missing OpenAI credentials!\")\n",
    "        print(\"   Create a .env file or uncomment the lines above to set credentials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup MLflow Tracing\n",
    "\n",
    "Enable MLflow tracing to capture all agent actions and LLM calls automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai.common.mlflow_config import setup_mlflow_tracking\n",
    "\n",
    "setup_mlflow_tracking(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    enable_autolog=True\n",
    ")\n",
    "\n",
    "print(\"\\n[Step 1] MLflow tracing enabled\")\n",
    "print(f\"  ‚îî‚îÄ Experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"  ‚îî‚îÄ View traces: mlflow ui\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import the Agent Class\n",
    "\n",
    "Import the `AgentToolSelectionJudge` class that demonstrates the complete LLM-as-a-Judge pattern:\n",
    "1. Agent performs an action (`select_tool`) - traced with MLflow\n",
    "2. Judge evaluates the action (`evaluate`) - uses `make_judge()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the AgentToolSelectionJudge class from the module\n",
    "from genai.agents.tools_selection.tool_selection_judge import AgentToolSelectionJudge\n",
    "\n",
    "print(\"‚úì AgentToolSelectionJudge imported successfully\")\n",
    "print(\"\\nThe class provides:\")\n",
    "print(\"  - select_tool(): Agent selects a tool based on user query\")\n",
    "print(\"  - evaluate(): Judge evaluates the agent's tool selection\")\n",
    "print(\"  - Uses MLflow tracing and make_judge() for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Agent and Judge\n",
    "\n",
    "Create the configuration and instantiate our judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent configuration\n",
    "config = AgentConfig(\n",
    "    model=AGENT_MODEL,\n",
    "    provider=PROVIDER,\n",
    "    temperature=TEMPERATURE\n",
    ")\n",
    "\n",
    "# Initialize the judge\n",
    "judge = AgentToolSelectionJudge(config, judge_model=JUDGE_MODEL)\n",
    "\n",
    "print(\"\\n[Step 2] Initializing Agent and Judge\")\n",
    "print(f\"  ‚îî‚îÄ Provider: {config.provider}\")\n",
    "print(f\"  ‚îî‚îÄ Agent Model: {config.model}\")\n",
    "print(f\"  ‚îî‚îÄ Judge Model: {JUDGE_MODEL}\")\n",
    "print(f\"  ‚îî‚îÄ Temperature: {config.temperature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Test Scenario\n",
    "\n",
    "Set up a user query and available tools for the agent to choose from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scenario\n",
    "user_request = \"What's the weather like in San Francisco?\"\n",
    "available_tools = [\"get_weather_api\", \"search_web\", \"get_calendar\", \"send_email\"]\n",
    "\n",
    "print(\"\\n[Step 3] Test Scenario\")\n",
    "print(f\"  ‚îî‚îÄ User Query: {user_request}\")\n",
    "print(f\"  ‚îî‚îÄ Available Tools: {available_tools}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example queries to try:\n",
    "#user_request = \"Send email to John about the meeting\"\n",
    "# user_request = \"What meetings do I have today?\"\n",
    "# user_request = \"Search for information about machine learning\"\n",
    "# user_request = \"What's the current stock price of AAPL?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Step 4] Agent selects a tool...\")\n",
    "tool_selected = judge.select_tool(user_request, available_tools)\n",
    "print(f\"  ‚îî‚îÄ ‚úì Selected: {tool_selected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Judge Evaluates the Selection\n",
    "\n",
    "Now the judge evaluates whether the agent made the right choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Step 5] Judge evaluates the selection...\")\n",
    "\n",
    "# Get the trace ID from the agent's action\n",
    "trace_id = mlflow.get_last_active_trace_id()\n",
    "\n",
    "# Evaluate with the judge\n",
    "result = judge.evaluate(trace_id)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n[Step 6] Evaluation Results\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Decision: {'‚úì CORRECT' if result['is_correct'] else '‚úó INCORRECT'}\")\n",
    "print(\"\\nReasoning:\")\n",
    "print(f\"{result['reasoning']}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Detailed Traces in MLflow UI\n",
    "\n",
    "You can view detailed traces in the MLflow UI to see:\n",
    "- The full conversation flow\n",
    "- LLM inputs and outputs\n",
    "- Execution times\n",
    "- All logged parameters\n",
    "\n",
    "Run this command in your terminal:\n",
    "```bash\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "Then navigate to: http://localhost:5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Try It Yourself!\n",
    "\n",
    "Run the complete workflow with a custom query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Query: Send email to John about the meeting\n",
      "Available Tools: ['get_weather_api', 'search_web', 'get_calendar', 'send_email']\n",
      "======================================================================\n",
      "\n",
      "‚úì Agent selected: send_email\n",
      "\n",
      "Decision: ‚úì CORRECT\n",
      "\n",
      "Reasoning:\n",
      "The user's request was to \"Send email to John about the meeting\". The agent correctly identified and selected the `send_email` tool from the available options. This tool directly aligns with the user's intent and is the most appropriate choice among the given tools (`get_weather_api`, `search_web`, `get_calendar`).\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-ed530ba71ed5014fe5504a9c77b59a7b&amp;experiment_id=1&amp;trace_id=tr-f5ae76d8e6d4f7533df73d02c3557bfe&amp;experiment_id=1&amp;trace_id=tr-d7d4f9e499b0bd377e7cf8e45fab1909&amp;experiment_id=1&amp;trace_id=tr-df42c3f40bda7ffe3414df78dab3d5a8&amp;experiment_id=1&amp;version=3.6.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-ed530ba71ed5014fe5504a9c77b59a7b), Trace(trace_id=tr-f5ae76d8e6d4f7533df73d02c3557bfe), Trace(trace_id=tr-d7d4f9e499b0bd377e7cf8e45fab1909), Trace(trace_id=tr-df42c3f40bda7ffe3414df78dab3d5a8)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_evaluation(query: str, tools: List[str]):\n",
    "    \"\"\"\n",
    "    Complete workflow: Agent selects tool ‚Üí Judge evaluates\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Available Tools: {tools}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Agent selects tool\n",
    "    selected = judge.select_tool(query, tools)\n",
    "    print(f\"‚úì Agent selected: {selected}\\n\")\n",
    "    \n",
    "    # Judge evaluates\n",
    "    trace_id = mlflow.get_last_active_trace_id()\n",
    "    result = judge.evaluate(trace_id)\n",
    "    \n",
    "    # Show results\n",
    "    print(f\"Decision: {'‚úì CORRECT' if result['is_correct'] else '‚úó INCORRECT'}\")\n",
    "    print(f\"\\nReasoning:\\n{result['reasoning']}\")\n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Try it!\n",
    "result = run_evaluation(\n",
    "    query=\"Send email to John about the meeting\",\n",
    "    tools=[\"get_weather_api\", \"search_web\", \"get_calendar\", \"send_email\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Experiment: Test Multiple Scenarios\n",
    "\n",
    "Let's evaluate multiple queries and see how the judge performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test scenarios\n",
    "test_scenarios = [\n",
    "    (\"What's the weather in Boston?\", [\"get_weather_api\", \"search_web\", \"get_calendar\", \"send_email\"]),\n",
    "    (\"Schedule a meeting for tomorrow\", [\"get_weather_api\", \"search_web\", \"get_calendar\", \"send_email\"]),\n",
    "    (\"Find information about Python\", [\"get_weather_api\", \"search_web\", \"get_calendar\", \"send_email\"]),\n",
    "    (\"Send a message to Sarah\", [\"get_weather_api\", \"search_web\", \"get_calendar\", \"send_email\"]),\n",
    "]\n",
    "\n",
    "# Run all scenarios\n",
    "results = []\n",
    "for query, tools in test_scenarios:\n",
    "    result = run_evaluation(query, tools)\n",
    "    results.append({\n",
    "        \"query\": query,\n",
    "        \"correct\": result[\"is_correct\"]\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "correct_count = sum(1 for r in results if r[\"correct\"])\n",
    "total = len(results)\n",
    "print(f\"\\nüìä Summary: {correct_count}/{total} selections were correct ({correct_count/total*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Customization\n",
    "\n",
    "### Modify Evaluation Criteria\n",
    "\n",
    "The judge's evaluation criteria are defined in `prompts.py`. You can view them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current Judge Instructions:\")\n",
    "print(\"=\" * 70)\n",
    "print(get_judge_instructions())\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Tool Selection Prompt\n",
    "\n",
    "See how the agent is instructed to select tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = get_tool_selection_prompt(\n",
    "    \"What's the weather?\",\n",
    "    [\"get_weather_api\", \"search_web\"]\n",
    ")\n",
    "\n",
    "print(\"Example Tool Selection Prompt:\")\n",
    "print(\"=\" * 70)\n",
    "print(example_prompt)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Key Concepts Summary\n",
    "\n",
    "### MLflow Tracing\n",
    "- Automatically captures function inputs/outputs\n",
    "- Tracks execution time and metadata\n",
    "- Creates parent-child relationships for nested calls\n",
    "\n",
    "### MLflow Judge\n",
    "- Created with `make_judge()`\n",
    "- Takes predefined evaluation criteria\n",
    "- Returns structured feedback (value + rationale)\n",
    "\n",
    "### Separation of Concerns\n",
    "- **Agent**: Performs the task (tool selection)\n",
    "- **Judge**: Evaluates the agent's performance\n",
    "- **Prompts**: Define behavior (easy to modify in `prompts.py`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "1. **Explore MLflow UI**: Run `mlflow ui` to see detailed traces\n",
    "2. **Modify Prompts**: Edit `prompts.py` to change evaluation criteria\n",
    "3. **Try Different Models**: Experiment with different agent and judge models\n",
    "4. **Add More Tools**: Expand the `available_tools` list\n",
    "5. **Apply to Your Use Case**: Adapt this pattern for your own agent evaluations\n",
    "\n",
    "## üìñ Resources\n",
    "\n",
    "- [MLflow GenAI Judges Documentation](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html)\n",
    "- [Original Tutorial](https://medium.com/@juanc.olamendy/using-llm-as-a-judge-to-evaluate-agent-outputs-a-comprehensive-tutorial-00b6f1f356cc)\n",
    "- [Tool Selection Judge README](README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
