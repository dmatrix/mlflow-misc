{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge Tutorial: Tool Selection Evaluation with MLflow\n",
    "\n",
    "This interactive notebook demonstrates how to use MLflow's LLM-as-a-Judge pattern to evaluate AI agent decisions.\n",
    "\n",
    "## Tutorial Goals\n",
    "\n",
    "1. Use MLflow tracing to capture agent actions\n",
    "2. Create a judge using `mlflow.genai.judges.make_judge()`\n",
    "3. Evaluate agent decisions using the judge\n",
    "4. Integrate with MLflow experiments for reproducibility\n",
    "\n",
    "## Scenario\n",
    "\n",
    "An AI agent selects a tool to answer a user query. The judge evaluates whether the agent chose the appropriate tool.\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "- Does the selected tool match the user's intent?\n",
    "- Can this tool address the task requirements?\n",
    "- Are there more suitable tools available?\n",
    "\n",
    "---\n",
    "\n",
    "Based on: [Using LLM as a Judge](https://medium.com/@juanc.olamendy/using-llm-as-a-judge-to-evaluate-agent-outputs-a-comprehensive-tutorial-00b6f1f356cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Dependencies\n",
    "\n",
    "First, let's import all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded environment variables from /Users/jules/git-repos/mlflow-misc/genai/agents/tools_selection/.env\n"
     ]
    }
   ],
   "source": [
    "from genai.common import get_client\n",
    "from genai.common.config import AgentConfig\n",
    "from genai.agents.tools_selection.prompts import get_judge_instructions, get_tool_selection_prompt\n",
    "import mlflow\n",
    "from mlflow.entities import SpanType\n",
    "from mlflow.genai.judges import make_judge\n",
    "from typing import Dict, Any, List\n",
    "from typing_extensions import Literal\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables from .env file if it exists\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        load_dotenv(env_file)\n",
    "        print(f\"âœ“ Loaded environment variables from {env_file.absolute()}\")\n",
    "    else:\n",
    "        print(f\"â„¹ï¸  No .env file found at {env_file.absolute()}\")\n",
    "        print(\"   You can create one with your credentials or set environment variables manually\")\n",
    "except ImportError:\n",
    "    print(\"â„¹ï¸  python-dotenv not installed. Install with: pip install python-dotenv\")\n",
    "    print(\"   Or set environment variables manually in the cells below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration: Set Up Environment\n",
    "\n",
    "**Two ways to configure credentials:**\n",
    "\n",
    "1. **Recommended**: Create a `.env` file in this directory with your credentials\n",
    "2. **Alternative**: Uncomment and set credentials in the cells below\n",
    "\n",
    "### Create a `.env` file (Recommended)\n",
    "\n",
    "Create a file named `.env` in the same directory as this notebook:\n",
    "\n",
    "**For Databricks:**\n",
    "```\n",
    "DATABRICKS_TOKEN=your-token-here\n",
    "DATABRICKS_HOST=https://your-workspace.cloud.databricks.com\n",
    "```\n",
    "\n",
    "**For OpenAI:**\n",
    "```\n",
    "OPENAI_API_KEY=sk-your-key-here\n",
    "```\n",
    "\n",
    "The cell above will automatically load these credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration set:\n",
      "  Provider: databricks\n",
      "  Agent Model: databricks-gpt-5\n",
      "  Judge Model: databricks-gemini-2-5-flash\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION: Choose your provider and models\n",
    "# ============================================================================\n",
    "\n",
    "# Option 1: Databricks (default)\n",
    "PROVIDER = \"databricks\"\n",
    "AGENT_MODEL = \"databricks-gpt-5\"\n",
    "JUDGE_MODEL = \"databricks-gemini-2-5-flash\"\n",
    "\n",
    "# Option 2: OpenAI (uncomment to use)\n",
    "# PROVIDER = \"openai\"\n",
    "# AGENT_MODEL = \"gpt-4o-mini\"\n",
    "# JUDGE_MODEL = \"gpt-4o\"\n",
    "\n",
    "# Other settings\n",
    "TEMPERATURE = 1.0\n",
    "EXPERIMENT_NAME = \"tool-selection-judge-notebook\"\n",
    "\n",
    "print(f\"âœ“ Configuration set:\")\n",
    "print(f\"  Provider: {PROVIDER}\")\n",
    "print(f\"  Agent Model: {AGENT_MODEL}\")\n",
    "print(f\"  Judge Model: {JUDGE_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Credentials (Optional Manual Setup)\n",
    "\n",
    "If you didn't create a `.env` file, you can set credentials manually by uncommenting the appropriate lines below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Databricks credentials found\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MANUAL CREDENTIAL SETUP (if not using .env file)\n",
    "# ============================================================================\n",
    "\n",
    "# For Databricks - Uncomment and set these if you didn't create a .env file\n",
    "# os.environ[\"DATABRICKS_TOKEN\"] = \"your-token-here\"\n",
    "# os.environ[\"DATABRICKS_HOST\"] = \"https://your-workspace.cloud.databricks.com\"\n",
    "\n",
    "# For OpenAI - Uncomment and set this if you didn't create a .env file\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-your-key-here\"\n",
    "\n",
    "# Verify credentials are set\n",
    "if PROVIDER == \"databricks\":\n",
    "    if \"DATABRICKS_TOKEN\" in os.environ and \"DATABRICKS_HOST\" in os.environ:\n",
    "        print(\"âœ“ Databricks credentials found\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Missing Databricks credentials!\")\n",
    "        print(\"   Create a .env file or uncomment the lines above to set credentials\")\n",
    "elif PROVIDER == \"openai\":\n",
    "    if \"OPENAI_API_KEY\" in os.environ:\n",
    "        print(\"âœ“ OpenAI credentials found\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Missing OpenAI credentials!\")\n",
    "        print(\"   Create a .env file or uncomment the lines above to set credentials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup MLflow Tracing\n",
    "\n",
    "Enable MLflow tracing to capture all agent actions and LLM calls automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/28 13:31:11 INFO mlflow.tracking.fluent: Experiment with name 'tool-selection-judge-notebook' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ MLflow tracing enabled\n",
      "  Experiment: tool-selection-judge-notebook\n",
      "  View traces: mlflow ui\n"
     ]
    }
   ],
   "source": [
    "from genai.common.mlflow_config import setup_mlflow_tracking\n",
    "\n",
    "setup_mlflow_tracking(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    enable_autolog=True\n",
    ")\n",
    "\n",
    "print(\"âœ“ MLflow tracing enabled\")\n",
    "print(f\"  Experiment: {EXPERIMENT_NAME}\")\n",
    "print(\"  View traces: mlflow ui\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the Agent Class\n",
    "\n",
    "This class demonstrates the complete LLM-as-a-Judge pattern:\n",
    "1. Agent performs an action (`select_tool`) - traced with MLflow\n",
    "2. Judge evaluates the action (`evaluate`) - uses `make_judge()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class AgentToolSelectionJudge:\n    \"\"\"\n    Tutorial: LLM-as-a-Judge for Tool Selection Evaluation.\n    \n    The judge is a specialized LLM that assesses whether the agent made\n    the right decision based on predefined criteria.\n    \"\"\"\n    \n    def __init__(self, config: AgentConfig, judge_model: str = None):\n        \"\"\"Initialize the agent and judge.\"\"\"\n        # Initialize the agent's LLM client\n        provider_kwargs = config.get_provider_kwargs()\n        self.client = get_client(config.provider, **provider_kwargs)\n        self.config = config\n        self.judge_model = judge_model or config.model\n        \n        # Initialize the MLflow judge\n        self._init_judge()\n    \n    def _init_judge(self):\n        \"\"\"\n        Create an MLflow Judge using make_judge().\n        \n        The judge:\n        - Takes evaluation instructions (criteria)\n        - Uses an LLM to perform the evaluation\n        - Returns structured feedback (value + rationale)\n        \"\"\"\n        # Set up environment for Databricks (needed by LiteLLM)\n        if self.config.provider == \"databricks\":\n            os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"DATABRICKS_TOKEN\", \"\")\n            os.environ[\"OPENAI_API_BASE\"] = f\"{self.config.databricks_host}/serving-endpoints\"\n            model_uri = f\"openai:/{self.judge_model}\"\n        else:\n            model_uri = self.judge_model\n        \n        # Create the judge\n        self.judge = make_judge(\n            name=\"tool_selection_quality\",\n            instructions=get_judge_instructions(),\n            feedback_value_type=Literal[\"correct\", \"incorrect\"],\n            model=model_uri\n        )\n    \n    @mlflow.trace(span_type=SpanType.AGENT, name=\"select_tool\")\n    def select_tool(self, user_request: str, available_tools: List[str]) -> str:\n        \"\"\"\n        Agent Action with MLflow Tracing.\n        \n        The @mlflow.trace decorator automatically captures:\n        - Input parameters (user_request, available_tools)\n        - Output (selected tool)\n        - Execution time and metadata\n        \"\"\"\n        # Get the tool selection prompt\n        prompt = get_tool_selection_prompt(user_request, available_tools)\n        \n        # Call the LLM to select a tool\n        api_params = {\n            \"model\": self.config.model,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n            \"max_tokens\": 50\n        }\n        \n        # Add temperature if supported (OpenAI only)\n        if self.config.provider == \"openai\":\n            api_params[\"temperature\"] = 0.0\n        \n        response = self._call_llm(**api_params)\n        tool_selected = response.choices[0].message.content.strip()\n        \n        # Note: No need to log_param here - the trace automatically captures inputs/outputs\n        return tool_selected\n    \n    def evaluate(self, trace_id: str) -> Dict[str, Any]:\n        \"\"\"\n        Evaluate with the Judge.\n        \n        The judge evaluates the trace and returns structured feedback:\n        - feedback.value: \"correct\" or \"incorrect\"\n        - feedback.rationale: Detailed explanation\n        \"\"\"\n        # Fetch the trace from MLflow\n        trace = mlflow.get_trace(trace_id)\n        \n        # Call the judge to evaluate the trace\n        feedback = self.judge(trace=trace)\n        \n        # Return structured result\n        return {\n            \"is_correct\": feedback.value == \"correct\",\n            \"reasoning\": feedback.rationale\n        }\n    \n    @mlflow.trace(span_type=SpanType.LLM, name=\"llm_call\")\n    def _call_llm(self, **api_params):\n        \"\"\"Call LLM with MLflow tracing.\"\"\"\n        return self.client.chat.completions.create(**api_params)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Agent and Judge\n",
    "\n",
    "Create the configuration and instantiate our judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Agent and Judge initialized\n",
      "  Provider: databricks\n",
      "  Agent Model: databricks-gpt-5\n",
      "  Judge Model: databricks-gemini-2-5-flash\n",
      "  Temperature: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Create agent configuration\n",
    "config = AgentConfig(\n",
    "    model=AGENT_MODEL,\n",
    "    provider=PROVIDER,\n",
    "    temperature=TEMPERATURE\n",
    ")\n",
    "\n",
    "# Initialize the judge\n",
    "judge = AgentToolSelectionJudge(config, judge_model=JUDGE_MODEL)\n",
    "\n",
    "print(\"âœ“ Agent and Judge initialized\")\n",
    "print(f\"  Provider: {config.provider}\")\n",
    "print(f\"  Agent Model: {config.model}\")\n",
    "print(f\"  Judge Model: {JUDGE_MODEL}\")\n",
    "print(f\"  Temperature: {config.temperature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Test Scenario\n",
    "\n",
    "Set up a user query and available tools for the agent to choose from.\n",
    "\n",
    "**ðŸ’¡ Try different queries to see how the agent and judge respond!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Scenario:\n",
      "  User Query: What's the weather like in San Francisco?\n",
      "  Available Tools: ['get_weather_api', 'search_web', 'get_calendar', 'send_email']\n"
     ]
    }
   ],
   "source": [
    "# Define the scenario\n",
    "user_request = \"What's the weather like in San Francisco?\"\n",
    "available_tools = [\"get_weather_api\", \"search_web\", \"get_calendar\", \"send_email\"]\n",
    "\n",
    "print(\"Test Scenario:\")\n",
    "print(f\"  User Query: {user_request}\")\n",
    "print(f\"  Available Tools: {available_tools}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Try Different Queries\n",
    "\n",
    "Uncomment one of these examples or write your own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example queries to try:\n",
    "#user_request = \"Send email to John about the meeting\"\n",
    "# user_request = \"What meetings do I have today?\"\n",
    "# user_request = \"Search for information about machine learning\"\n",
    "# user_request = \"What's the current stock price of AAPL?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Agent Performs Action\n",
    "\n",
    "The agent selects a tool based on the user query. MLflow automatically traces this action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent is selecting a tool...\n",
      "\n",
      "âœ“ Agent selected: send_email\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-e77d6522597847039a2cedeb4aeedc13&amp;experiment_id=1&amp;version=3.6.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-e77d6522597847039a2cedeb4aeedc13)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Agent is selecting a tool...\\n\")\n",
    "\n",
    "tool_selected = judge.select_tool(user_request, available_tools)\n",
    "\n",
    "print(f\"âœ“ Agent selected: {tool_selected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Judge Evaluates the Selection\n",
    "\n",
    "Now the judge evaluates whether the agent made the right choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge is evaluating the selection...\n",
      "\n",
      "======================================================================\n",
      "Decision: âœ“ CORRECT\n",
      "\n",
      "Reasoning:\n",
      "The user's request was \"Send email to John about the meeting\". The agent correctly selected the `send_email` tool from the available options: `get_weather_api`, `search_web`, `get_calendar`, `send_email`. This tool directly matches the user's intent and is the most suitable for the task. No more appropriate tools were overlooked.\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-0c0313400682b9ee2822d18b02a37a1a&amp;experiment_id=1&amp;trace_id=tr-c303a3e08eb96efbdec656ffb39a972f&amp;experiment_id=1&amp;trace_id=tr-42d346827b20cc6cc3788eebf07477fa&amp;experiment_id=1&amp;version=3.6.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-0c0313400682b9ee2822d18b02a37a1a), Trace(trace_id=tr-c303a3e08eb96efbdec656ffb39a972f), Trace(trace_id=tr-42d346827b20cc6cc3788eebf07477fa)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Judge is evaluating the selection...\\n\")\n",
    "\n",
    "# Get the trace ID from the agent's action\n",
    "trace_id = mlflow.get_last_active_trace_id()\n",
    "\n",
    "# Evaluate with the judge\n",
    "result = judge.evaluate(trace_id)\n",
    "\n",
    "# Display results\n",
    "print(\"=\" * 70)\n",
    "print(f\"Decision: {'âœ“ CORRECT' if result['is_correct'] else 'âœ— INCORRECT'}\")\n",
    "print(\"\\nReasoning:\")\n",
    "print(f\"{result['reasoning']}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: View Detailed Traces\n",
    "\n",
    "You can view detailed traces in the MLflow UI to see:\n",
    "- The full conversation flow\n",
    "- LLM inputs and outputs\n",
    "- Execution times\n",
    "- All logged parameters\n",
    "\n",
    "Run this command in your terminal:\n",
    "```bash\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "Then navigate to: http://localhost:5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Try It Yourself!\n",
    "\n",
    "Run the complete workflow with a custom query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Query: Send email to John about the meeting\n",
      "Available Tools: ['get_weather_api', 'search_web', 'get_calendar', 'send_email']\n",
      "======================================================================\n",
      "\n",
      "âœ“ Agent selected: send_email\n",
      "\n",
      "Decision: âœ“ CORRECT\n",
      "\n",
      "Reasoning:\n",
      "The user's request was to \"Send email to John about the meeting\". The agent correctly identified and selected the `send_email` tool from the available options. This tool directly aligns with the user's intent and is the most appropriate choice among the given tools (`get_weather_api`, `search_web`, `get_calendar`).\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-ed530ba71ed5014fe5504a9c77b59a7b&amp;experiment_id=1&amp;trace_id=tr-f5ae76d8e6d4f7533df73d02c3557bfe&amp;experiment_id=1&amp;trace_id=tr-d7d4f9e499b0bd377e7cf8e45fab1909&amp;experiment_id=1&amp;trace_id=tr-df42c3f40bda7ffe3414df78dab3d5a8&amp;experiment_id=1&amp;version=3.6.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-ed530ba71ed5014fe5504a9c77b59a7b), Trace(trace_id=tr-f5ae76d8e6d4f7533df73d02c3557bfe), Trace(trace_id=tr-d7d4f9e499b0bd377e7cf8e45fab1909), Trace(trace_id=tr-df42c3f40bda7ffe3414df78dab3d5a8)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_evaluation(query: str, tools: List[str]):\n",
    "    \"\"\"\n",
    "    Complete workflow: Agent selects tool â†’ Judge evaluates\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Available Tools: {tools}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Agent selects tool\n",
    "    selected = judge.select_tool(query, tools)\n",
    "    print(f\"âœ“ Agent selected: {selected}\\n\")\n",
    "    \n",
    "    # Judge evaluates\n",
    "    trace_id = mlflow.get_last_active_trace_id()\n",
    "    result = judge.evaluate(trace_id)\n",
    "    \n",
    "    # Show results\n",
    "    print(f\"Decision: {'âœ“ CORRECT' if result['is_correct'] else 'âœ— INCORRECT'}\")\n",
    "    print(f\"\\nReasoning:\\n{result['reasoning']}\")\n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Try it!\n",
    "result = run_evaluation(\n",
    "    query=\"Send email to John about the meeting\",\n",
    "    tools=[\"get_weather_api\", \"search_web\", \"get_calendar\", \"send_email\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Experiment: Test Multiple Scenarios\n",
    "\n",
    "Let's evaluate multiple queries and see how the judge performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test scenarios\n",
    "test_scenarios = [\n",
    "    (\"What's the weather in Boston?\", [\"get_weather_api\", \"search_web\", \"get_calendar\", \"send_email\"]),\n",
    "    (\"Schedule a meeting for tomorrow\", [\"get_weather_api\", \"search_web\", \"get_calendar\", \"send_email\"]),\n",
    "    (\"Find information about Python\", [\"get_weather_api\", \"search_web\", \"get_calendar\", \"send_email\"]),\n",
    "    (\"Send a message to Sarah\", [\"get_weather_api\", \"search_web\", \"get_calendar\", \"send_email\"]),\n",
    "]\n",
    "\n",
    "# Run all scenarios\n",
    "results = []\n",
    "for query, tools in test_scenarios:\n",
    "    result = run_evaluation(query, tools)\n",
    "    results.append({\n",
    "        \"query\": query,\n",
    "        \"correct\": result[\"is_correct\"]\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "correct_count = sum(1 for r in results if r[\"correct\"])\n",
    "total = len(results)\n",
    "print(f\"\\nðŸ“Š Summary: {correct_count}/{total} selections were correct ({correct_count/total*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¨ Customization\n",
    "\n",
    "### Modify Evaluation Criteria\n",
    "\n",
    "The judge's evaluation criteria are defined in `prompts.py`. You can view them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current Judge Instructions:\")\n",
    "print(\"=\" * 70)\n",
    "print(get_judge_instructions())\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Tool Selection Prompt\n",
    "\n",
    "See how the agent is instructed to select tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = get_tool_selection_prompt(\n",
    "    \"What's the weather?\",\n",
    "    [\"get_weather_api\", \"search_web\"]\n",
    ")\n",
    "\n",
    "print(\"Example Tool Selection Prompt:\")\n",
    "print(\"=\" * 70)\n",
    "print(example_prompt)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Key Concepts Summary\n",
    "\n",
    "### MLflow Tracing\n",
    "- Automatically captures function inputs/outputs\n",
    "- Tracks execution time and metadata\n",
    "- Creates parent-child relationships for nested calls\n",
    "\n",
    "### MLflow Judge\n",
    "- Created with `make_judge()`\n",
    "- Takes predefined evaluation criteria\n",
    "- Returns structured feedback (value + rationale)\n",
    "\n",
    "### Separation of Concerns\n",
    "- **Agent**: Performs the task (tool selection)\n",
    "- **Judge**: Evaluates the agent's performance\n",
    "- **Prompts**: Define behavior (easy to modify in `prompts.py`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Next Steps\n",
    "\n",
    "1. **Explore MLflow UI**: Run `mlflow ui` to see detailed traces\n",
    "2. **Modify Prompts**: Edit `prompts.py` to change evaluation criteria\n",
    "3. **Try Different Models**: Experiment with different agent and judge models\n",
    "4. **Add More Tools**: Expand the `available_tools` list\n",
    "5. **Apply to Your Use Case**: Adapt this pattern for your own agent evaluations\n",
    "\n",
    "## ðŸ“– Resources\n",
    "\n",
    "- [MLflow GenAI Judges Documentation](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html)\n",
    "- [Original Tutorial](https://medium.com/@juanc.olamendy/using-llm-as-a-judge-to-evaluate-agent-outputs-a-comprehensive-tutorial-00b6f1f356cc)\n",
    "- [Tool Selection Judge README](README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}