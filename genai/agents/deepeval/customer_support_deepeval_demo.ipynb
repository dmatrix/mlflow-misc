{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow DeepEval Integration Demo: Multi-Turn Conversation Evaluation\n",
    "\n",
    "This notebook demonstrates MLflow 3.8's DeepEval integration for session-level evaluation:\n",
    "1. Use `mlflow.genai.scorers.deepeval` for industry-standard metrics\n",
    "2. Evaluate multi-turn conversations with DeepEval scorers\n",
    "3. Extract and interpret DeepEval evaluation results\n",
    "\n",
    "**Key Feature**: MLflow 3.8+ provides native integration with DeepEval metrics through `mlflow.genai.scorers.deepeval`, enabling you to use industry-standard conversational AI metrics seamlessly within MLflow's evaluation framework.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to use DeepEval scorers via MLflow integration\n",
    "- How to evaluate session-level conversations with industry metrics\n",
    "- How to extract DeepEval results from evaluation DataFrames\n",
    "- Best practices for multi-turn conversation evaluation with DeepEval\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai.common.config import AgentConfig\n",
    "from genai.agents.multi_turn.customer_support_agent_simple import CustomerSupportAgentSimple\n",
    "from genai.agents.multi_turn.scenarios import get_scenario_account_access, get_scenario_printer_troubleshooting\n",
    "import mlflow\n",
    "from mlflow.genai.scorers.deepeval import (\n",
    "    ConversationCompleteness,\n",
    "    KnowledgeRetention,\n",
    "    TopicAdherence\n",
    ")\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        load_dotenv(env_file)\n",
    "        print(f\"‚úì Loaded environment variables from {env_file.absolute()}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è  No .env file found. Set environment variables manually.\")\n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è  python-dotenv not installed. Set environment variables manually.\")\n",
    "\n",
    "# Convenience for display\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provider and model configuration\n",
    "PROVIDER = \"databricks\"  # or \"openai\"\n",
    "AGENT_MODEL = \"databricks-gpt-5-2\"\n",
    "JUDGE_MODEL = \"databricks-gemini-2-5-flash\"  # Used by DeepEval scorers\n",
    "TEMPERATURE = 1.0\n",
    "EXPERIMENT_NAME = \"customer-support-deepeval-demo\"\n",
    "\n",
    "print(\"‚úì Configuration:\")\n",
    "print(f\"  Provider: {PROVIDER}\")\n",
    "print(f\"  Agent Model: {AGENT_MODEL}\")\n",
    "print(f\"  Judge Model (for DeepEval): {JUDGE_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup MLflow Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.openai.autolog()\n",
    "using_databricks_mlflow = False\n",
    "\n",
    "if using_databricks_mlflow:\n",
    "    mlflow.set_tracking_uri(\"databricks\")\n",
    "    EXPERIMENT_NAME = f\"/Users/your-username/{EXPERIMENT_NAME}\"\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "else:\n",
    "    mlflow.set_tracking_uri(None)\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "print(f\"‚úì MLflow tracking enabled\")\n",
    "print(f\"  Experiment: {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Agent (Conversation-Only)\n",
    "\n",
    "Note: This agent handles conversations only. We'll set up DeepEval evaluation separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AgentConfig(\n",
    "    model=AGENT_MODEL,\n",
    "    provider=PROVIDER,\n",
    "    temperature=TEMPERATURE,\n",
    "    mlflow_experiment=EXPERIMENT_NAME\n",
    ")\n",
    "\n",
    "agent = CustomerSupportAgentSimple(config)\n",
    "\n",
    "print(\"‚úì Agent initialized (conversation-only)\")\n",
    "print(f\"  Provider: {config.provider}\")\n",
    "print(f\"  Model: {config.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure DeepEval Environment\n",
    "\n",
    "DeepEval scorers need to know which model endpoint to use for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure environment for DeepEval to use Databricks/OpenAI endpoints\n",
    "if PROVIDER == \"databricks\":\n",
    "    databricks_host = os.environ.get(\"DATABRICKS_HOST\", \"\")\n",
    "    if databricks_host:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"DATABRICKS_TOKEN\", \"\")\n",
    "        os.environ[\"OPENAI_API_BASE\"] = f\"{databricks_host}/serving-endpoints\"\n",
    "        print(f\"‚úì DeepEval configured for Databricks\")\n",
    "        print(f\"  Endpoint: {databricks_host}\")\n",
    "    judge_model_uri = f\"openai:/{JUDGE_MODEL}\"\n",
    "else:\n",
    "    judge_model_uri = JUDGE_MODEL\n",
    "    print(f\"‚úì DeepEval configured for OpenAI\")\n",
    "\n",
    "print(f\"  Judge Model URI: {judge_model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Conversation Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load account access scenario\n",
    "scenario = get_scenario_account_access()\n",
    "\n",
    "print(f\"Scenario: {scenario['name']}\")\n",
    "print(f\"Description: {scenario['description']}\")\n",
    "print(f\"Turns: {len(scenario['messages'])}\")\n",
    "print(f\"Session ID: {scenario['session_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the conversation\n",
    "# Under the hood, it tags messages with a session id (mlflow.update_current_trace(metadata={\"mlflow.trace.session\": session_id}))\n",
    "conv_result = agent.run_conversation(\n",
    "    messages=scenario['messages'],\n",
    "    session_id=scenario['session_id']\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Conversation completed\")\n",
    "print(f\"  Turns: {conv_result['turns']}\")\n",
    "print(f\"  Session ID: {scenario['session_id']}\")\n",
    "print(f\"\\nView traces: mlflow ui\")\n",
    "if not using_databricks_mlflow:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    print(f\"  http://127.0.0.1:5000/#/experiments/{experiment.experiment_id}/chat-sessions/{scenario['session_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ DeepEval Evaluation Showcase: MLflow Integration\n",
    "\n",
    "Now we'll demonstrate MLflow's DeepEval integration step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create DeepEval Scorers via MLflow Integration\n",
    "\n",
    "**Key MLflow 3.8 Feature**: Use `mlflow.genai.scorers.deepeval` for industry-standard metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating DeepEval scorers...\\n\")\n",
    "\n",
    "# ConversationCompleteness: Evaluates if conversation satisfies user's needs\n",
    "completeness_scorer = ConversationCompleteness(\n",
    "    model=judge_model_uri,\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "# KnowledgeRetention: Assesses ability to retain information across turns\n",
    "knowledge_retention_scorer = KnowledgeRetention(\n",
    "    model=judge_model_uri,\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "# TopicAdherence: Checks if conversation stays on relevant topics\n",
    "topic_adherence_scorer = TopicAdherence(\n",
    "    model=judge_model_uri,\n",
    "    include_reason=True,\n",
    "    relevant_topics=[\"customer support\", \"technical help\", \"account access\"]\n",
    ")\n",
    "\n",
    "print(\"‚úì DeepEval scorers created:\")\n",
    "print(f\"  - ConversationCompleteness (session-level: {completeness_scorer.is_session_level_scorer})\")\n",
    "print(f\"  - KnowledgeRetention (session-level: {knowledge_retention_scorer.is_session_level_scorer})\")\n",
    "print(f\"  - TopicAdherence (session-level: {topic_adherence_scorer.is_session_level_scorer})\")\n",
    "print(\"\\nüí° All DeepEval scorers are session-level by default!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Search for Traces with `mlflow.search_traces()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get experiment and search for traces\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "session_traces = mlflow.search_traces(\n",
    "    locations=[experiment.experiment_id],\n",
    "    filter_string=f\"metadata.`mlflow.trace.session` = '{scenario['session_id']}'\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Found {len(session_traces)} traces for session '{scenario['session_id']}'\")\n",
    "print(f\"  Each trace = 1 conversation turn\")\n",
    "print(f\"\\nTraces overview:\")\n",
    "display(session_traces[[\"request_time\", \"request\", \"response\"]].sort_values(by=\"request_time\", ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate with `mlflow.genai.evaluate()` + DeepEval Scorers\n",
    "\n",
    "**Key MLflow API**: This is where MLflow's DeepEval integration shines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Evaluating conversation with DeepEval scorers...\\n\")\n",
    "\n",
    "with mlflow.start_run(run_name=f\"{scenario['name']} - DeepEval\") as run:\n",
    "    eval_results = mlflow.genai.evaluate(\n",
    "        data=session_traces,\n",
    "        scorers=[\n",
    "            completeness_scorer,\n",
    "            knowledge_retention_scorer,\n",
    "            topic_adherence_scorer\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(\"‚úì DeepEval evaluation complete\")\n",
    "print(f\"  Run ID: {eval_results.run_id}\")\n",
    "if not using_databricks_mlflow:\n",
    "    print(f\"\\nView results:\")\n",
    "    print(f\"  http://localhost:5000/#/experiments/{experiment.experiment_id}/evaluation-runs?selectedRunUuid={eval_results.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Extract DeepEval Results from DataFrame\n",
    "\n",
    "DeepEval scores appear as columns in the result DataFrame with format `MetricName/value` and `MetricName/reason`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = eval_results.result_df\n",
    "\n",
    "print(f\"Result DataFrame shape: {result_df.shape}\")\n",
    "print(f\"\\nDeepEval metric columns:\")\n",
    "deepeval_cols = [col for col in result_df.columns if any(metric in col for metric in ['ConversationCompleteness', 'KnowledgeRetention', 'TopicAdherence'])]\n",
    "for col in deepeval_cols:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Individual Metric Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Extract ConversationCompleteness\n",
    "completeness_score = result_df['ConversationCompleteness/value'].iloc[0] if 'ConversationCompleteness/value' in result_df.columns else None\n",
    "completeness_reason = result_df['ConversationCompleteness/reason'].iloc[0] if 'ConversationCompleteness/reason' in result_df.columns else None\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä ConversationCompleteness\")\n",
    "print(\"=\"*70)\n",
    "if completeness_score is not None and not (isinstance(completeness_score, float) and math.isnan(completeness_score)):\n",
    "    print(f\"Score: {completeness_score}\")\n",
    "    if completeness_reason and not (isinstance(completeness_reason, float) and math.isnan(completeness_reason)):\n",
    "        print(f\"Reason: {completeness_reason}\")\n",
    "else:\n",
    "    print(\"Score: N/A (evaluation may have failed)\")\n",
    "    print(\"üí° Tip: Check that session contains multiple traces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract KnowledgeRetention\n",
    "retention_score = result_df['KnowledgeRetention/value'].iloc[0] if 'KnowledgeRetention/value' in result_df.columns else None\n",
    "retention_reason = result_df['KnowledgeRetention/reason'].iloc[0] if 'KnowledgeRetention/reason' in result_df.columns else None\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üß† KnowledgeRetention\")\n",
    "print(\"=\"*70)\n",
    "if retention_score is not None and not (isinstance(retention_score, float) and math.isnan(retention_score)):\n",
    "    print(f\"Score: {retention_score}\")\n",
    "    if retention_reason and not (isinstance(retention_reason, float) and math.isnan(retention_reason)):\n",
    "        print(f\"Reason: {retention_reason}\")\n",
    "else:\n",
    "    print(\"Score: N/A (evaluation may have failed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract TopicAdherence\n",
    "topic_score = result_df['TopicAdherence/value'].iloc[0] if 'TopicAdherence/value' in result_df.columns else None\n",
    "topic_reason = result_df['TopicAdherence/reason'].iloc[0] if 'TopicAdherence/reason' in result_df.columns else None\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üéØ TopicAdherence\")\n",
    "print(\"=\"*70)\n",
    "if topic_score is not None and not (isinstance(topic_score, float) and math.isnan(topic_score)):\n",
    "    print(f\"Score: {topic_score}\")\n",
    "    if topic_reason and not (isinstance(topic_reason, float) and math.isnan(topic_reason)):\n",
    "        print(f\"Reason: {topic_reason}\")\n",
    "else:\n",
    "    print(\"Score: N/A (evaluation may have failed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Aggregated Metrics\n",
    "\n",
    "MLflow also provides aggregated metrics across all traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üìà Aggregated Metrics (mean across traces)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nMetrics summary:\")\n",
    "for key, value in eval_results.metrics.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä What Just Happened?\n",
    "\n",
    "We just demonstrated the complete MLflow + DeepEval workflow:\n",
    "\n",
    "1. **Created DeepEval Scorers**: Used `mlflow.genai.scorers.deepeval` for industry-standard metrics\n",
    "   - ConversationCompleteness\n",
    "   - KnowledgeRetention\n",
    "   - TopicAdherence\n",
    "\n",
    "2. **Searched Traces**: Used `mlflow.search_traces()` to find all conversation turns\n",
    "\n",
    "3. **Evaluated with DeepEval**: Used `mlflow.genai.evaluate()` with DeepEval scorers\n",
    "\n",
    "4. **Extracted Results**: Retrieved scores and reasoning from the result DataFrame\n",
    "\n",
    "**Key Insights**:\n",
    "- DeepEval scorers integrate seamlessly with MLflow's evaluation framework\n",
    "- All DeepEval conversational metrics are session-level by default\n",
    "- Results include both scores (0.0-1.0) and human-readable reasoning\n",
    "- MLflow automatically aggregates metrics across traces\n",
    "\n",
    "**Benefits over MLflow Native Judges**:\n",
    "- Industry-standard metrics with established definitions\n",
    "- Pre-built conversational AI evaluation metrics\n",
    "- Active DeepEval community and metric library\n",
    "- Consistent scoring across different use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Try Another Scenario\n",
    "\n",
    "Let's evaluate a different conversation to see how DeepEval metrics compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load printer troubleshooting scenario\n",
    "scenario2 = get_scenario_printer_troubleshooting()\n",
    "\n",
    "print(f\"Scenario: {scenario2['name']}\")\n",
    "print(f\"Description: {scenario2['description']}\")\n",
    "print(f\"Session ID: {scenario2['session_id']}\")\n",
    "\n",
    "# Run conversation\n",
    "conv_result2 = agent.run_conversation(\n",
    "    messages=scenario2['messages'],\n",
    "    session_id=scenario2['session_id']\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Conversation completed with {conv_result2['turns']} turns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search traces for second scenario\n",
    "session_traces2 = mlflow.search_traces(\n",
    "    locations=[experiment.experiment_id],\n",
    "    filter_string=f\"metadata.`mlflow.trace.session` = '{scenario2['session_id']}'\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Found {len(session_traces2)} traces for '{scenario2['name']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with DeepEval\n",
    "with mlflow.start_run(run_name=f\"{scenario2['name']} - DeepEval\") as run:\n",
    "    eval_results2 = mlflow.genai.evaluate(\n",
    "        data=session_traces2,\n",
    "        scorers=[\n",
    "            completeness_scorer,\n",
    "            knowledge_retention_scorer,\n",
    "            TopicAdherence(\n",
    "                model=judge_model_uri,\n",
    "                include_reason=True,\n",
    "                relevant_topics=[\"customer support\", \"technical help\", \"printer problems\"]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(\"‚úì Evaluation complete\")\n",
    "print(f\"\\nMetrics summary:\")\n",
    "for key, value in eval_results2.metrics.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "1. **MLflow 3.8+ DeepEval Integration**: Seamlessly use industry-standard metrics within MLflow\n",
    "\n",
    "2. **Session-Level Evaluation**: DeepEval scorers automatically evaluate entire conversations\n",
    "\n",
    "3. **Rich Results**: Get both quantitative scores (0.0-1.0) and qualitative reasoning\n",
    "\n",
    "4. **Flexible Metrics**: Customize scorers (e.g., `relevant_topics` for TopicAdherence)\n",
    "\n",
    "5. **Unified Workflow**: Same `mlflow.genai.evaluate()` API for both native judges and DeepEval scorers\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Next Steps\n",
    "\n",
    "- Explore more DeepEval metrics (ContextualRecall, ContextualPrecision, etc.)\n",
    "- Combine DeepEval scorers with MLflow native judges for hybrid evaluation\n",
    "- Batch evaluate multiple conversation sessions for quality monitoring\n",
    "- Use MLflow UI to compare evaluation results across different scenarios\n",
    "\n",
    "**Documentation**:\n",
    "- [MLflow DeepEval Integration](https://mlflow.org/docs/latest/genai/eval-monitor/scorers/deepeval-scorers/)\n",
    "- [DeepEval Metrics](https://docs.confident-ai.com/)\n",
    "- [MLflow Session Tracking](https://mlflow.org/docs/latest/genai/tracing/track-users-sessions/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
