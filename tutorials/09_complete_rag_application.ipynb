{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: System Architecture\n",
    "\n",
    "### RAG Pipeline Flow\n",
    "\n",
    "![RAG pipleine](images/10_Building-a-Complete-RAG-Application.png)\n",
    "\n",
    "\n",
    "### What Gets Tracked\n",
    "\n",
    "**Experiment Level:**\n",
    "- Model configuration\n",
    "- Prompt versions\n",
    "- Hyperparameters (top_k, temperature)\n",
    "\n",
    "**Run Level:**\n",
    "- Individual queries\n",
    "- Performance metrics\n",
    "- Cost per query\n",
    "\n",
    "**Trace Level:**\n",
    "- Every operation's timing\n",
    "- Inputs and outputs\n",
    "- Custom attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1.9: Complete RAG Application\n",
    "\n",
    "## Building a Production-Ready RAG System with Full Observability\n",
    "\n",
    "Welcome to the final notebook! This brings together everything you've learned to build a complete, production-ready RAG (Retrieval-Augmented Generation) application with comprehensive MLflow tracking and tracing.\n",
    "\n",
    "### What You'll Build\n",
    "\n",
    "A full RAG system that includes:\n",
    "- ‚úÖ Document embedding and indexing\n",
    "- ‚úÖ Semantic search / retrieval\n",
    "- ‚úÖ Context-aware response generation\n",
    "- ‚úÖ Complete experiment tracking\n",
    "- ‚úÖ End-to-end tracing\n",
    "- ‚úÖ Cost monitoring\n",
    "- ‚úÖ Performance metrics\n",
    "- ‚úÖ Error handling\n",
    "- ‚úÖ Caching strategies\n",
    "- ‚úÖ RAG evaluation with RAGAS metrics\n",
    "\n",
    "### Prerequisites\n",
    "- Completed all previous notebooks (1.1-1.8)\n",
    "- Understanding of experiment tracking and tracing\n",
    "\n",
    "### Estimated Time: 25-30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/08 14:14:00 INFO mlflow.tracking.fluent: Experiment with name '11-complete-rag-system' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment configured for production RAG system\n",
      "   MLflow tracking: http://localhost:5000\n",
      "   Experiment: 11-complete-rag-system\n",
      "   Model: gpt-5.2\n",
      "   Judge Model: gpt-5.2\n",
      "   Use Databricks Provider: False\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import hashlib\n",
    "from typing import List, Dict \n",
    "from datetime import datetime\n",
    "from utils.clnt_utils import is_databricks_ai_gateway_client, get_databricks_ai_gateway_client, get_ai_gateway_model_names, get_openai_client\n",
    "\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "EXPERIMENT_NAME = \"11-complete-rag-system\"\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# Check if we are using a Databricks AI Gateway client\n",
    "use_databricks_provider = is_databricks_ai_gateway_client()\n",
    "if use_databricks_provider:\n",
    "    client = get_databricks_ai_gateway_client()\n",
    "    models = get_ai_gateway_model_names()\n",
    "    JUDGE_MODEL = models[2]\n",
    "    AGENT_MODEL = models[0]\n",
    "    JUDGE_MODEL_URI = f\"databricks:/{JUDGE_MODEL}\"\n",
    "else:\n",
    "    # Initialize as an OpenAI client\n",
    "    client = get_openai_client()\n",
    "    JUDGE_MODEL = \"gpt-5.2\"\n",
    "    AGENT_MODEL = \"gpt-5.2\"\n",
    "    JUDGE_MODEL_URI = f\"openai:/{JUDGE_MODEL}\"\n",
    "\n",
    "# Enable autologging\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "print(\"‚úÖ Environment configured for production RAG system\")\n",
    "print(f\"   MLflow tracking: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"   Experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"   Model: {AGENT_MODEL}\")\n",
    "print(f\"   Judge Model: {JUDGE_MODEL}\")\n",
    "print(f\"   Use Databricks Provider: {use_databricks_provider}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Document Store and Embeddings\n",
    "\n",
    "In production, you'd use a vector database like Pinecone, Weaviate, or ChromaDB.\n",
    "For this tutorial, we'll use in-memory storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Document store initialized with 10 documents\n"
     ]
    }
   ],
   "source": [
    "# Sample document corpus (in production, load from database)\n",
    "DOCUMENT_STORE = {  \n",
    "    \"doc1\": \"MLflow is an open source developer platform to build AI applications and models with confidence.\",\n",
    "    \"doc2\": \"MLflow Tracing provides comprehensive observability for GenAI applications. It captures LLM calls, retrieval steps, tool usage, and agent reasoning with full input/output visibility.\",\n",
    "    \"doc3\": \"MLflow integrates with 40+ frameworks including OpenAI, Anthropic, LangChain, LlamaIndex, DSPy, and AutoGen. Each integration provides automatic tracing and experiment tracking.\",\n",
    "    \"doc4\": \"MLflow Evaluation enables systematic testing of GenAI applications using LLM-as-judge metrics, custom scorers, and human feedback. It supports both batch and online evaluation.\",\n",
    "    \"doc5\": \"MLflow Prompt Registry allows teams to version, share, and manage prompts centrally. It tracks which prompts are used in which experiments and enables A/B testing.\",\n",
    "    \"doc6\": \"MLflow supports collaborative development with experiment sharing, model versioning, and deployment tracking. Teams can compare results and iterate systematically.\",\n",
    "    \"doc7\": \"MLflow provides cost tracking for LLM applications by monitoring token usage, API calls, and compute resources. This helps teams optimize spending and budget effectively.\",\n",
    "    \"doc8\": \"MLflow is fully open source and vendor-neutral, ensuring no lock-in. It works with any cloud provider, ML framework, or LLM provider.\",\n",
    "    \"doc9\": \"MLflow is a platform for the complete machine learning lifecycle. It provides experiment tracking, model packaging, and deployment capabilities across various ML frameworks.\",\n",
    "    \"doc10\": \"MLflow offers Helpful Assitant, performaance metrics dashboards, Judge Builder online and continous monitoring, and MemAling for dyanmic optization, and more.\"\n",
    "}\n",
    "print(f\"üìö Document store initialized with {len(DOCUMENT_STORE)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding function defined with caching\n"
     ]
    }
   ],
   "source": [
    "# Memory Cache for embeddings but in production, use Redis or something similar vector database\n",
    "EMBEDDING_CACHE = {}\n",
    "\n",
    "# creating a span for the embedding function\n",
    "@mlflow.trace(name=\"embed_text\", span_type=\"EMBEDDING\")\n",
    "def embed_text(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Generate embeddings with caching.\n",
    "    \"\"\"\n",
    "    # Check cache\n",
    "    cache_key = hashlib.md5(text.encode()).hexdigest()\n",
    "    span = mlflow.get_current_active_span()\n",
    "    if cache_key in EMBEDDING_CACHE:\n",
    "        span.set_attributes({\"cache_hit\": True})\n",
    "        return EMBEDDING_CACHE[cache_key]\n",
    "\n",
    "    span.set_attributes({\"cache_hit\": False, \"text_length\": len(text)})\n",
    "    \n",
    "    # Generate embedding\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=text\n",
    "    )\n",
    "    \n",
    "    embedding = response.data[0].embedding\n",
    "    span.set_attributes({\"embedding_dim\": len(embedding)})\n",
    "    \n",
    "    # Cache for future use\n",
    "    EMBEDDING_CACHE[cache_key] = embedding\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "print(\"‚úÖ Embedding function defined with caching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute embeedings for each doc and add them to the memory cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Computing document embeddings...\n",
      "  ‚úì doc1\n",
      "  ‚úì doc2\n",
      "  ‚úì doc3\n",
      "  ‚úì doc4\n",
      "  ‚úì doc5\n",
      "  ‚úì doc6\n",
      "  ‚úì doc7\n",
      "  ‚úì doc8\n",
      "  ‚úì doc9\n",
      "  ‚úì doc10\n",
      "\n",
      "‚úÖ 10 documents embedded and cached\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-c106b163bd188919d16e3a8933926a32&amp;experiment_id=1&amp;trace_id=tr-8cd448de19b1842d09d611ffc2abcf21&amp;experiment_id=1&amp;trace_id=tr-1ae6f0970ca2197fdbc60c956a25ea35&amp;experiment_id=1&amp;trace_id=tr-e77a2ad4ecf83168a725a58c08849df3&amp;experiment_id=1&amp;trace_id=tr-3e4bb4f364595b3e3a398481403f5316&amp;experiment_id=1&amp;trace_id=tr-680d8ea9abd63ef6df4dfd766e29a643&amp;experiment_id=1&amp;trace_id=tr-e5950a65a4eae91e3e97fd0420ab73ea&amp;experiment_id=1&amp;trace_id=tr-59c83a8e98b1b0477d49452979f2ceab&amp;experiment_id=1&amp;trace_id=tr-b69b79544257a7879dc0272509c92ffd&amp;experiment_id=1&amp;trace_id=tr-9eed4aeb79ba445768e6abd7345f7cf1&amp;experiment_id=1&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-c106b163bd188919d16e3a8933926a32), Trace(trace_id=tr-8cd448de19b1842d09d611ffc2abcf21), Trace(trace_id=tr-1ae6f0970ca2197fdbc60c956a25ea35), Trace(trace_id=tr-e77a2ad4ecf83168a725a58c08849df3), Trace(trace_id=tr-3e4bb4f364595b3e3a398481403f5316), Trace(trace_id=tr-680d8ea9abd63ef6df4dfd766e29a643), Trace(trace_id=tr-e5950a65a4eae91e3e97fd0420ab73ea), Trace(trace_id=tr-59c83a8e98b1b0477d49452979f2ceab), Trace(trace_id=tr-b69b79544257a7879dc0272509c92ffd), Trace(trace_id=tr-9eed4aeb79ba445768e6abd7345f7cf1)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pre-compute document embeddings\n",
    "print(\"\\nüîÑ Computing document embeddings...\")\n",
    "\n",
    "DOC_EMBEDDINGS = {}\n",
    "\n",
    "for doc_id, text in DOCUMENT_STORE.items():\n",
    "    DOC_EMBEDDINGS[doc_id] = embed_text(text)\n",
    "    print(f\"  ‚úì {doc_id}\")\n",
    "\n",
    "print(f\"\\n‚úÖ {len(DOC_EMBEDDINGS)} documents embedded and cached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Query Processing and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Query validation function defined\n"
     ]
    }
   ],
   "source": [
    "@mlflow.trace(name=\"validate_query\", span_type=\"PARSER\")\n",
    "def validate_query(query: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Validate and preprocess user query.\n",
    "    \"\"\"\n",
    "    span = mlflow.get_current_active_span()\n",
    "    span.set_attributes({\"original_length\": len(query)})\n",
    "    \n",
    "    # Basic validation\n",
    "    if not query or len(query.strip()) == 0:\n",
    "        mlflow.log_span_attribute(\"validation_error\", \"empty_query\")\n",
    "        raise ValueError(\"Query cannot be empty\")\n",
    "    \n",
    "    if len(query) > 1000:\n",
    "        span.set_attributes({\"validation_warning\": \"query_too_long\"})\n",
    "        query = query[:1000]  # Truncate\n",
    "    \n",
    "    # Preprocess\n",
    "    processed_query = query.strip()\n",
    "    span.set_attributes({\n",
    "        \"processed_length\": len(processed_query),\n",
    "        \"validation_passed\": True\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"original\": query,\n",
    "        \"processed\": processed_query,\n",
    "        \"valid\": True\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Query validation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Semantic Search with Similarity Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Semantic search function defined\n"
     ]
    }
   ],
   "source": [
    "@mlflow.trace(name=\"semantic_search\", span_type=\"RETRIEVER\")\n",
    "def search_documents(\n",
    "    query_embedding: List[float],\n",
    "    doc_embeddings: Dict[str, List[float]],\n",
    "    top_k: int = 3,\n",
    "    min_score: float = 0.7\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search for most relevant documents using cosine similarity.\n",
    "    \"\"\"\n",
    "    span = mlflow.get_current_active_span()\n",
    "    span.set_attributes({\n",
    "        \"corpus_size\": len(doc_embeddings),\n",
    "        \"top_k\": top_k,\n",
    "        \"min_score\": min_score\n",
    "    })\n",
    "    \n",
    "    # Calculate cosine similarity for all documents\n",
    "    scores = {}\n",
    "    for doc_id, doc_emb in doc_embeddings.items():\n",
    "        similarity = np.dot(query_embedding, doc_emb) / \\\n",
    "                     (np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb))\n",
    "        scores[doc_id] = float(similarity)\n",
    "    \n",
    "    # Sort by score and filter by minimum threshold\n",
    "    sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    filtered_docs = [(doc_id, score) for doc_id, score in sorted_docs if score >= min_score]\n",
    "    \n",
    "    # Get top k\n",
    "    top_docs = filtered_docs[:top_k]\n",
    "    \n",
    "    # Return in LangChain Document format (page_content + metadata)\n",
    "    # MLflow's RAGAS integration expects this format in RETRIEVER span outputs\n",
    "    results = [\n",
    "        {\n",
    "            \"page_content\": DOCUMENT_STORE[doc_id],\n",
    "            \"metadata\": {\"doc_id\": doc_id, \"score\": score}\n",
    "        }\n",
    "        for doc_id, score in top_docs\n",
    "    ]\n",
    "    \n",
    "    # Log metrics\n",
    "    if results:\n",
    "        span.set_attributes({\n",
    "            \"num_results\": len(results),\n",
    "            \"top_score\": results[0][\"metadata\"][\"score\"],\n",
    "            \"avg_score\": np.mean([r[\"metadata\"][\"score\"] for r in results]),\n",
    "            \"min_result_score\": results[-1][\"metadata\"][\"score\"]\n",
    "        })\n",
    "    else:\n",
    "        span.set_attributes({\"num_results\": 0,\n",
    "                             \"retrieval_warning\": \"no_docs_above_threshold\"})\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Semantic search function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Context Assembly and Prompt Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Context assembly function defined\n"
     ]
    }
   ],
   "source": [
    "@mlflow.trace(name=\"assemble_context\", span_type=\"PARSER\")\n",
    "def assemble_context(query: str, retrieved_docs: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Assemble context from retrieved documents and construct prompt.\n",
    "    \"\"\"\n",
    "    span = mlflow.get_current_active_span()\n",
    "    span.set_attributes({\"num_docs\": len(retrieved_docs)})\n",
    "    \n",
    "    if not retrieved_docs:\n",
    "        span.set_attributes({\"context_warning\": \"no_docs_retrieved\"})\n",
    "        return None\n",
    "    \n",
    "    # Format context\n",
    "    context_parts = []\n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        context_parts.append(\n",
    "            f\"[Document {i}] (Relevance: {doc['metadata']['score']:.2f})\\n{doc['page_content']}\"\n",
    "        )\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Construct prompt with system instructions\n",
    "    prompt = f\"\"\"You are a helpful AI assistant that answers questions based on provided context.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Answer the question using ONLY the information in the context below\n",
    "- If the answer is not in the context, say \"I don't have enough information to answer that\"\n",
    "- Be concise but complete\n",
    "- Cite which document(s) you used if relevant\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "    span.set_attributes({\n",
    "        \"context_length\": len(context),\n",
    "        \"prompt_length\": len(prompt)\n",
    "    })\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "print(\"‚úÖ Context assembly function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: LLM Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Answer generation function defined\n"
     ]
    }
   ],
   "source": [
    "@mlflow.trace(name=\"generate_response\", span_type=\"LLM\")\n",
    "def generate_answer(\n",
    "    prompt: str,\n",
    "    model: str = \"gpt-5.2\",\n",
    "    temperature: float = 0.1,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Generate answer using LLM.\n",
    "    \"\"\"\n",
    "    span = mlflow.get_current_active_span()\n",
    "    span.set_attributes({\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature\n",
    "    })\n",
    "    \n",
    "    if not prompt:\n",
    "        span.set_attributes({\"generation_error\": \"empty_prompt\"})\n",
    "        raise ValueError(\"Prompt cannot be empty\")\n",
    "    \n",
    "    # Call LLM (automatically traced by OpenAI autolog)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    # Log generation metrics\n",
    "    span.set_attributes({\n",
    "        \"answer_length\": len(answer),\n",
    "        \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "        \"completion_tokens\": response.usage.completion_tokens,\n",
    "        \"total_tokens\": response.usage.total_tokens,\n",
    "        \"finish_reason\": response.choices[0].finish_reason\n",
    "    })\n",
    "    span.set_attributes({\n",
    "        \"finish_reason\": response.choices[0].finish_reason\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"tokens\": response.usage.total_tokens,\n",
    "        \"model\": model,\n",
    "        \"finish_reason\": response.choices[0].finish_reason\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Answer generation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Response Validation and Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Response validation function defined\n"
     ]
    }
   ],
   "source": [
    "@mlflow.trace(name=\"validate_response\", span_type=\"PARSER\")\n",
    "def validate_response(answer: str, min_length: int = 10) -> Dict:\n",
    "    \"\"\"\n",
    "    Validate and post-process generated response.\n",
    "    \"\"\"\n",
    "    span = mlflow.get_current_active_span()\n",
    "    span.set_attributes({\n",
    "        \"min_length_threshold\": min_length,\n",
    "        \"answer_length\": len(answer)\n",
    "    })\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # Check length\n",
    "    if len(answer) < min_length:\n",
    "        issues.append(\"too_short\")\n",
    "    \n",
    "    # Check for common error patterns\n",
    "    error_patterns = [\n",
    "        \"I don't have enough information\",\n",
    "        \"I cannot answer\",\n",
    "        \"I apologize\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in error_patterns:\n",
    "        if pattern.lower() in answer.lower():\n",
    "            issues.append(f\"contains_{pattern.replace(' ', '_').lower()}\")\n",
    "    \n",
    "    span.set_attributes({\n",
    "        \"validation_issues\": \",\".join(issues) if issues else \"none\",\n",
    "        \"is_valid\": len(issues) == 0\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"is_valid\": len(issues) == 0,\n",
    "        \"issues\": issues,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Response validation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Complete RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Complete RAG pipeline defined\n"
     ]
    }
   ],
   "source": [
    "@mlflow.trace(name=\"rag_pipeline\", span_type=\"CHAIN\")\n",
    "def rag_qa_system(\n",
    "    user_query: str,\n",
    "    top_k: int = 3,\n",
    "    min_score: float = 0.7,\n",
    "    model: str = \"gpt-5.2\",\n",
    "    temperature: float = 0.1\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline with full observability.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with answer, metadata, and status\n",
    "    \"\"\"\n",
    "    # Start MLflow run for tracking\n",
    "    with mlflow.start_run(run_name=\"rag_query\"):\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_params({\n",
    "            \"query\": user_query[:100],  # Truncate for readability\n",
    "            \"top_k\": top_k,\n",
    "            \"min_score\": min_score,\n",
    "            \"model\": model,\n",
    "            \"temperature\": temperature,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Validate query\n",
    "            validation_result = validate_query(user_query)\n",
    "            processed_query = validation_result[\"processed\"]\n",
    "            \n",
    "            # Step 2: Generate query embedding\n",
    "            query_embedding = embed_text(processed_query)\n",
    "            \n",
    "            # Step 3: Search documents\n",
    "            retrieved_docs = search_documents(\n",
    "                query_embedding,\n",
    "                DOC_EMBEDDINGS,\n",
    "                top_k=top_k,\n",
    "                min_score=min_score\n",
    "            )\n",
    "            \n",
    "            if not retrieved_docs:\n",
    "                mlflow.log_metric(\"retrieval_success\", 0)\n",
    "                return {\n",
    "                    \"status\": \"no_relevant_docs\",\n",
    "                    \"answer\": \"I couldn't find relevant information to answer your question.\",\n",
    "                    \"query\": user_query,\n",
    "                    \"retrieved_docs\": []\n",
    "                }\n",
    "            \n",
    "            mlflow.log_metric(\"num_docs_retrieved\", len(retrieved_docs))\n",
    "            mlflow.log_metric(\"avg_relevance_score\", \n",
    "                            np.mean([d[\"metadata\"][\"score\"] for d in retrieved_docs]))\n",
    "            mlflow.log_metric(\"retrieval_success\", 1)\n",
    "            \n",
    "            # Step 4: Assemble context and construct prompt\n",
    "            prompt = assemble_context(processed_query, retrieved_docs)\n",
    "            \n",
    "            # Step 5: Generate answer\n",
    "            generation_result = generate_answer(\n",
    "                prompt,\n",
    "                model=model,\n",
    "                temperature=temperature\n",
    "            )\n",
    "            \n",
    "            # Step 6: Validate response\n",
    "            validation = validate_response(generation_result[\"answer\"])\n",
    "            \n",
    "            # Log final metrics\n",
    "            mlflow.log_metric(\"total_tokens\", generation_result[\"tokens\"])\n",
    "            mlflow.log_metric(\"answer_valid\", 1 if validation[\"is_valid\"] else 0)\n",
    "            mlflow.log_metric(\"answer_length\", len(generation_result[\"answer\"]))\n",
    "            \n",
    "            # Log artifacts\n",
    "            mlflow.log_text(user_query, \"query.txt\")\n",
    "            mlflow.log_text(generation_result[\"answer\"], \"answer.txt\")\n",
    "            mlflow.log_text(prompt, \"full_prompt.txt\")\n",
    "            mlflow.log_dict(\n",
    "                {\"docs\": retrieved_docs},\n",
    "                \"retrieved_docs.json\"\n",
    "            )\n",
    "            \n",
    "            # Construct result\n",
    "            result = {\n",
    "                \"status\": \"success\",\n",
    "                \"query\": user_query,\n",
    "                \"answer\": generation_result[\"answer\"],\n",
    "                \"retrieved_docs\": retrieved_docs,\n",
    "                \"metadata\": {\n",
    "                    \"num_docs\": len(retrieved_docs),\n",
    "                    \"avg_relevance\": float(np.mean([d[\"metadata\"][\"score\"] for d in retrieved_docs])),\n",
    "                    \"tokens_used\": generation_result[\"tokens\"],\n",
    "                    \"model\": model,\n",
    "                    \"is_valid\": validation[\"is_valid\"],\n",
    "                    \"validation_issues\": validation[\"issues\"]\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Log error\n",
    "            mlflow.log_param(\"error_type\", type(e).__name__)\n",
    "            mlflow.log_param(\"error_message\", str(e))\n",
    "            mlflow.log_metric(\"pipeline_success\", 0)\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"query\": user_query,\n",
    "                \"error\": str(e),\n",
    "                \"error_type\": type(e).__name__\n",
    "            }\n",
    "\n",
    "print(\"‚úÖ Complete RAG pipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: Testing the RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing RAG System\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Query 1: What tracing capabilities does MLflow provide?\n",
      "--------------------------------------------------------------------------------\n",
      "üèÉ View run rag_query at: http://localhost:5000/#/experiments/1/runs/5e2bdbf983b6452caa084b4728ee7c4c\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/1\n",
      "\n",
      "Answer: MLflow Tracing provides observability for GenAI applications by capturing **LLM calls, retrieval steps, tool usage, and agent reasoning**, with **full input/output visibility**. (Document 1)\n",
      "\n",
      "Metadata:\n",
      "  - Documents used: 1\n",
      "  - Avg relevance: 0.730\n",
      "  - Tokens: 181\n",
      "  - Latency: 1.64s\n",
      "  - Valid: True\n",
      "\n",
      "Query 2: How does MLflow help with cost tracking?\n",
      "--------------------------------------------------------------------------------\n",
      "üèÉ View run rag_query at: http://localhost:5000/#/experiments/1/runs/5ec49cf25c7d4210aaaf9ace33b8c1a9\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/1\n",
      "\n",
      "Answer: MLflow helps with cost tracking for LLM applications by monitoring **token usage, API calls, and compute resources**, enabling teams to **optimize spending and budget effectively**. (Document 1)\n",
      "\n",
      "Metadata:\n",
      "  - Documents used: 1\n",
      "  - Avg relevance: 0.749\n",
      "  - Tokens: 177\n",
      "  - Latency: 1.70s\n",
      "  - Valid: True\n",
      "\n",
      "Query 3: Can MLflow integrate with LangChain?\n",
      "--------------------------------------------------------------------------------\n",
      "üèÉ View run rag_query at: http://localhost:5000/#/experiments/1/runs/685c102c657c4dc79dc57f6b5758277a\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/1\n",
      "\n",
      "Error: Unknown error\n",
      "\n",
      "Query 4: What is the purpose of MLflow Prompt Registry?\n",
      "--------------------------------------------------------------------------------\n",
      "üèÉ View run rag_query at: http://localhost:5000/#/experiments/1/runs/4fc664fa78ec4a4bbe0be649dea0b9db\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/1\n",
      "\n",
      "Answer: MLflow Prompt Registry is used to **version, share, and centrally manage prompts**, while also **tracking which prompts are used in which experiments** and **enabling A/B testing**. (Document 1)\n",
      "\n",
      "Metadata:\n",
      "  - Documents used: 1\n",
      "  - Avg relevance: 0.772\n",
      "  - Tokens: 182\n",
      "  - Latency: 1.93s\n",
      "  - Valid: True\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ All queries processed!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-f2719003f4351aa5bc302c6afee70816&amp;experiment_id=1&amp;trace_id=tr-41e7296cf58f9b7dac8154b12cb02fc3&amp;experiment_id=1&amp;trace_id=tr-9b9aac05ba976832e955e0feb41994d8&amp;experiment_id=1&amp;trace_id=tr-431f6318cb33f29e2b5e0fbc94428d6a&amp;experiment_id=1&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-f2719003f4351aa5bc302c6afee70816), Trace(trace_id=tr-41e7296cf58f9b7dac8154b12cb02fc3), Trace(trace_id=tr-9b9aac05ba976832e955e0feb41994d8), Trace(trace_id=tr-431f6318cb33f29e2b5e0fbc94428d6a)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What tracing capabilities does MLflow provide?\",\n",
    "    \"How does MLflow help with cost tracking?\",\n",
    "    \"Can MLflow integrate with LangChain?\",\n",
    "    \"What is the purpose of MLflow Prompt Registry?\",\n",
    "]\n",
    "\n",
    "print(\"\\nüß™ Testing RAG System\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nQuery {i}: {query}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = rag_qa_system(query, top_k=3, min_score=0.7)\n",
    "    latency = time.time() - start_time\n",
    "    \n",
    "    if result[\"status\"] == \"success\":\n",
    "        print(f\"\\nAnswer: {result['answer']}\")\n",
    "        print(\"\\nMetadata:\")\n",
    "        print(f\"  - Documents used: {result['metadata']['num_docs']}\")\n",
    "        print(f\"  - Avg relevance: {result['metadata']['avg_relevance']:.3f}\")\n",
    "        print(f\"  - Tokens: {result['metadata']['tokens_used']}\")\n",
    "        print(f\"  - Latency: {latency:.2f}s\")\n",
    "        print(f\"  - Valid: {result['metadata']['is_valid']}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"success\": True,\n",
    "            \"latency\": latency,\n",
    "            \"tokens\": result['metadata']['tokens_used']\n",
    "        })\n",
    "    else:\n",
    "        print(f\"\\nError: {result.get('error', 'Unknown error')}\")\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"success\": False,\n",
    "            \"latency\": latency\n",
    "        })\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\n‚úÖ All queries processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11: Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Performance Summary\n",
      "\n",
      "============================================================\n",
      "Success Rate: 3/4 (75.0%)\n",
      "\n",
      "Latency Stats:\n",
      "  Average: 1.76s\n",
      "  Min: 1.64s\n",
      "  Max: 1.93s\n",
      "  Std Dev: 0.13s\n",
      "\n",
      "Token Usage:\n",
      "  Average: 180 tokens\n",
      "  Total: 540 tokens\n",
      "  Est. Cost: $0.000081\n",
      "\n",
      "Cache Performance:\n",
      "  Embedding cache hits: 14 embeddings cached\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze performance\n",
    "print(\"\\nüìä Performance Summary\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "successful = [r for r in results if r[\"success\"]]\n",
    "\n",
    "if successful:\n",
    "    latencies = [r[\"latency\"] for r in successful]\n",
    "    tokens = [r[\"tokens\"] for r in successful]\n",
    "    \n",
    "    print(f\"Success Rate: {len(successful)}/{len(results)} ({len(successful)/len(results)*100:.1f}%)\")\n",
    "    print(\"\\nLatency Stats:\")\n",
    "    print(f\"  Average: {np.mean(latencies):.2f}s\")\n",
    "    print(f\"  Min: {np.min(latencies):.2f}s\")\n",
    "    print(f\"  Max: {np.max(latencies):.2f}s\")\n",
    "    print(f\"  Std Dev: {np.std(latencies):.2f}s\")\n",
    "    \n",
    "    print(\"\\nToken Usage:\")\n",
    "    print(f\"  Average: {np.mean(tokens):.0f} tokens\")\n",
    "    print(f\"  Total: {np.sum(tokens):.0f} tokens\")\n",
    "    print(f\"  Est. Cost: ${np.sum(tokens) * 0.15 / 1_000_000:.6f}\")\n",
    "    \n",
    "    print(\"\\nCache Performance:\")\n",
    "    print(f\"  Embedding cache hits: {len(EMBEDDING_CACHE)} embeddings cached\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 12: RAG Evaluation with RAGAS Metrics\n",
    "\n",
    "Now let's evaluate our RAG system using RAGAS (Retrieval-Augmented Generation Assessment) metrics. These metrics help assess the quality of both retrieval and generation.\n",
    "\n",
    "| Metric | What It Measures | Requirements |\n",
    "|--------|-----------------|--------------|\n",
    "| **Faithfulness** | Is the answer grounded in the retrieved context? | Traces with RETRIEVER spans |\n",
    "| **ContextRelevance** | Is the retrieved context relevant to the query? | Traces with RETRIEVER spans |\n",
    "| **ContextPrecision** | Are retrieved docs ranked by relevance to expected output? | `expectations['expected_output']` |\n",
    "\n",
    "**Important:** RAGAS scorers extract context from **traces with RETRIEVER spans**, not from static datasets. This is why we search for traces from our RAG pipeline rather than constructing a manual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - LiteLLM async logging disabled\n",
      "‚úÖ Jupyter async compatibility configured\n",
      "   - nest_asyncio applied for nested event loop support\n",
      "   - Asyncio error logging suppressed\n"
     ]
    }
   ],
   "source": [
    "# Workaround for async event loop issues in Jupyter notebooks\n",
    "# Libraries like RAGAS run async code that can conflict with Jupyter's event loop\n",
    "\n",
    "import logging\n",
    "\n",
    "# 1. Use nest_asyncio to allow nested event loops (essential for Jupyter + async libraries)\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 2. Suppress noisy asyncio error messages (they don't affect results)\n",
    "logging.getLogger(\"asyncio\").setLevel(logging.CRITICAL)\n",
    "\n",
    "# 3. If litellm is installed, disable its async logging to prevent conflicts\n",
    "try:\n",
    "    import litellm\n",
    "    litellm.success_callback = []\n",
    "    litellm.failure_callback = []\n",
    "    litellm._async_success_callback = []\n",
    "    litellm._async_failure_callback = []\n",
    "    litellm.disable_streaming_logging = True\n",
    "    litellm.turn_off_message_logging = True\n",
    "    logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n",
    "    print(\"   - LiteLLM async logging disabled\")\n",
    "except ImportError:\n",
    "    pass  # litellm not installed, no workaround needed\n",
    "\n",
    "print(\"‚úÖ Jupyter async compatibility configured\")\n",
    "print(\"   - nest_asyncio applied for nested event loop support\")\n",
    "print(\"   - Asyncio error logging suppressed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAGAS scorers initialized:\n",
      "   - Faithfulness (checks if answer is grounded in retrieved context)\n",
      "   - ContextRelevance (checks if retrieved context is relevant to query)\n",
      "\n",
      "‚ö†Ô∏è  ContextPrecision not used - it requires expected_output in expectations\n"
     ]
    }
   ],
   "source": [
    "from mlflow.genai.scorers.ragas import Faithfulness, ContextRelevance\n",
    "\n",
    "\n",
    "# Initialize RAGAS scorers (requires: pip install ragas)\n",
    "# Note: We're using Faithfulness and ContextRelevance which work with traces containing RETRIEVER spans\n",
    "# ContextPrecision is not included because it requires expectations['expected_output']\n",
    "\n",
    "faithfulness_scorer = Faithfulness(model=JUDGE_MODEL_URI)\n",
    "context_relevance_scorer = ContextRelevance(model=JUDGE_MODEL_URI)\n",
    "\n",
    "print(\"‚úÖ RAGAS scorers initialized:\")\n",
    "print(\"   - Faithfulness (checks if answer is grounded in retrieved context)\")\n",
    "print(\"   - ContextRelevance (checks if retrieved context is relevant to query)\")\n",
    "print(\"\\n‚ö†Ô∏è  ContextPrecision not used - it requires expected_output in expectations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mg/tr3ld7vx6gq1ym7p48vjgmpc0000gp/T/ipykernel_95909/2815135260.py:9: FutureWarning: Parameter 'experiment_ids' is deprecated. Please use 'locations' instead.\n",
      "  rag_traces = mlflow.search_traces(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 4 RAG pipeline traces for evaluation\n",
      "\n",
      "These traces contain RETRIEVER spans that RAGAS scorers need:\n",
      "   - semantic_search span with retrieved documents\n",
      "   - Full input/output flow for faithfulness checking\n",
      "\n",
      "   Retriever output keys: ['page_content', 'metadata']\n",
      "   Has page_content: True\n"
     ]
    }
   ],
   "source": [
    "# Get traces from the RAG pipeline runs (which contain RETRIEVER spans)\n",
    "# RAGAS scorers like Faithfulness and ContextRelevance extract context \n",
    "# from RETRIEVER spans in traces, not from static datasets\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "# Search for traces from our RAG pipeline, limited to the most recent batch\n",
    "# (avoids picking up old traces that may have different output formats)\n",
    "rag_traces = mlflow.search_traces(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    filter_string=\"name = 'rag_pipeline'\",\n",
    "    max_results=len(test_queries),\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Found {len(rag_traces)} RAG pipeline traces for evaluation\")\n",
    "print(\"\\nThese traces contain RETRIEVER spans that RAGAS scorers need:\")\n",
    "print(\"   - semantic_search span with retrieved documents\")\n",
    "print(\"   - Full input/output flow for faithfulness checking\")\n",
    "\n",
    "# Verify traces have the required page_content field in RETRIEVER outputs\n",
    "from mlflow.entities import Trace as TraceEntity\n",
    "sample_trace = rag_traces.iloc[0]['trace']\n",
    "if isinstance(sample_trace, str):\n",
    "    sample_trace = TraceEntity.from_json(sample_trace)\n",
    "retriever_spans = [s for s in sample_trace.data.spans if s.span_type == 'RETRIEVER']\n",
    "if retriever_spans:\n",
    "    outputs = retriever_spans[0].outputs\n",
    "    if isinstance(outputs, list) and len(outputs) > 0 and isinstance(outputs[0], dict):\n",
    "        has_page_content = 'page_content' in outputs[0]\n",
    "        print(f\"\\n   Retriever output keys: {list(outputs[0].keys())}\")\n",
    "        print(f\"   Has page_content: {has_page_content}\")\n",
    "        if not has_page_content:\n",
    "            print(\"\\n‚ö†Ô∏è  WARNING: Traces missing 'page_content' in RETRIEVER outputs!\")\n",
    "            print(\"   Re-run from Step 5 to regenerate traces with the correct format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation on the traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/08 14:15:21 INFO mlflow.models.evaluation.utils.trace: Auto tracing is temporarily enabled during the model evaluation for computing some metrics and debugging. To disable tracing, call `mlflow.autolog(disable=True)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Running RAGAS evaluation on RAG traces...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a632adce464fa8b7dfebf7ea43c0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/4 [Elapsed: 00:00, Remaining: ?] "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/08 14:15:21 ERROR mlflow.genai.scorers.ragas: Missing required parameter for RAGAS metric ContextRelevance: RAGAS metric 'ContextRelevance' requires 'trace with retrieval spans' parameter, which is missing. \n",
      "Make sure your trace includes retrieval spans. Example: use @mlflow.trace(span_type=SpanType.RETRIEVER) decorator\n",
      "2026/02/08 14:15:21 ERROR mlflow.genai.scorers.ragas: Missing required parameter for RAGAS metric Faithfulness: RAGAS metric 'Faithfulness' requires 'trace with retrieval spans' parameter, which is missing. \n",
      "Make sure your trace includes retrieval spans. Example: use @mlflow.trace(span_type=SpanType.RETRIEVER) decorator\n",
      "/Users/jules/git-repos/mlflow-misc/.venv/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_worker.py:75: RuntimeWarning: coroutine 'Logging.async_success_handler' was never awaited\n",
      "  self._queue = None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <title>Evaluation output</title>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <style>\n",
       "        body {\n",
       "            font-family: Arial, sans-serif;\n",
       "        }\n",
       "\n",
       "        .header {\n",
       "            a.button {\n",
       "                padding: 4px 8px;\n",
       "                line-height: 20px;\n",
       "                box-shadow: none;\n",
       "                height: 20px;\n",
       "                display: inline-flex;\n",
       "                align-items: center;\n",
       "                justify-content: center;\n",
       "                vertical-align: middle;\n",
       "                background-color: rgb(34, 114, 180);\n",
       "                color: rgb(255, 255, 255);\n",
       "                text-decoration: none;\n",
       "                animation-duration: 0s;\n",
       "                transition: none 0s ease 0s;\n",
       "                position: relative;\n",
       "                white-space: nowrap;\n",
       "                text-align: center;\n",
       "                border: 1px solid rgb(192, 205, 216);\n",
       "                cursor: pointer;\n",
       "                user-select: none;\n",
       "                touch-action: manipulation;\n",
       "                border-radius: 4px;\n",
       "                gap: 6px;\n",
       "            }\n",
       "\n",
       "            a.button:hover {\n",
       "                background-color: rgb(14, 83, 139) !important;\n",
       "                border-color: transparent !important;\n",
       "                color: rgb(255, 255, 255) !important;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .warnings-section {\n",
       "            margin-top: 8px;\n",
       "\n",
       "            ul {\n",
       "                list-style-type: none;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .instructions-section {\n",
       "            margin-top: 16px;\n",
       "            font-size: 14px;\n",
       "\n",
       "            ul {\n",
       "                margin-top: 0;\n",
       "                margin-bottom: 0;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        code {\n",
       "            font-family: monospace;\n",
       "        }\n",
       "\n",
       "        .note {\n",
       "            color: #666;\n",
       "        }\n",
       "\n",
       "        a {\n",
       "            color: #2272B4;\n",
       "            text-decoration: none;\n",
       "        }\n",
       "\n",
       "        a:hover {\n",
       "            color: #005580;\n",
       "        }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "    <div class=\"header\">\n",
       "        <a href=\"http://localhost:5000/#/experiments/1/evaluation-runs?selectedRunUuid=7afaabfc582e441296f5787b29bad983\" class=\"button\">\n",
       "            View evaluation results in MLflow\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1em\" height=\"1em\" fill=\"none\" viewBox=\"0 0 16 16\" aria-hidden=\"true\" focusable=\"false\" class=\"\">\n",
       "                <path fill=\"currentColor\" d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"></path>\n",
       "                <path fill=\"currentColor\" d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"></path>\n",
       "            </svg>\n",
       "        </a>\n",
       "    </div>\n",
       "</div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ RAGAS evaluation complete!\n",
      "\n",
      "üìä RAGAS Metrics Summary:\n",
      "--------------------------------------------------\n",
      "  ContextRelevance/mean: 1.000\n",
      "  Faithfulness/mean: 0.944\n"
     ]
    }
   ],
   "source": [
    "# Run RAGAS evaluation on traces (which contain RETRIEVER spans)\n",
    "# Note: ContextPrecision requires expected_output in expectations, so we skip it here\n",
    "# To use ContextPrecision, you'd need to log expectations to traces:\n",
    "#   mlflow.log_expectation(trace_id, name='expected_output', value='...', source='...')\n",
    "\n",
    "print(\"üîÑ Running RAGAS evaluation on RAG traces...\\n\")\n",
    "\n",
    "ragas_results = mlflow.genai.evaluate(\n",
    "    data=rag_traces,  # Pass traces (not static data) - they contain RETRIEVER spans\n",
    "    scorers=[\n",
    "        faithfulness_scorer,      # Checks if answer is grounded in retrieved context\n",
    "        context_relevance_scorer, # Checks if retrieved context is relevant to query\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ RAGAS evaluation complete!\")\n",
    "print(\"\\nüìä RAGAS Metrics Summary:\")\n",
    "print(\"-\" * 50)\n",
    "for metric_name, value in ragas_results.metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric_name}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {metric_name}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 13: Viewing Results in MLflow UI\n",
    "\n",
    "Now let's explore what was tracked in MLflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 12: Viewing Results in MLflow UI\n",
    "\n",
    "Now let's explore what was tracked in MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
      "‚ïë         Analyzing Results in MLflow UI                       ‚ïë\n",
      "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
      "\n",
      "üîç EXPERIMENTS VIEW:\n",
      "   Navigate to: http://localhost:5000\n",
      "   Select: \"10-complete-rag-system\" experiment\n",
      "\n",
      "   You'll see:\n",
      "   - All RAG query runs\n",
      "   - Parameters (query, model, top_k)\n",
      "   - Metrics (tokens, relevance, latency)\n",
      "   - Artifacts (queries, answers, prompts)\n",
      "\n",
      "üìä COMPARING RUNS:\n",
      "   1. Select multiple runs\n",
      "   2. Click \"Compare\"\n",
      "   3. View side-by-side:\n",
      "      - Which queries used most tokens?\n",
      "      - Which had highest relevance scores?\n",
      "      - Performance variations\n",
      "\n",
      "üå≥ TRACES VIEW:\n",
      "   Click \"Traces\" tab to see:\n",
      "\n",
      "   Timeline visualization:\n",
      "   rag_pipeline (CHAIN) ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2.5s\n",
      "   ‚îú‚îÄ validate_query (PARSER) ‚îÅ‚îÅ 0.01s\n",
      "   ‚îú‚îÄ embed_text (EMBEDDING) ‚îÅ‚îÅ‚îÅ‚îÅ 0.3s\n",
      "   ‚îú‚îÄ semantic_search (RETRIEVER) ‚îÅ 0.05s\n",
      "   ‚îú‚îÄ assemble_context (PARSER) ‚îÅ 0.02s\n",
      "   ‚îú‚îÄ generate_response (LLM) ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2.0s\n",
      "   ‚îÇ  ‚îî‚îÄ OpenAI API call ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.9s\n",
      "   ‚îî‚îÄ validate_response (PARSER) ‚îÅ 0.01s\n",
      "\n",
      "üîé SPAN DETAILS:\n",
      "   Click on any span to see:\n",
      "   - Inputs and outputs\n",
      "   - Custom attributes\n",
      "   - Timing information\n",
      "   - Cache hit status\n",
      "   - Relevance scores\n",
      "\n",
      "üìà KEY INSIGHTS:\n",
      "   1. Performance Bottlenecks:\n",
      "      - Which step takes longest?\n",
      "      - Is it the LLM or retrieval?\n",
      "\n",
      "   2. Quality Metrics:\n",
      "      - Average relevance scores\n",
      "      - Documents per query\n",
      "      - Answer validation rates\n",
      "\n",
      "   3. Cost Analysis:\n",
      "      - Token usage per query\n",
      "      - Cache effectiveness\n",
      "      - Cost per operation\n",
      "\n",
      "   4. Error Patterns:\n",
      "      - Failed queries\n",
      "      - Low relevance scores\n",
      "      - Validation issues\n",
      "\n",
      "üí° OPTIMIZATION OPPORTUNITIES:\n",
      "   Based on traces, you can:\n",
      "   - Adjust top_k if retrieval is slow\n",
      "   - Increase min_score if quality is poor\n",
      "   - Optimize prompts to reduce tokens\n",
      "   - Add more aggressive caching\n",
      "   - Implement parallel retrieval\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë         Analyzing Results in MLflow UI                       ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "üîç EXPERIMENTS VIEW:\n",
    "   Navigate to: http://localhost:5000\n",
    "   Select: \"10-complete-rag-system\" experiment\n",
    "   \n",
    "   You'll see:\n",
    "   - All RAG query runs\n",
    "   - Parameters (query, model, top_k)\n",
    "   - Metrics (tokens, relevance, latency)\n",
    "   - Artifacts (queries, answers, prompts)\n",
    "\n",
    "üìä COMPARING RUNS:\n",
    "   1. Select multiple runs\n",
    "   2. Click \"Compare\"\n",
    "   3. View side-by-side:\n",
    "      - Which queries used most tokens?\n",
    "      - Which had highest relevance scores?\n",
    "      - Performance variations\n",
    "\n",
    "üå≥ TRACES VIEW:\n",
    "   Click \"Traces\" tab to see:\n",
    "   \n",
    "   Timeline visualization:\n",
    "   rag_pipeline (CHAIN) ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2.5s\n",
    "   ‚îú‚îÄ validate_query (PARSER) ‚îÅ‚îÅ 0.01s\n",
    "   ‚îú‚îÄ embed_text (EMBEDDING) ‚îÅ‚îÅ‚îÅ‚îÅ 0.3s\n",
    "   ‚îú‚îÄ semantic_search (RETRIEVER) ‚îÅ 0.05s\n",
    "   ‚îú‚îÄ assemble_context (PARSER) ‚îÅ 0.02s\n",
    "   ‚îú‚îÄ generate_response (LLM) ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2.0s\n",
    "   ‚îÇ  ‚îî‚îÄ OpenAI API call ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.9s\n",
    "   ‚îî‚îÄ validate_response (PARSER) ‚îÅ 0.01s\n",
    "\n",
    "üîé SPAN DETAILS:\n",
    "   Click on any span to see:\n",
    "   - Inputs and outputs\n",
    "   - Custom attributes\n",
    "   - Timing information\n",
    "   - Cache hit status\n",
    "   - Relevance scores\n",
    "\n",
    "üìà KEY INSIGHTS:\n",
    "   1. Performance Bottlenecks:\n",
    "      - Which step takes longest?\n",
    "      - Is it the LLM or retrieval?\n",
    "   \n",
    "   2. Quality Metrics:\n",
    "      - Average relevance scores\n",
    "      - Documents per query\n",
    "      - Answer validation rates\n",
    "   \n",
    "   3. Cost Analysis:\n",
    "      - Token usage per query\n",
    "      - Cache effectiveness\n",
    "      - Cost per operation\n",
    "   \n",
    "   4. Error Patterns:\n",
    "      - Failed queries\n",
    "      - Low relevance scores\n",
    "      - Validation issues\n",
    "\n",
    "üí° OPTIMIZATION OPPORTUNITIES:\n",
    "   Based on traces, you can:\n",
    "   - Adjust top_k if retrieval is slow\n",
    "   - Increase min_score if quality is poor\n",
    "   - Optimize prompts to reduce tokens\n",
    "   - Add more aggressive caching\n",
    "   - Implement parallel retrieval\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Congratulations! You've built a complete, production-ready RAG system with:\n",
    "\n",
    "### ‚úÖ Core Functionality\n",
    "- Document embedding and indexing\n",
    "- Semantic search with cosine similarity\n",
    "- Context-aware response generation\n",
    "- Input validation and output validation\n",
    "\n",
    "### ‚úÖ Observability\n",
    "- Complete experiment tracking\n",
    "- End-to-end distributed tracing\n",
    "- Custom span attributes\n",
    "- Performance metrics\n",
    "- Cost tracking\n",
    "\n",
    "### ‚úÖ Production Features\n",
    "- Embedding caching\n",
    "- Error handling\n",
    "- Query validation\n",
    "- Response validation\n",
    "- Relevance scoring\n",
    "\n",
    "### ‚úÖ RAG Evaluation\n",
    "- RAGAS metrics (Faithfulness, Context Precision)\n",
    "- Retrieval groundedness scoring\n",
    "- LLM-as-Judge evaluation patterns\n",
    "\n",
    "### üéØ What You Can Do Next\n",
    "\n",
    "**Immediate Improvements:**\n",
    "1. Add a vector database (Pinecone, Weaviate, ChromaDB)\n",
    "2. Implement streaming responses\n",
    "3. Add reranking for better relevance\n",
    "4. Create a web UI (Streamlit, Gradio)\n",
    "5. Set up monitoring dashboards\n",
    "\n",
    "**Production Enhancements:**\n",
    "1. Deploy as an API (FastAPI)\n",
    "2. Add rate limiting\n",
    "3. Implement user authentication\n",
    "4. Set up A/B testing framework\n",
    "5. Create alerting for quality/cost\n",
    "\n",
    "### üìö Complete Tutorial Series\n",
    "\n",
    "You've now completed Tutorial 1:\n",
    "\n",
    "1. ‚úÖ **Setup and Introduction**\n",
    "2. ‚úÖ **Experiment Tracking for LLMs**\n",
    "3. ‚úÖ **Introduction to Tracing**\n",
    "4. ‚úÖ **Manual Tracing and Advanced Observability**\n",
    "5. ‚úÖ **Prompt Management**\n",
    "6. ‚úÖ **Framework Integrations**\n",
    "7. ‚úÖ **Evaluating Agents**\n",
    "8. ‚úÖ **Prompt Optimization with GEPA**\n",
    "9. ‚úÖ **Complete RAG Application** (This notebook!)\n",
    "\n",
    "### üöÄ Next Tutorial Series\n",
    "\n",
    "Ready to go deeper?\n",
    "\n",
    "- **Tutorial 2**: Prompt Engineering and Version Control\n",
    "- **Tutorial 3**: Advanced Tracing and Debugging\n",
    "- **Tutorial 4**: Advanced Agent Evaluation\n",
    "- **Tutorial 5**: Optimizing Prompts for Performance\n",
    "\n",
    "### üéâ Congratulations!\n",
    "\n",
    "You now have:\n",
    "- A production-ready RAG system\n",
    "- Complete observability into your GenAI app\n",
    "- Understanding of MLflow's core capabilities\n",
    "- Patterns for building scalable LLM applications\n",
    "- RAG evaluation using RAGAS metrics\n",
    "\n",
    "**Keep building, keep learning, and happy coding!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
