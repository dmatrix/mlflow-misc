{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Tutorial 1.8: Complete RAG Application\n\n## Building a Production-Ready RAG System with Full Observability\n\nWelcome to the final notebook! This brings together everything you've learned to build a complete, production-ready RAG (Retrieval-Augmented Generation) application with comprehensive MLflow tracking and tracing.\n\n### What You'll Build\n\nA full RAG system that includes:\n- âœ… Document embedding and indexing\n- âœ… Semantic search / retrieval\n- âœ… Context-aware response generation\n- âœ… Complete experiment tracking\n- âœ… End-to-end tracing\n- âœ… Cost monitoring\n- âœ… Performance metrics\n- âœ… Error handling\n- âœ… Caching strategies\n- âœ… RAG evaluation with RAGAS metrics\n\n### Prerequisites\n- Completed all previous notebooks (1.1-1.7)\n- Understanding of experiment tracking and tracing\n\n### Estimated Time: 25-30 minutes"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: System Architecture\n",
    "\n",
    "### RAG Pipeline Flow\n",
    "\n",
    "```\n",
    "User Query\n",
    "    â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Query Processing (traced)          â”‚\n",
    "â”‚  - Validation                       â”‚\n",
    "â”‚  - Preprocessing                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Embedding Generation (traced)      â”‚\n",
    "â”‚  - OpenAI embeddings                â”‚\n",
    "â”‚  - Caching                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Vector Search (traced)             â”‚\n",
    "â”‚  - Cosine similarity                â”‚\n",
    "â”‚  - Top-k retrieval                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Context Assembly (traced)          â”‚\n",
    "â”‚  - Reranking (optional)             â”‚\n",
    "â”‚  - Prompt construction              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  LLM Generation (auto-traced)       â”‚\n",
    "â”‚  - GPT-4o-mini                      â”‚\n",
    "â”‚  - Streaming (optional)             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Response Validation (traced)       â”‚\n",
    "â”‚  - Quality checks                   â”‚\n",
    "â”‚  - Formatting                       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â†“\n",
    "Final Answer + Metadata\n",
    "```\n",
    "\n",
    "### What Gets Tracked\n",
    "\n",
    "**Experiment Level:**\n",
    "- Model configuration\n",
    "- Prompt versions\n",
    "- Hyperparameters (top_k, temperature)\n",
    "\n",
    "**Run Level:**\n",
    "- Individual queries\n",
    "- Performance metrics\n",
    "- Cost per query\n",
    "\n",
    "**Trace Level:**\n",
    "- Every operation's timing\n",
    "- Inputs and outputs\n",
    "- Custom attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "from typing import List, Dict, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri(\"./mlruns\")\n",
    "mlflow.set_experiment(\"10-complete-rag-system\")\n",
    "\n",
    "# Enable autologging\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Initialize OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "print(\"âœ… Environment configured for production RAG system\")\n",
    "print(f\"   MLflow tracking: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"   Experiment: {mlflow.get_experiment_by_name('10-complete-rag-system').name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Document Store and Embeddings\n",
    "\n",
    "In production, you'd use a vector database like Pinecone, Weaviate, or ChromaDB.\n",
    "For this tutorial, we'll use in-memory storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample document corpus (in production, load from database)\n",
    "DOCUMENT_STORE = {\n",
    "    \"doc1\": \"MLflow is an open source platform for the complete machine learning lifecycle. It provides experiment tracking, model packaging, and deployment capabilities across various ML frameworks.\",\n",
    "    \"doc2\": \"MLflow Tracing provides comprehensive observability for GenAI applications. It captures LLM calls, retrieval steps, tool usage, and agent reasoning with full input/output visibility.\",\n",
    "    \"doc3\": \"MLflow integrates with 30+ frameworks including OpenAI, Anthropic, LangChain, LlamaIndex, DSPy, and AutoGen. Each integration provides automatic tracing and experiment tracking.\",\n",
    "    \"doc4\": \"MLflow Evaluation enables systematic testing of GenAI applications using LLM-as-judge metrics, custom scorers, and human feedback. It supports both batch and online evaluation.\",\n",
    "    \"doc5\": \"MLflow Prompt Registry allows teams to version, share, and manage prompts centrally. It tracks which prompts are used in which experiments and enables A/B testing.\",\n",
    "    \"doc6\": \"MLflow supports collaborative development with experiment sharing, model versioning, and deployment tracking. Teams can compare results and iterate systematically.\",\n",
    "    \"doc7\": \"MLflow provides cost tracking for LLM applications by monitoring token usage, API calls, and compute resources. This helps teams optimize spending and budget effectively.\",\n",
    "    \"doc8\": \"MLflow is fully open source and vendor-neutral, ensuring no lock-in. It works with any cloud provider, ML framework, or LLM provider.\",\n",
    "}\n",
    "\n",
    "print(f\"ğŸ“š Document store initialized with {len(DOCUMENT_STORE)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache for embeddings (in production, use Redis or similar)\n",
    "EMBEDDING_CACHE = {}\n",
    "\n",
    "@mlflow.trace(name=\"embed_text\", span_type=\"EMBEDDING\")\n",
    "def embed_text(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Generate embeddings with caching.\n",
    "    \"\"\"\n",
    "    # Check cache\n",
    "    cache_key = hashlib.md5(text.encode()).hexdigest()\n",
    "    \n",
    "    if cache_key in EMBEDDING_CACHE:\n",
    "        mlflow.log_span_attribute(\"cache_hit\", True)\n",
    "        return EMBEDDING_CACHE[cache_key]\n",
    "    \n",
    "    mlflow.log_span_attribute(\"cache_hit\", False)\n",
    "    mlflow.log_span_attribute(\"text_length\", len(text))\n",
    "    \n",
    "    # Generate embedding\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=text\n",
    "    )\n",
    "    \n",
    "    embedding = response.data[0].embedding\n",
    "    mlflow.log_span_attribute(\"embedding_dim\", len(embedding))\n",
    "    \n",
    "    # Cache for future use\n",
    "    EMBEDDING_CACHE[cache_key] = embedding\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "print(\"âœ… Embedding function defined with caching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compute document embeddings\n",
    "print(\"\\nğŸ”„ Computing document embeddings...\")\n",
    "\n",
    "DOC_EMBEDDINGS = {}\n",
    "\n",
    "for doc_id, text in DOCUMENT_STORE.items():\n",
    "    DOC_EMBEDDINGS[doc_id] = embed_text(text)\n",
    "    print(f\"  âœ“ {doc_id}\")\n",
    "\n",
    "print(f\"\\nâœ… {len(DOC_EMBEDDINGS)} documents embedded and cached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Query Processing and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@mlflow.trace(name=\"validate_query\", span_type=\"PARSER\")\n",
    "def validate_query(query: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Validate and preprocess user query.\n",
    "    \"\"\"\n",
    "    mlflow.log_span_attribute(\"original_length\", len(query))\n",
    "    \n",
    "    # Basic validation\n",
    "    if not query or len(query.strip()) == 0:\n",
    "        mlflow.log_span_attribute(\"validation_error\", \"empty_query\")\n",
    "        raise ValueError(\"Query cannot be empty\")\n",
    "    \n",
    "    if len(query) > 500:\n",
    "        mlflow.log_span_attribute(\"validation_warning\", \"query_too_long\")\n",
    "        query = query[:500]  # Truncate\n",
    "    \n",
    "    # Preprocess\n",
    "    processed_query = query.strip()\n",
    "    \n",
    "    mlflow.log_span_attribute(\"processed_length\", len(processed_query))\n",
    "    mlflow.log_span_attribute(\"validation_passed\", True)\n",
    "    \n",
    "    return {\n",
    "        \"original\": query,\n",
    "        \"processed\": processed_query,\n",
    "        \"valid\": True\n",
    "    }\n",
    "\n",
    "print(\"âœ… Query validation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Semantic Search with Similarity Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@mlflow.trace(name=\"semantic_search\", span_type=\"RETRIEVER\")\n",
    "def search_documents(\n",
    "    query_embedding: List[float],\n",
    "    doc_embeddings: Dict[str, List[float]],\n",
    "    top_k: int = 3,\n",
    "    min_score: float = 0.7\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search for most relevant documents using cosine similarity.\n",
    "    \"\"\"\n",
    "    mlflow.log_span_attribute(\"corpus_size\", len(doc_embeddings))\n",
    "    mlflow.log_span_attribute(\"top_k\", top_k)\n",
    "    mlflow.log_span_attribute(\"min_score\", min_score)\n",
    "    \n",
    "    # Calculate cosine similarity for all documents\n",
    "    scores = {}\n",
    "    for doc_id, doc_emb in doc_embeddings.items():\n",
    "        similarity = np.dot(query_embedding, doc_emb) / \\\n",
    "                     (np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb))\n",
    "        scores[doc_id] = float(similarity)\n",
    "    \n",
    "    # Sort by score and filter by minimum threshold\n",
    "    sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    filtered_docs = [(doc_id, score) for doc_id, score in sorted_docs if score >= min_score]\n",
    "    \n",
    "    # Get top k\n",
    "    top_docs = filtered_docs[:top_k]\n",
    "    \n",
    "    results = [\n",
    "        {\n",
    "            \"doc_id\": doc_id,\n",
    "            \"score\": score,\n",
    "            \"text\": DOCUMENT_STORE[doc_id]\n",
    "        }\n",
    "        for doc_id, score in top_docs\n",
    "    ]\n",
    "    \n",
    "    # Log metrics\n",
    "    if results:\n",
    "        mlflow.log_span_attribute(\"num_results\", len(results))\n",
    "        mlflow.log_span_attribute(\"top_score\", results[0][\"score\"])\n",
    "        mlflow.log_span_attribute(\"avg_score\", np.mean([r[\"score\"] for r in results]))\n",
    "        mlflow.log_span_attribute(\"min_result_score\", results[-1][\"score\"])\n",
    "    else:\n",
    "        mlflow.log_span_attribute(\"num_results\", 0)\n",
    "        mlflow.log_span_attribute(\"retrieval_warning\", \"no_docs_above_threshold\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"âœ… Semantic search function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Context Assembly and Prompt Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@mlflow.trace(name=\"assemble_context\", span_type=\"PARSER\")\n",
    "def assemble_context(query: str, retrieved_docs: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Assemble context from retrieved documents and construct prompt.\n",
    "    \"\"\"\n",
    "    mlflow.log_span_attribute(\"num_docs\", len(retrieved_docs))\n",
    "    \n",
    "    if not retrieved_docs:\n",
    "        mlflow.log_span_attribute(\"context_warning\", \"no_docs_retrieved\")\n",
    "        return None\n",
    "    \n",
    "    # Format context\n",
    "    context_parts = []\n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        context_parts.append(\n",
    "            f\"[Document {i}] (Relevance: {doc['score']:.2f})\\n{doc['text']}\"\n",
    "        )\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Construct prompt with system instructions\n",
    "    prompt = f\"\"\"You are a helpful AI assistant that answers questions based on provided context.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Answer the question using ONLY the information in the context below\n",
    "- If the answer is not in the context, say \"I don't have enough information to answer that\"\n",
    "- Be concise but complete\n",
    "- Cite which document(s) you used if relevant\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "    \n",
    "    mlflow.log_span_attribute(\"context_length\", len(context))\n",
    "    mlflow.log_span_attribute(\"prompt_length\", len(prompt))\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "print(\"âœ… Context assembly function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: LLM Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@mlflow.trace(name=\"generate_response\", span_type=\"LLM\")\n",
    "def generate_answer(\n",
    "    prompt: str,\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    temperature: float = 0.1,\n",
    "    max_tokens: int = 300\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Generate answer using LLM.\n",
    "    \"\"\"\n",
    "    mlflow.log_span_attribute(\"model\", model)\n",
    "    mlflow.log_span_attribute(\"temperature\", temperature)\n",
    "    mlflow.log_span_attribute(\"max_tokens\", max_tokens)\n",
    "    \n",
    "    if not prompt:\n",
    "        mlflow.log_span_attribute(\"generation_error\", \"empty_prompt\")\n",
    "        raise ValueError(\"Prompt cannot be empty\")\n",
    "    \n",
    "    # Call LLM (automatically traced by OpenAI autolog)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    # Log generation metrics\n",
    "    mlflow.log_span_attribute(\"answer_length\", len(answer))\n",
    "    mlflow.log_span_attribute(\"prompt_tokens\", response.usage.prompt_tokens)\n",
    "    mlflow.log_span_attribute(\"completion_tokens\", response.usage.completion_tokens)\n",
    "    mlflow.log_span_attribute(\"total_tokens\", response.usage.total_tokens)\n",
    "    mlflow.log_span_attribute(\"finish_reason\", response.choices[0].finish_reason)\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"tokens\": response.usage.total_tokens,\n",
    "        \"model\": model,\n",
    "        \"finish_reason\": response.choices[0].finish_reason\n",
    "    }\n",
    "\n",
    "print(\"âœ… Answer generation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Response Validation and Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@mlflow.trace(name=\"validate_response\", span_type=\"PARSER\")\n",
    "def validate_response(answer: str, min_length: int = 10) -> Dict:\n",
    "    \"\"\"\n",
    "    Validate and post-process generated response.\n",
    "    \"\"\"\n",
    "    mlflow.log_span_attribute(\"min_length_threshold\", min_length)\n",
    "    mlflow.log_span_attribute(\"answer_length\", len(answer))\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # Check length\n",
    "    if len(answer) < min_length:\n",
    "        issues.append(\"too_short\")\n",
    "    \n",
    "    # Check for common error patterns\n",
    "    error_patterns = [\n",
    "        \"I don't have enough information\",\n",
    "        \"I cannot answer\",\n",
    "        \"I apologize\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in error_patterns:\n",
    "        if pattern.lower() in answer.lower():\n",
    "            issues.append(f\"contains_{pattern.replace(' ', '_').lower()}\")\n",
    "    \n",
    "    mlflow.log_span_attribute(\"validation_issues\", \",\".join(issues) if issues else \"none\")\n",
    "    mlflow.log_span_attribute(\"is_valid\", len(issues) == 0)\n",
    "    \n",
    "    return {\n",
    "        \"is_valid\": len(issues) == 0,\n",
    "        \"issues\": issues,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "\n",
    "print(\"âœ… Response validation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Complete RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@mlflow.trace(name=\"rag_pipeline\", span_type=\"CHAIN\")\n",
    "def rag_qa_system(\n",
    "    user_query: str,\n",
    "    top_k: int = 3,\n",
    "    min_score: float = 0.7,\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    temperature: float = 0.1\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline with full observability.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with answer, metadata, and status\n",
    "    \"\"\"\n",
    "    # Start MLflow run for tracking\n",
    "    with mlflow.start_run(run_name=\"rag_query\"):\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_params({\n",
    "            \"query\": user_query[:100],  # Truncate for readability\n",
    "            \"top_k\": top_k,\n",
    "            \"min_score\": min_score,\n",
    "            \"model\": model,\n",
    "            \"temperature\": temperature,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Validate query\n",
    "            validation_result = validate_query(user_query)\n",
    "            processed_query = validation_result[\"processed\"]\n",
    "            \n",
    "            # Step 2: Generate query embedding\n",
    "            query_embedding = embed_text(processed_query)\n",
    "            \n",
    "            # Step 3: Search documents\n",
    "            retrieved_docs = search_documents(\n",
    "                query_embedding,\n",
    "                DOC_EMBEDDINGS,\n",
    "                top_k=top_k,\n",
    "                min_score=min_score\n",
    "            )\n",
    "            \n",
    "            if not retrieved_docs:\n",
    "                mlflow.log_metric(\"retrieval_success\", 0)\n",
    "                return {\n",
    "                    \"status\": \"no_relevant_docs\",\n",
    "                    \"answer\": \"I couldn't find relevant information to answer your question.\",\n",
    "                    \"query\": user_query,\n",
    "                    \"retrieved_docs\": []\n",
    "                }\n",
    "            \n",
    "            mlflow.log_metric(\"num_docs_retrieved\", len(retrieved_docs))\n",
    "            mlflow.log_metric(\"avg_relevance_score\", \n",
    "                            np.mean([d[\"score\"] for d in retrieved_docs]))\n",
    "            mlflow.log_metric(\"retrieval_success\", 1)\n",
    "            \n",
    "            # Step 4: Assemble context and construct prompt\n",
    "            prompt = assemble_context(processed_query, retrieved_docs)\n",
    "            \n",
    "            # Step 5: Generate answer\n",
    "            generation_result = generate_answer(\n",
    "                prompt,\n",
    "                model=model,\n",
    "                temperature=temperature\n",
    "            )\n",
    "            \n",
    "            # Step 6: Validate response\n",
    "            validation = validate_response(generation_result[\"answer\"])\n",
    "            \n",
    "            # Log final metrics\n",
    "            mlflow.log_metric(\"total_tokens\", generation_result[\"tokens\"])\n",
    "            mlflow.log_metric(\"answer_valid\", 1 if validation[\"is_valid\"] else 0)\n",
    "            mlflow.log_metric(\"answer_length\", len(generation_result[\"answer\"]))\n",
    "            \n",
    "            # Log artifacts\n",
    "            mlflow.log_text(user_query, \"query.txt\")\n",
    "            mlflow.log_text(generation_result[\"answer\"], \"answer.txt\")\n",
    "            mlflow.log_text(prompt, \"full_prompt.txt\")\n",
    "            mlflow.log_dict(\n",
    "                {\"docs\": retrieved_docs},\n",
    "                \"retrieved_docs.json\"\n",
    "            )\n",
    "            \n",
    "            # Construct result\n",
    "            result = {\n",
    "                \"status\": \"success\",\n",
    "                \"query\": user_query,\n",
    "                \"answer\": generation_result[\"answer\"],\n",
    "                \"retrieved_docs\": retrieved_docs,\n",
    "                \"metadata\": {\n",
    "                    \"num_docs\": len(retrieved_docs),\n",
    "                    \"avg_relevance\": float(np.mean([d[\"score\"] for d in retrieved_docs])),\n",
    "                    \"tokens_used\": generation_result[\"tokens\"],\n",
    "                    \"model\": model,\n",
    "                    \"is_valid\": validation[\"is_valid\"],\n",
    "                    \"validation_issues\": validation[\"issues\"]\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Log error\n",
    "            mlflow.log_param(\"error_type\", type(e).__name__)\n",
    "            mlflow.log_param(\"error_message\", str(e))\n",
    "            mlflow.log_metric(\"pipeline_success\", 0)\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"query\": user_query,\n",
    "                \"error\": str(e),\n",
    "                \"error_type\": type(e).__name__\n",
    "            }\n",
    "\n",
    "print(\"âœ… Complete RAG pipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: Testing the RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What tracing capabilities does MLflow provide?\",\n",
    "    \"How does MLflow help with cost tracking?\",\n",
    "    \"Can MLflow integrate with LangChain?\",\n",
    "    \"What is the purpose of MLflow Prompt Registry?\",\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ§ª Testing RAG System\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nQuery {i}: {query}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = rag_qa_system(query, top_k=3, min_score=0.7)\n",
    "    latency = time.time() - start_time\n",
    "    \n",
    "    if result[\"status\"] == \"success\":\n",
    "        print(f\"\\nAnswer: {result['answer']}\")\n",
    "        print(f\"\\nMetadata:\")\n",
    "        print(f\"  - Documents used: {result['metadata']['num_docs']}\")\n",
    "        print(f\"  - Avg relevance: {result['metadata']['avg_relevance']:.3f}\")\n",
    "        print(f\"  - Tokens: {result['metadata']['tokens_used']}\")\n",
    "        print(f\"  - Latency: {latency:.2f}s\")\n",
    "        print(f\"  - Valid: {result['metadata']['is_valid']}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"success\": True,\n",
    "            \"latency\": latency,\n",
    "            \"tokens\": result['metadata']['tokens_used']\n",
    "        })\n",
    "    else:\n",
    "        print(f\"\\nError: {result.get('error', 'Unknown error')}\")\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"success\": False,\n",
    "            \"latency\": latency\n",
    "        })\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nâœ… All queries processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11: Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance\n",
    "print(\"\\nğŸ“Š Performance Summary\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "successful = [r for r in results if r[\"success\"]]\n",
    "\n",
    "if successful:\n",
    "    latencies = [r[\"latency\"] for r in successful]\n",
    "    tokens = [r[\"tokens\"] for r in successful]\n",
    "    \n",
    "    print(f\"Success Rate: {len(successful)}/{len(results)} ({len(successful)/len(results)*100:.1f}%)\")\n",
    "    print(f\"\\nLatency Stats:\")\n",
    "    print(f\"  Average: {np.mean(latencies):.2f}s\")\n",
    "    print(f\"  Min: {np.min(latencies):.2f}s\")\n",
    "    print(f\"  Max: {np.max(latencies):.2f}s\")\n",
    "    print(f\"  Std Dev: {np.std(latencies):.2f}s\")\n",
    "    \n",
    "    print(f\"\\nToken Usage:\")\n",
    "    print(f\"  Average: {np.mean(tokens):.0f} tokens\")\n",
    "    print(f\"  Total: {np.sum(tokens):.0f} tokens\")\n",
    "    print(f\"  Est. Cost: ${np.sum(tokens) * 0.15 / 1_000_000:.6f}\")\n",
    "    \n",
    "    print(f\"\\nCache Performance:\")\n",
    "    print(f\"  Embedding cache hits: {len(EMBEDDING_CACHE)} embeddings cached\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Step 12: RAG Evaluation with RAGAS Metrics\n\nNow let's evaluate our RAG system using RAGAS (Retrieval-Augmented Generation Assessment) metrics. These metrics help assess the quality of both retrieval and generation.\n\n| Metric | What It Measures |\n|--------|-----------------|\n| **Faithfulness** | Is the answer grounded in the retrieved context? |\n| **Context Precision** | Are the retrieved documents relevant to the question? |\n| **RetrievalGroundedness** | Does the answer stay true to retrieved information? |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from mlflow.genai.scorers import RetrievalGroundedness\nfrom mlflow.genai.scorers.ragas import Faithfulness, ContextPrecision\n\n# Judge model for RAGAS evaluation\nJUDGE_MODEL = \"openai:/gpt-4o-mini\"\n\n# Initialize RAGAS scorers\nfaithfulness_scorer = Faithfulness(model=JUDGE_MODEL)\ncontext_precision_scorer = ContextPrecision(model=JUDGE_MODEL)\ngroundedness_scorer = RetrievalGroundedness(model=JUDGE_MODEL)\n\nprint(\"âœ… RAGAS scorers initialized:\")\nprint(\"   - Faithfulness\")\nprint(\"   - ContextPrecision\")\nprint(\"   - RetrievalGroundedness\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Prepare RAG evaluation dataset from our test results\n# We need to format the data with inputs, outputs, and context\n\nrag_eval_dataset = []\n\nfor query in test_queries:\n    # Run the RAG pipeline to get results (uses cached embeddings)\n    result = rag_qa_system(query, top_k=3, min_score=0.7)\n    \n    if result[\"status\"] == \"success\":\n        # Extract context from retrieved docs\n        context = [doc[\"text\"] for doc in result[\"retrieved_docs\"]]\n        \n        rag_eval_dataset.append({\n            \"inputs\": {\"question\": query},\n            \"outputs\": {\"response\": result[\"answer\"]},\n            \"context\": {\"retrieved_context\": context}\n        })\n\nprint(f\"âœ… RAG evaluation dataset prepared with {len(rag_eval_dataset)} examples\")\nfor i, item in enumerate(rag_eval_dataset, 1):\n    print(f\"   {i}. {item['inputs']['question'][:50]}...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Run RAGAS evaluation\nprint(\"ğŸ”„ Running RAGAS evaluation on RAG outputs...\\n\")\n\nragas_results = mlflow.genai.evaluate(\n    data=rag_eval_dataset,\n    scorers=[\n        faithfulness_scorer,\n        context_precision_scorer,\n        groundedness_scorer\n    ]\n)\n\nprint(\"\\nâœ… RAGAS evaluation complete!\")\nprint(\"\\nğŸ“Š RAGAS Metrics Summary:\")\nprint(\"-\" * 50)\nfor metric_name, value in ragas_results.metrics.items():\n    if isinstance(value, float):\n        print(f\"  {metric_name}: {value:.3f}\")\n    else:\n        print(f\"  {metric_name}: {value}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Step 13: Viewing Results in MLflow UI\n\nNow let's explore what was tracked in MLflow.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 12: Viewing Results in MLflow UI\n",
    "\n",
    "Now let's explore what was tracked in MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘         Analyzing Results in MLflow UI                       â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ğŸ” EXPERIMENTS VIEW:\n",
    "   Navigate to: http://localhost:5000\n",
    "   Select: \"10-complete-rag-system\" experiment\n",
    "   \n",
    "   You'll see:\n",
    "   - All RAG query runs\n",
    "   - Parameters (query, model, top_k)\n",
    "   - Metrics (tokens, relevance, latency)\n",
    "   - Artifacts (queries, answers, prompts)\n",
    "\n",
    "ğŸ“Š COMPARING RUNS:\n",
    "   1. Select multiple runs\n",
    "   2. Click \"Compare\"\n",
    "   3. View side-by-side:\n",
    "      - Which queries used most tokens?\n",
    "      - Which had highest relevance scores?\n",
    "      - Performance variations\n",
    "\n",
    "ğŸŒ³ TRACES VIEW:\n",
    "   Click \"Traces\" tab to see:\n",
    "   \n",
    "   Timeline visualization:\n",
    "   rag_pipeline (CHAIN) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2.5s\n",
    "   â”œâ”€ validate_query (PARSER) â”â” 0.01s\n",
    "   â”œâ”€ embed_text (EMBEDDING) â”â”â”â” 0.3s\n",
    "   â”œâ”€ semantic_search (RETRIEVER) â” 0.05s\n",
    "   â”œâ”€ assemble_context (PARSER) â” 0.02s\n",
    "   â”œâ”€ generate_response (LLM) â”â”â”â”â” 2.0s\n",
    "   â”‚  â””â”€ OpenAI API call â”â”â”â”â”â”â”â”â” 1.9s\n",
    "   â””â”€ validate_response (PARSER) â” 0.01s\n",
    "\n",
    "ğŸ” SPAN DETAILS:\n",
    "   Click on any span to see:\n",
    "   - Inputs and outputs\n",
    "   - Custom attributes\n",
    "   - Timing information\n",
    "   - Cache hit status\n",
    "   - Relevance scores\n",
    "\n",
    "ğŸ“ˆ KEY INSIGHTS:\n",
    "   1. Performance Bottlenecks:\n",
    "      - Which step takes longest?\n",
    "      - Is it the LLM or retrieval?\n",
    "   \n",
    "   2. Quality Metrics:\n",
    "      - Average relevance scores\n",
    "      - Documents per query\n",
    "      - Answer validation rates\n",
    "   \n",
    "   3. Cost Analysis:\n",
    "      - Token usage per query\n",
    "      - Cache effectiveness\n",
    "      - Cost per operation\n",
    "   \n",
    "   4. Error Patterns:\n",
    "      - Failed queries\n",
    "      - Low relevance scores\n",
    "      - Validation issues\n",
    "\n",
    "ğŸ’¡ OPTIMIZATION OPPORTUNITIES:\n",
    "   Based on traces, you can:\n",
    "   - Adjust top_k if retrieval is slow\n",
    "   - Increase min_score if quality is poor\n",
    "   - Optimize prompts to reduce tokens\n",
    "   - Add more aggressive caching\n",
    "   - Implement parallel retrieval\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Summary\n\nCongratulations! You've built a complete, production-ready RAG system with:\n\n### âœ… Core Functionality\n- Document embedding and indexing\n- Semantic search with cosine similarity\n- Context-aware response generation\n- Input validation and output validation\n\n### âœ… Observability\n- Complete experiment tracking\n- End-to-end distributed tracing\n- Custom span attributes\n- Performance metrics\n- Cost tracking\n\n### âœ… Production Features\n- Embedding caching\n- Error handling\n- Query validation\n- Response validation\n- Relevance scoring\n\n### âœ… RAG Evaluation\n- RAGAS metrics (Faithfulness, Context Precision)\n- Retrieval groundedness scoring\n- LLM-as-Judge evaluation patterns\n\n### ğŸ¯ What You Can Do Next\n\n**Immediate Improvements:**\n1. Add a vector database (Pinecone, Weaviate, ChromaDB)\n2. Implement streaming responses\n3. Add reranking for better relevance\n4. Create a web UI (Streamlit, Gradio)\n5. Set up monitoring dashboards\n\n**Production Enhancements:**\n1. Deploy as an API (FastAPI)\n2. Add rate limiting\n3. Implement user authentication\n4. Set up A/B testing framework\n5. Create alerting for quality/cost\n\n### ğŸ“š Complete Tutorial Series\n\nYou've now completed Tutorial 1:\n\n1. âœ… **Setup and Introduction**\n2. âœ… **Experiment Tracking for LLMs**\n3. âœ… **Introduction to Tracing**\n4. âœ… **Manual Tracing and Advanced Observability**\n5. âœ… **Prompt Management**\n6. âœ… **Framework Integrations**\n7. âœ… **Evaluating Agents**\n8. âœ… **Complete RAG Application** (This notebook!)\n\n### ğŸš€ Next Tutorial Series\n\nReady to go deeper?\n\n- **Tutorial 2**: Prompt Engineering and Version Control\n- **Tutorial 3**: Advanced Tracing and Debugging\n- **Tutorial 4**: Advanced Agent Evaluation\n- **Tutorial 5**: Optimizing Prompts for Performance\n\n### ğŸ‰ Congratulations!\n\nYou now have:\n- A production-ready RAG system\n- Complete observability into your GenAI app\n- Understanding of MLflow's core capabilities\n- Patterns for building scalable LLM applications\n- RAG evaluation using RAGAS metrics\n\n**Keep building, keep learning, and happy coding!** ğŸš€"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}