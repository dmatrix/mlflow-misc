{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb2ab264",
   "metadata": {},
   "source": [
    "# 1. Setup and Dependencies #\n",
    "First, install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd50f7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv add openai-agents mlflow datasets openai gepa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8007878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from typing import Any\n",
    "\n",
    "import mlflow\n",
    "from agents import Agent, Runner\n",
    "from datasets import load_dataset\n",
    "from mlflow.entities import Feedback\n",
    "from mlflow.genai import evaluate, scorer\n",
    "from mlflow.genai.optimize import GepaPromptOptimizer\n",
    "from mlflow.genai.judges import CategoricalRating\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"HotpotQA Optimization\")\n",
    "\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Avoid hanging due to the conflict between async and threading (not necessary for sync agents)\n",
    "os.environ[\"MLFLOW_GENAI_EVAL_MAX_WORKERS\"] = \"1\"\n",
    "# Skip trace validation to avoid NonRecordingSpan errors when evaluate() runs predict_fn with tracing disabled\n",
    "os.environ[\"MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION\"] = \"True\"\n",
    "\n",
    "# If running on notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145eeea3",
   "metadata": {},
   "source": [
    "# 2. Create and Register Your Base Prompt\n",
    "\n",
    "Start with a simple, straightforward prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fc1da63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/09 16:27:54 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for prompt version to finish creation. Prompt name: hotpotqa-user-prompt, version 2\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"You are a question answering assistant. Answer questions based ONLY on the provided context.\n",
    "\n",
    "IMPORTANT INSTRUCTIONS:\n",
    "- For yes/no questions, answer ONLY \"yes\" or \"no\"\n",
    "- Do NOT include phrases like \"based on the context\" or \"according to the documents\"\n",
    "\n",
    "Context:\n",
    "{{context}}\n",
    "\n",
    "Question: {{question}}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Register the prompt in MLflow\n",
    "base_prompt = mlflow.genai.register_prompt(\n",
    "    name=\"hotpotqa-user-prompt\",\n",
    "    template=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2533080b",
   "metadata": {},
   "source": [
    "# 3. Initialize the OpenAI Agent\n",
    "\n",
    "Setup your agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb607e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment configured\n",
      "   Provider: OpenAI\n",
      "   Model: gpt-5.2\n",
      "   Optimizer model: openai:/gpt-5.2\n",
      "   Tracking URI: http://localhost:5000\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "from utils.clnt_utils import is_databricks_ai_gateway_client, get_databricks_ai_gateway_client, get_openai_client, get_ai_gateway_model_names\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "# Configure client and model based on provider\n",
    "use_databricks_provider = is_databricks_ai_gateway_client()\n",
    "if use_databricks_provider:\n",
    "    client = get_databricks_ai_gateway_client()\n",
    "    model_name = get_ai_gateway_model_names()[0]\n",
    "    optimizer_model = f\"databricks:/{model_name}\"\n",
    "else:\n",
    "    client = get_openai_client()\n",
    "    model_name = \"gpt-5.2\"\n",
    "    optimizer_model = f\"openai:/{model_name}\"\n",
    "\n",
    "# Enable autologging\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Runner.run() expects an Agent instance, not a model string. Create a simple agent that uses this model.\n",
    "# The full prompt (context + question) is passed as the user message in predict_fn.\n",
    "qa_agent = Agent(\n",
    "    name=\"HotpotQA\",\n",
    "    model=model_name,\n",
    "    instructions=\"Answer the question using only the information provided in the conversation.\",\n",
    ")\n",
    "\n",
    "print(\"\\u2705 Environment configured\")\n",
    "print(f\"   Provider: {'Databricks AI Gateway' if use_databricks_provider else 'OpenAI'}\")\n",
    "print(f\"   Model: {model_name}\")\n",
    "print(f\"   Optimizer model: {optimizer_model}\")\n",
    "print(f\"   Tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445165a2",
   "metadata": {},
   "source": [
    "# 4. Create a Prediction Function\n",
    "\n",
    "The prediction function formats the context and question using the prompt template, then runs the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77808dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wrapper for `predict_fn` to run the agent with different prompts\n",
    "def create_predict_fn(prompt_uri: str):\n",
    "    prompt = mlflow.genai.load_prompt(prompt_uri)\n",
    "\n",
    "    # Do not decorate with @mlflow.trace here: evaluate() runs predict_fn under trace_disabled\n",
    "    # for validation, which would raise 'NonRecordingSpan' object has no attribute 'context'.\n",
    "    # MLflow wraps predict_fn with tracing when running the actual evaluation.\n",
    "    def predict_fn(context: str, question: str) -> str:\n",
    "        \"\"\"Predict function that uses the agent with the MLflow prompt.\"\"\"\n",
    "        # Use prompt.format() with template variables\n",
    "        user_message = prompt.format(context=context, question=question)\n",
    "\n",
    "        # Run your agent (Runner.run expects an Agent instance, not a model string)\n",
    "        result = asyncio.run(Runner.run(qa_agent, user_message))\n",
    "\n",
    "        return result.final_output\n",
    "\n",
    "    return predict_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bbac41",
   "metadata": {},
   "source": [
    "# 5. Baseline Evaluation\n",
    "\n",
    "Before optimizing, establish a baseline by evaluating the agent on a validation set. Here, we define a simple custom scorer that compares the system outputs and expected outputs for equality, but you can use any Scorer objects. See the Scorer Overview for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85cc89b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading HotpotQA dataset (validation split)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 100 samples\n",
      "\n",
      "Running evaluation on 100 samples...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/09 16:28:29 INFO mlflow.models.evaluation.utils.trace: Auto tracing is temporarily enabled during the model evaluation for computing some metrics and debugging. To disable tracing, call `mlflow.autolog(disable=True)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f2b5fc1ea34e09a0eb870300be43e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/100 [Elapsed: 00:00, Remaining: ?] "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <title>Evaluation output</title>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <style>\n",
       "        body {\n",
       "            font-family: Arial, sans-serif;\n",
       "        }\n",
       "\n",
       "        .header {\n",
       "            a.button {\n",
       "                padding: 4px 8px;\n",
       "                line-height: 20px;\n",
       "                box-shadow: none;\n",
       "                height: 20px;\n",
       "                display: inline-flex;\n",
       "                align-items: center;\n",
       "                justify-content: center;\n",
       "                vertical-align: middle;\n",
       "                background-color: rgb(34, 114, 180);\n",
       "                color: rgb(255, 255, 255);\n",
       "                text-decoration: none;\n",
       "                animation-duration: 0s;\n",
       "                transition: none 0s ease 0s;\n",
       "                position: relative;\n",
       "                white-space: nowrap;\n",
       "                text-align: center;\n",
       "                border: 1px solid rgb(192, 205, 216);\n",
       "                cursor: pointer;\n",
       "                user-select: none;\n",
       "                touch-action: manipulation;\n",
       "                border-radius: 4px;\n",
       "                gap: 6px;\n",
       "            }\n",
       "\n",
       "            a.button:hover {\n",
       "                background-color: rgb(14, 83, 139) !important;\n",
       "                border-color: transparent !important;\n",
       "                color: rgb(255, 255, 255) !important;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .warnings-section {\n",
       "            margin-top: 8px;\n",
       "\n",
       "            ul {\n",
       "                list-style-type: none;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .instructions-section {\n",
       "            margin-top: 16px;\n",
       "            font-size: 14px;\n",
       "\n",
       "            ul {\n",
       "                margin-top: 0;\n",
       "                margin-bottom: 0;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        code {\n",
       "            font-family: monospace;\n",
       "        }\n",
       "\n",
       "        .note {\n",
       "            color: #666;\n",
       "        }\n",
       "\n",
       "        a {\n",
       "            color: #2272B4;\n",
       "            text-decoration: none;\n",
       "        }\n",
       "\n",
       "        a:hover {\n",
       "            color: #005580;\n",
       "        }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "    <div class=\"header\">\n",
       "        <a href=\"http://localhost:5000/#/experiments/2/evaluation-runs?selectedRunUuid=e8a177d0304b4e9facce3bfd1de25d74\" class=\"button\">\n",
       "            View evaluation results in MLflow\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1em\" height=\"1em\" fill=\"none\" viewBox=\"0 0 16 16\" aria-hidden=\"true\" focusable=\"false\" class=\"\">\n",
       "                <path fill=\"currentColor\" d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"></path>\n",
       "                <path fill=\"currentColor\" d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"></path>\n",
       "            </svg>\n",
       "        </a>\n",
       "    </div>\n",
       "</div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 0.58%\n"
     ]
    }
   ],
   "source": [
    "# Ensure trace validation is skipped (avoids NonRecordingSpan when predict_fn runs under trace_disabled)\n",
    "os.environ[\"MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION\"] = \"True\"\n",
    "\n",
    "def prepare_hotpotqa_data(num_samples: int, split: str = \"validation\") -> list[dict]:\n",
    "    \"\"\"Load and prepare HotpotQA data for MLflow GenAI (evaluate/optimize).\"\"\"\n",
    "    print(f\"\\nLoading HotpotQA dataset ({split} split)...\")\n",
    "    dataset = load_dataset(\"hotpot_qa\", \"distractor\", split=split)\n",
    "    dataset = dataset.select(range(0, min(num_samples, len(dataset))))\n",
    "\n",
    "    data = []\n",
    "    for example in dataset:\n",
    "        # Format context from HotpotQA\n",
    "        context_text = \"\\n\\n\".join([\n",
    "            f\"Document {i+1}: {title}\\n{' '.join(sentences)}\"\n",
    "            for i, (title, sentences) in enumerate(zip(example[\"context\"][\"title\"], example[\"context\"][\"sentences\"]))\n",
    "        ])\n",
    "\n",
    "        data.append({\n",
    "            \"inputs\": {\n",
    "                \"context\": context_text,\n",
    "                \"question\": example[\"question\"],\n",
    "            },\n",
    "            \"expectations\": {\n",
    "                \"expected_response\": example[\"answer\"],\n",
    "            }\n",
    "        })\n",
    "\n",
    "    print(f\"Prepared {len(data)} samples\")\n",
    "    return data\n",
    "\n",
    "# Define a scorer for exact match\n",
    "@scorer\n",
    "def equivalence(outputs: str, expectations: dict[str, Any]) -> Feedback:\n",
    "    return Feedback(\n",
    "        name=\"equivalence\",\n",
    "        value=CategoricalRating.YES if outputs == expectations[\"expected_response\"] else CategoricalRating.NO,\n",
    "    )\n",
    "\n",
    "def run_benchmark(\n",
    "    prompt_uri: str,\n",
    "    num_samples: int,\n",
    "    split: str = \"validation\",\n",
    ") -> dict:\n",
    "    \"\"\"Run the agent on HotpotQA benchmark using mlflow.genai.evaluate().\"\"\"\n",
    "\n",
    "    # Prepare evaluation data\n",
    "    eval_data = prepare_hotpotqa_data(num_samples, split)\n",
    "\n",
    "    # Create prediction function\n",
    "    predict_fn = create_predict_fn(prompt_uri)\n",
    "\n",
    "    # Run evaluation\n",
    "    print(f\"\\nRunning evaluation on {len(eval_data)} samples...\\n\")\n",
    "\n",
    "    results = evaluate(\n",
    "        data=eval_data,\n",
    "        predict_fn=predict_fn,\n",
    "        scorers=[equivalence],\n",
    "    )\n",
    "\n",
    "    # Extract metrics\n",
    "    accuracy = results.metrics.get(\"equivalence/mean\", 0.0) / 100.0\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"metrics\": results.metrics,\n",
    "        \"results\": results,\n",
    "    }\n",
    "\n",
    "\n",
    "# Run baseline evaluation\n",
    "baseline_metrics = run_benchmark(base_prompt.uri, num_samples=100)\n",
    "\n",
    "print(f\"Baseline Accuracy: {baseline_metrics['accuracy']:.2%}\")\n",
    "# Output: Baseline Accuracy: 50.0%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1781347f",
   "metadata": {},
   "source": [
    "# 6. Optimize the Prompt\n",
    "\n",
    "Now comes the exciting part - using MLflow to automatically improve the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04226864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading HotpotQA dataset (train split)...\n",
      "Prepared 100 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jules/git-repos/mlflow-misc/.venv/lib/python3.11/site-packages/mlflow/data/dataset_source_registry.py:148: UserWarning: The specified dataset source can be interpreted in multiple ways: LocalArtifactDatasetSource, LocalArtifactDatasetSource. MLflow will assume that this is a LocalArtifactDatasetSource source.\n",
      "  return _dataset_source_registry.resolve(\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data using shared function\n",
    "train_data = prepare_hotpotqa_data(num_samples=100, split=\"train\")\n",
    "\n",
    "# Run optimization\n",
    "result = mlflow.genai.optimize_prompts(\n",
    "    predict_fn=create_predict_fn(base_prompt.uri),\n",
    "    train_data=train_data,\n",
    "    prompt_uris=[base_prompt.uri],\n",
    "    optimizer=GepaPromptOptimizer(\n",
    "        reflection_model=\"openai:/gpt-4o\",\n",
    "        max_metric_calls=500,\n",
    "    ),\n",
    "    scorers=[equivalence],\n",
    "    enable_tracking=True,\n",
    ")\n",
    "\n",
    "# Get the optimized prompt URI\n",
    "optimized_prompt_uri = result.optimized_prompts[0].uri\n",
    "print(f\"  Base prompt: {base_prompt.uri}\")\n",
    "print(f\"  Optimized prompt: {optimized_prompt_uri}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
