{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1.5: Prompt Management\n",
    "\n",
    "![](images/6_Prompt-Management.png)\n",
    "\n",
    "## Version Control for Prompts\n",
    "\n",
    "Welcome to prompt management! Prompts are the instructions that guide LLMs, and managing them properly is crucial for maintaining quality and collaboration in GenAI applications.\n",
    "\n",
    "### What You'll Learn\n",
    "- Why prompt management matters\n",
    "- Creating reusable prompt templates\n",
    "- Versioning prompts systematically\n",
    "- Linking prompts to experiments\n",
    "- Best practices for prompt organization\n",
    "- Sharing prompts across teams\n",
    "\n",
    "### Prerequisites\n",
    "- Completed previous notebooks (1.1-1.4)\n",
    "- Understanding of experiment tracking\n",
    "\n",
    "### Estimated Time: 15-20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Why Manage Prompts?\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Without proper management:\n",
    "\n",
    "```python\n",
    "# Scattered prompts in code\n",
    "prompt1 = \"You are a helpful assistant. Answer: {question}\"\n",
    "prompt2 = \"You are helpful. Respond to: {question}\"  # Which one works better?\n",
    "prompt3 = \"Answer {question}\"  # Lost track of what works\n",
    "```\n",
    "\n",
    "**Problems:**\n",
    "- âŒ Prompts hardcoded in multiple places\n",
    "- âŒ No version history\n",
    "- âŒ Hard to A/B test different versions\n",
    "- âŒ Difficult to collaborate\n",
    "- âŒ Can't track which prompt generated which output\n",
    "\n",
    "### The Solution: Prompt Management\n",
    "\n",
    "```python\n",
    "# Centralized, versioned prompts\n",
    "prompt_template = mlflow.log_text(template, \"prompts/qa_v1.txt\")\n",
    "# Now you can:\n",
    "# âœ… Version prompts\n",
    "# âœ… Track performance\n",
    "# âœ… Share with team\n",
    "# âœ… Link to experiments\n",
    "```\n",
    "\n",
    "### Benefits\n",
    "\n",
    "1. **Reproducibility**: Know exactly which prompt was used\n",
    "2. **Collaboration**: Share prompts across team\n",
    "3. **Experimentation**: Systematic A/B testing\n",
    "4. **Version Control**: Track prompt evolution\n",
    "5. **Governance**: Audit and approval processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/04 21:10:55 INFO mlflow.tracking.fluent: Experiment with name '08-prompt-management' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment configured: using Databricks AI Gateway client\n",
      "   Tracking URI: http://localhost:5000\n",
      "âœ… Environment configured\n",
      "   OpenAI autologging: ENABLED\n",
      "âœ… Environment configured for prompt management\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "from utils.clnt_utils import is_databricks_ai_gateway_client, get_databricks_ai_gateway_client, get_openai_client, get_ai_gateway_model_names\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "use_databricks_provider = is_databricks_ai_gateway_client()\n",
    "if use_databricks_provider:\n",
    "    client = get_databricks_ai_gateway_client()\n",
    "    model_name = get_ai_gateway_model_names()[0]\n",
    "else:\n",
    "    # Initialize OpenAI\n",
    "    client = get_openai_client()\n",
    "    model_name = \"gpt-5-2\"\n",
    "\n",
    "print(\"âœ… Environment configured: using\", \"Databricks AI Gateway\" if use_databricks_provider else \"OpenAI\", \"client\")\n",
    "print(f\"   Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "# Create experiment\n",
    "mlflow.set_experiment(\"08-prompt-management\")\n",
    "\n",
    "\n",
    "print(\"âœ… Environment configured\")\n",
    "print(\"   OpenAI autologging: ENABLED\")\n",
    "\n",
    "\n",
    "print(\"âœ… Environment configured for prompt management\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Basic Prompt Logging\n",
    "\n",
    "Let's start by logging prompts as artifacts in experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is prompt engineering?\n",
      "\n",
      "Answer: Prompt engineering is the practice of designing and refining the instructions (prompts) given to an AI model to reliably produce the desired output, often by specifying context, constraints, examples, and formatting.\n",
      "\n",
      "âœ… Prompt and response logged!\n",
      "   Run ID: ffe1585e4e1b4f51b38333e569ec5f97\n",
      "ğŸƒ View run basic-prompt-logging at: http://localhost:5000/#/experiments/8/runs/ffe1585e4e1b4f51b38333e569ec5f97\n",
      "ğŸ§ª View experiment at: http://localhost:5000/#/experiments/8\n"
     ]
    }
   ],
   "source": [
    "# Basic prompt logging\n",
    "with mlflow.start_run(run_name=\"basic-prompt-logging\") as run:\n",
    "    \n",
    "    # Define prompt\n",
    "    prompt_template = \"\"\"You are a helpful AI assistant.\n",
    "\n",
    "Answer the following question concisely:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Log prompt as artifact\n",
    "    mlflow.log_text(prompt_template, \"prompt_template.txt\")\n",
    "    \n",
    "    # Log prompt metadata\n",
    "    mlflow.log_param(\"prompt_version\", \"v1.0\")\n",
    "    mlflow.log_param(\"prompt_author\", \"jules\")\n",
    "    mlflow.log_param(\"prompt_purpose\", \"general_qa\")\n",
    "    \n",
    "    # Use the prompt\n",
    "    question = \"What is prompt engineering?\"\n",
    "    filled_prompt = prompt_template.format(question=question)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": filled_prompt}],\n",
    "        temperature=1.0\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    # Log the filled prompt and response\n",
    "    mlflow.log_text(filled_prompt, \"filled_prompt.txt\")\n",
    "    mlflow.log_text(answer, \"response.txt\")\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "    print(\"\\nâœ… Prompt and response logged!\")\n",
    "    print(f\"   Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ What We Logged\n",
    "\n",
    "1. **Template**: The reusable prompt structure\n",
    "2. **Metadata**: Version, author, purpose\n",
    "3. **Filled prompt**: The actual prompt sent to LLM\n",
    "4. **Response**: The LLM output\n",
    "\n",
    "This creates a complete audit trail!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Prompt Templates with Variables\n",
    "\n",
    "Create flexible prompt templates with multiple variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PromptTemplate class defined\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "# Advanced prompt template\n",
    "class PromptTemplate:\n",
    "    \"\"\"\n",
    "    Simple prompt template manager.\n",
    "    \"\"\"\n",
    "    def __init__(self, template: str, variables: List[str], metadata: Dict = None):\n",
    "        self.template = template\n",
    "        self.variables = variables\n",
    "        self.metadata = metadata or {}\n",
    "        self.metadata['created_at'] = datetime.now().isoformat()\n",
    "    \n",
    "    def format(self, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Fill template with variables.\n",
    "        \"\"\"\n",
    "        # Validate all required variables are provided\n",
    "        missing = set(self.variables) - set(kwargs.keys())\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing variables: {missing}\")\n",
    "        \n",
    "        return self.template.format(**kwargs)\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Serialize to dictionary.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"template\": self.template,\n",
    "            \"variables\": self.variables,\n",
    "            \"metadata\": self.metadata\n",
    "        }\n",
    "    \n",
    "    def save(self, run) -> str:\n",
    "        \"\"\"\n",
    "        Save prompt to MLflow run.\n",
    "        \"\"\"\n",
    "        # Save template\n",
    "        mlflow.log_text(self.template, \"prompt_template.txt\")\n",
    "        \n",
    "        # Save full spec\n",
    "        mlflow.log_dict(self.to_dict(), \"prompt_spec.json\")\n",
    "        \n",
    "        # Log metadata as params\n",
    "        for key, value in self.metadata.items():\n",
    "            mlflow.log_param(f\"prompt_{key}\", value)\n",
    "        \n",
    "        return run.info.run_id\n",
    "\n",
    "print(\"âœ… PromptTemplate class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use our generic class template, using multiple variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "MLflow Tracing captures detailed, end-to-end execution traces of your ML or LLM applicationâ€”recording spans for steps like retrieval, model calls, and tool/function invocations along with timing, inputs/outputs, and metadata. These traces are logged to MLflow so you can visualize and debug runs, identify bottlenecks, and monitor quality and reliability in production.\n",
      "\n",
      "âœ… Template-based prompt logged!\n",
      "\n",
      "ğŸ“‹ Template info:\n",
      "   Variables: ['role', 'context', 'task']\n",
      "   Version: v2.0\n",
      "ğŸƒ View run template-with-variables at: http://localhost:5000/#/experiments/8/runs/f572c01bdc314d368968a03a52720676\n",
      "ğŸ§ª View experiment at: http://localhost:5000/#/experiments/8\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# Create and use a prompt template\n",
    "\n",
    "with mlflow.start_run(run_name=\"template-with-variables\") as run:\n",
    "    \n",
    "    # Define template\n",
    "    template = PromptTemplate(\n",
    "        template=\"\"\"You are a {role}.\n",
    "\n",
    "                 Context: {context}\n",
    "\n",
    "                Task: {task}\n",
    "\n",
    "                 Provide your response:\"\"\",\n",
    "        variables=[\"role\", \"context\", \"task\"],\n",
    "        metadata={\n",
    "            \"version\": \"v2.0\",\n",
    "            \"author\": \"jules\",\n",
    "            \"purpose\": \"role_based_qa\",\n",
    "            \"approved\": True\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Save template\n",
    "    template.save(run)\n",
    "    \n",
    "    # Use template\n",
    "    prompt = template.format(\n",
    "        role=\"technical documentation expert\",\n",
    "        context=\"MLflow is an open source MLOps platform\",\n",
    "        task=\"Explain MLflow tracing in 2 sentences\"\n",
    "    )\n",
    "    \n",
    "    # Make LLM call\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    # Log filled prompt and response\n",
    "    mlflow.log_text(prompt, \"filled_prompt.txt\")\n",
    "    mlflow.log_text(answer, \"response.txt\")\n",
    "    \n",
    "    print(f\"Answer:\\n{answer}\")\n",
    "    print(\"\\nâœ… Template-based prompt logged!\")\n",
    "    print(\"\\nğŸ“‹ Template info:\")\n",
    "    print(f\"   Variables: {template.variables}\")\n",
    "    print(f\"   Version: {template.metadata['version']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Prompt Versioning\n",
    "\n",
    "Track prompt evolution over time, with more specificity such as role, question, answer, and guidlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Testing prompt versions...\n",
      "\n",
      "v1.0: MLflow is an open-source platform for managing the end-to-en... (705 chars)\n",
      "ğŸƒ View run prompt_v1.0 at: http://localhost:5000/#/experiments/8/runs/2e1eded2b36d4d2db7029931c54beded\n",
      "ğŸ§ª View experiment at: http://localhost:5000/#/experiments/8\n",
      "v1.1: MLflow is an open-source platform for managing the end-to-en... (195 chars)\n",
      "ğŸƒ View run prompt_v1.1 at: http://localhost:5000/#/experiments/8/runs/5d8eb55c70d04aa182c12bbf9c5cfc92\n",
      "ğŸ§ª View experiment at: http://localhost:5000/#/experiments/8\n",
      "v2.0: MLflow is an open-source platform for managing the end-to-en... (291 chars)\n",
      "ğŸƒ View run prompt_v2.0 at: http://localhost:5000/#/experiments/8/runs/778171a6817a451b8c6222cafcf9bd22\n",
      "ğŸ§ª View experiment at: http://localhost:5000/#/experiments/8\n",
      "v3.0: MLflow is an open-source platform for managing the end-to-en... (337 chars)\n",
      "ğŸƒ View run prompt_v3.0 at: http://localhost:5000/#/experiments/8/runs/84252b62ee154a9387e072bc83ec878f\n",
      "ğŸ§ª View experiment at: http://localhost:5000/#/experiments/8\n",
      "\n",
      "âœ… All versions tested and logged!\n",
      "\n",
      "ğŸ“Š Compare in MLflow UI:\n",
      "   - Select all runs\n",
      "   - Compare responses\n",
      "   - Analyze metrics (length, tokens)\n",
      "   - Choose the best version!\n"
     ]
    }
   ],
   "source": [
    "# Version 1: Basic prompt, no role\n",
    "prompts = {\n",
    "    \"v1.0\": {\n",
    "        \"template\": \"Answer this question: {question}\",\n",
    "        \"changes\": \"Initial version\"\n",
    "    },\n",
    "    \"v1.1\": {\n",
    "        \"template\": \"You are helpful. Answer concisely: {question}\",\n",
    "        \"changes\": \"Added role and instruction for conciseness\"\n",
    "    },\n",
    "# version 2: with role, question and answer\n",
    "    \"v2.0\": {\n",
    "        \"template\": \"\"\"You are a helpful AI assistant. Answer the question concisely and accurately.\n",
    "        Question: {question}\n",
    "        Answer:\"\"\",\n",
    "        \"changes\": \"Structured format, emphasized accuracy\"\n",
    "    },\n",
    "# version 3: with role, question and answer, and guidelines\n",
    "    \"v3.0\": {\n",
    "        \"template\": \"\"\"You are a helpful AI assistant specializing in technical topics.\n",
    "\n",
    "        Guidelines:\n",
    "            - Answer concisely (2-3 sentences max)\n",
    "            - Be accurate and factual\n",
    "            - If unsure, say so\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\"\"\",\n",
    "        \"changes\": \"Added specialization, explicit guidelines\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Test all versions\n",
    "question = \"What is MLflow?\"\n",
    "results = []\n",
    "\n",
    "print(\"\\nğŸ”„ Testing prompt versions...\\n\")\n",
    "\n",
    "for version, prompt_info in prompts.items():\n",
    "    with mlflow.start_run(run_name=f\"prompt_{version}\") as run:\n",
    "        \n",
    "        # Log prompt metadata\n",
    "        mlflow.log_param(\"prompt_version\", version)\n",
    "        mlflow.log_param(\"changes\", prompt_info[\"changes\"])\n",
    "        \n",
    "        # Log template\n",
    "        mlflow.log_text(prompt_info[\"template\"], \"prompt_template.txt\")\n",
    "        \n",
    "        # Fill and use template\n",
    "        filled_prompt = prompt_info[\"template\"].format(question=question)\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": filled_prompt}],\n",
    "            temperature=1.0\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content\n",
    "        tokens = response.usage.total_tokens\n",
    "        \n",
    "        # Log results\n",
    "        mlflow.log_text(answer, \"response.txt\")\n",
    "        mlflow.log_metric(\"response_length\", len(answer))\n",
    "        mlflow.log_metric(\"total_tokens\", tokens)\n",
    "        \n",
    "        results.append({\n",
    "            \"version\": version,\n",
    "            \"answer\": answer,\n",
    "            \"length\": len(answer),\n",
    "            \"tokens\": tokens\n",
    "        })\n",
    "        \n",
    "        print(f\"{version}: {answer[:60]}... ({len(answer)} chars)\")\n",
    "\n",
    "print(\"\\nâœ… All versions tested and logged!\")\n",
    "print(\"\\nğŸ“Š Compare in MLflow UI:\")\n",
    "print(\"   - Select all runs\")\n",
    "print(\"   - Compare responses\")\n",
    "print(\"   - Analyze metrics (length, tokens)\")\n",
    "print(\"   - Choose the best version!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ˆ Prompt Version Analysis\n",
    "\n",
    "By versioning prompts, you can:\n",
    "\n",
    "1. **Track evolution**: See how prompts improve over time\n",
    "2. **A/B test**: Compare versions systematically\n",
    "3. **Rollback**: Revert to previous version if needed\n",
    "4. **Document changes**: Know why each change was made\n",
    "5. **Measure impact**: Did the change improve quality?\n",
    "\n",
    "**All visible in MLflow experiments!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Prompt Library\n",
    "\n",
    "Create a reusable prompt library for your organization, a practice similar to create resuable or shareable code library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Prompt library saved!\n",
      "\n",
      "ğŸ“š Available prompts:\n",
      "   - qa_simple: Simple Q&A\n",
      "   - qa_with_context: RAG Q&A\n",
      "   - summarization: Text summarization\n",
      "   - classification: Text classification\n",
      "   - creative_writing: Creative content generation\n",
      "\n",
      "ğŸ”— Library saved to run: 6c719c8269ae4dab83ac79c9653f078b\n",
      "ğŸƒ View run prompt-library at: http://localhost:5000/#/experiments/8/runs/6c719c8269ae4dab83ac79c9653f078b\n",
      "ğŸ§ª View experiment at: http://localhost:5000/#/experiments/8\n"
     ]
    }
   ],
   "source": [
    "# Prompt library\n",
    "PROMPT_LIBRARY = {\n",
    "    \"qa_simple\": {\n",
    "        \"template\": \"Answer this question: {question}\",\n",
    "        \"variables\": [\"question\"],\n",
    "        \"use_case\": \"Simple Q&A\",\n",
    "        \"recommended_temp\": 1.0\n",
    "    },\n",
    "    \"qa_with_context\": {\n",
    "        \"template\": \"\"\"Use the following context to answer the question.\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer based only on the context above:\"\"\",\n",
    "        \"variables\": [\"context\", \"question\"],\n",
    "        \"use_case\": \"RAG Q&A\",\n",
    "        \"recommended_temp\": 0.1\n",
    "    },\n",
    "    \"summarization\": {\n",
    "        \"template\": \"\"\"Summarize the following text in {length} sentences.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Summary:\"\"\",\n",
    "        \"variables\": [\"text\", \"length\"],\n",
    "        \"use_case\": \"Text summarization\",\n",
    "        \"recommended_temp\": 0.5\n",
    "    },\n",
    "    \"classification\": {\n",
    "        \"template\": \"\"\"Classify the following text into one of these categories: {categories}\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Category:\"\"\",\n",
    "        \"variables\": [\"text\", \"categories\"],\n",
    "        \"use_case\": \"Text classification\",\n",
    "        \"recommended_temp\": 0.0\n",
    "    },\n",
    "    \"creative_writing\": {\n",
    "        \"template\": \"\"\"Write a creative {format} about {topic}.\n",
    "\n",
    "Style: {style}\n",
    "\n",
    "Your {format}:\"\"\",\n",
    "        \"variables\": [\"format\", \"topic\", \"style\"],\n",
    "        \"use_case\": \"Creative content generation\",\n",
    "        \"recommended_temp\": 0.9\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save prompt library\n",
    "with mlflow.start_run(run_name=\"prompt-library\") as run:\n",
    "    \n",
    "    mlflow.log_dict(PROMPT_LIBRARY, \"prompt_library.json\")\n",
    "    mlflow.set_tag(\"artifact_type\", \"prompt_library\")\n",
    "    mlflow.set_tag(\"num_prompts\", len(PROMPT_LIBRARY))\n",
    "    \n",
    "    print(\"âœ… Prompt library saved!\")\n",
    "    print(\"\\nğŸ“š Available prompts:\")\n",
    "    for name, prompt in PROMPT_LIBRARY.items():\n",
    "        print(f\"   - {name}: {prompt['use_case']}\")\n",
    "    \n",
    "    print(f\"\\nğŸ”— Library saved to run: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“š Testing prompt library...\n",
      "\n",
      "ğŸƒ View run using_qa_with_context at: http://localhost:5000/#/experiments/8/runs/09bc974c368146899941688813c1c34b\n",
      "ğŸ§ª View experiment at: http://localhost:5000/#/experiments/8\n",
      "RAG Q&A: MLflow Tracing provides observability for GenAI applications.\n",
      "\n",
      "ğŸƒ View run using_classification at: http://localhost:5000/#/experiments/8/runs/55954aa8160a4dbe8b228bdf1d702990\n",
      "ğŸ§ª View experiment at: http://localhost:5000/#/experiments/8\n",
      "Classification: Positive\n",
      "\n",
      "âœ… Prompt library works!\n"
     ]
    }
   ],
   "source": [
    "# Use a prompt from the library\n",
    "def use_library_prompt(prompt_name: str, **kwargs):\n",
    "    \"\"\"\n",
    "    Load and use a prompt from the library.\n",
    "    \"\"\"\n",
    "    if prompt_name not in PROMPT_LIBRARY:\n",
    "        raise ValueError(f\"Prompt '{prompt_name}' not found in library\")\n",
    "    \n",
    "    prompt_info = PROMPT_LIBRARY[prompt_name]\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"using_{prompt_name}\"):\n",
    "        \n",
    "        # Log which prompt we're using\n",
    "        mlflow.log_param(\"prompt_name\", prompt_name)\n",
    "        mlflow.log_param(\"use_case\", prompt_info[\"use_case\"])\n",
    "        mlflow.log_param(\"recommended_temp\", prompt_info[\"recommended_temp\"])\n",
    "        \n",
    "        # Fill template\n",
    "        prompt = prompt_info[\"template\"].format(**kwargs)\n",
    "        mlflow.log_text(prompt, \"filled_prompt.txt\")\n",
    "        \n",
    "        # Make LLM call\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=prompt_info[\"recommended_temp\"]\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content\n",
    "        mlflow.log_text(answer, \"response.txt\")\n",
    "        \n",
    "        return answer\n",
    "\n",
    "# Test the library\n",
    "print(\"\\nğŸ“š Testing prompt library...\\n\")\n",
    "\n",
    "# Example 1: RAG Q&A\n",
    "answer1 = use_library_prompt(\n",
    "    \"qa_with_context\",\n",
    "    context=\"MLflow Tracing provides observability for GenAI applications.\",\n",
    "    question=\"What does MLflow Tracing do?\"\n",
    ")\n",
    "print(f\"RAG Q&A: {answer1}\\n\")\n",
    "\n",
    "# Example 2: Classification\n",
    "answer2 = use_library_prompt(\n",
    "    \"classification\",\n",
    "    text=\"I love using MLflow for my ML projects!\",\n",
    "    categories=\"Positive, Negative, Neutral\"\n",
    ")\n",
    "print(f\"Classification: {answer2}\")\n",
    "\n",
    "print(\"\\nâœ… Prompt library works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“š Prompt Library Benefits\n",
    "\n",
    "A centralized prompt library provides:\n",
    "\n",
    "1. **Consistency**: Everyone uses approved prompts\n",
    "2. **Reusability**: Don't reinvent the wheel\n",
    "3. **Best practices**: Recommended settings included\n",
    "4. **Easy updates**: Change once, affect all users\n",
    "5. **Governance**: Control what gets deployed\n",
    "\n",
    "**Essential for team collaboration!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: MLflow Prompt Registry\n",
    "\n",
    "Take your prompt management to production with **MLflow's Prompt Registry** - a centralized repository that provides versioning, aliases, and search capabilities.\n",
    "\n",
    "These capablities and features do not exist if you manually creatred a Python Dict as we did in step 6\n",
    "\n",
    "### Why Use the Prompt Registry?\n",
    "\n",
    "The Prompt Registry builds on the manual approach from Step 6, adding:\n",
    "\n",
    "| Feature | Manual (Step 6) | Prompt Registry |\n",
    "|---------|-----------------|-----------------|\n",
    "| Versioning | Manual params | Automatic with history |\n",
    "| Aliases | Not available | Built-in (prod/staging) |\n",
    "| Search | Not available | Tag-based filtering |\n",
    "| UI | Artifacts tab | Dedicated Registry view |\n",
    "| Rollback | Manual | One-click alias update |\n",
    "\n",
    "Let's migrate our `PROMPT_LIBRARY` to the Prompt Registry!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/04 21:13:45 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for prompt version to finish creation. Prompt name: tutorial-qa_simple, version 1\n",
      "2026/02/04 21:13:45 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for prompt version to finish creation. Prompt name: tutorial-qa_with_context, version 1\n",
      "2026/02/04 21:13:45 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for prompt version to finish creation. Prompt name: tutorial-summarization, version 1\n",
      "2026/02/04 21:13:45 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for prompt version to finish creation. Prompt name: tutorial-classification, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Registering prompts to MLflow Prompt Registry...\n",
      "\n",
      "âœ… Registered: tutorial-qa_simple (version 1)\n",
      "âœ… Registered: tutorial-qa_with_context (version 1)\n",
      "âœ… Registered: tutorial-summarization (version 1)\n",
      "âœ… Registered: tutorial-classification (version 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/04 21:13:45 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for prompt version to finish creation. Prompt name: tutorial-creative_writing, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Registered: tutorial-creative_writing (version 1)\n",
      "\n",
      "ğŸ“‹ Total prompts registered: 5\n",
      "   View them in MLflow UI â†’ Prompt Registry\n"
     ]
    }
   ],
   "source": [
    "# Register prompts from PROMPT_LIBRARY to MLflow Prompt Registry\n",
    "# Note: Prompt Registry uses Jinja2 syntax {{ variable }} instead of Python {variable}\n",
    "\n",
    "import re\n",
    "\n",
    "def convert_to_jinja2(template: str) -> str:\n",
    "    \"\"\"Convert Python format {var} to Jinja2 {{ var }} syntax.\"\"\"\n",
    "    # Match {word} but not {{ or }}\n",
    "    return re.sub(r'\\{(\\w+)\\}', r'{{ \\1 }}', template)\n",
    "\n",
    "print(\"ğŸ”„ Registering prompts to MLflow Prompt Registry...\\n\")\n",
    "\n",
    "registered_prompts = {}\n",
    "for name, prompt_info in PROMPT_LIBRARY.items():\n",
    "    prompt_name = f\"tutorial-{name}\"\n",
    "    \n",
    "    # Convert template to Jinja2 syntax\n",
    "    jinja_template = convert_to_jinja2(prompt_info[\"template\"])\n",
    "    \n",
    "    # Register to Prompt Registry\n",
    "    prompt = mlflow.genai.register_prompt(\n",
    "        name=prompt_name,\n",
    "        template=jinja_template,\n",
    "        commit_message=f\"Initial version - {prompt_info['use_case']}\",\n",
    "        tags={\n",
    "            \"use_case\": prompt_info[\"use_case\"],\n",
    "            \"recommended_temp\": str(prompt_info[\"recommended_temp\"]),\n",
    "            \"variables\": \",\".join(prompt_info[\"variables\"]),\n",
    "            \"author\": \"jules\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    registered_prompts[name] = prompt\n",
    "    print(f\"âœ… Registered: {prompt_name} (version {prompt.version})\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ Total prompts registered: {len(registered_prompts)}\")\n",
    "print(\"   View them in MLflow UI â†’ Prompt Registry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Using Registry Prompts\n",
    "\n",
    "Load prompts by version or alias, then use `format()` to fill variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Loading and using a prompt from the Registry...\n",
      "\n",
      "Loaded prompt: tutorial-qa_with_context\n",
      "   Version: 1\n",
      "   Template preview: Use the following context to answer the question.\n",
      "...\n",
      "\n",
      "ğŸ“ Filled prompt:\n",
      "Use the following context to answer the question.\n",
      "Context: MLflow Tracing provides observability for GenAI applications. It captures execution traces, latency metrics, and token usage.\n",
      "\n",
      "Question: What does MLflow Tracing provide?\n",
      "\n",
      "Answer based only on the context above:\n",
      "\n",
      "ğŸ¤– Answer: MLflow Tracing provides observability for GenAI applications by capturing execution traces, latency metrics, and token usage.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-c73034a3bbba5c5a441edd4e3cb8bbfd&amp;experiment_id=8&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-c73034a3bbba5c5a441edd4e3cb8bbfd)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a prompt from the registry and use it\n",
    "import mlflow\n",
    "\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "print(\"ğŸ“¥ Loading and using a prompt from the Registry...\\n\")\n",
    "\n",
    "# Load by name (gets latest version)\n",
    "prompt = mlflow.genai.load_prompt(\"tutorial-qa_with_context\")\n",
    "\n",
    "print(\"Loaded prompt: tutorial-qa_with_context\")\n",
    "print(f\"   Version: {prompt.version}\")\n",
    "print(f\"   Template preview: {prompt.template[:50]}...\")\n",
    "\n",
    "# Fill the template with variables\n",
    "filled_prompt = prompt.format(\n",
    "    context=\"MLflow Tracing provides observability for GenAI applications. It captures execution traces, latency metrics, and token usage.\",\n",
    "    question=\"What does MLflow Tracing provide?\"\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“ Filled prompt:\\n{filled_prompt}\")\n",
    "\n",
    "# Use the filled prompt with the LLM\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\"role\": \"user\", \"content\": filled_prompt}],\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(f\"\\nğŸ¤– Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aliases for Environment Management\n",
    "\n",
    "Aliases let you reference prompts by environment (e.g., `production`, `staging`) instead of version numbers. This enables safe deployments and easy rollbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ·ï¸ Setting up aliases for environment management...\n",
      "\n",
      "âœ… Set 'production' alias â†’ version 1\n",
      "âœ… Set 'staging' alias â†’ version 1\n",
      "\n",
      "ğŸ“¥ Loading prompt via alias...\n",
      "   Loaded production prompt (version 1)\n",
      "\n",
      "ğŸ¤– Production response: MLflow is an open source platform for the machine learning (ML) lifecycle.\n",
      "\n",
      "ğŸ’¡ Tip: Update the alias to point to a new version for safe rollouts!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-4a11504cf985427d24046915622debaa&amp;experiment_id=8&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-4a11504cf985427d24046915622debaa)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Set up aliases for the RAG Q&A prompt\n",
    "print(\"ğŸ·ï¸ Setting up aliases for environment management...\\n\")\n",
    "\n",
    "# Set version 1 as production\n",
    "mlflow.genai.set_prompt_alias(\"tutorial-qa_with_context\", \"production\", version=1)\n",
    "print(\"âœ… Set 'production' alias â†’ version 1\")\n",
    "\n",
    "# Set version 1 as staging (in real use, you'd have a newer version here)\n",
    "mlflow.genai.set_prompt_alias(\"tutorial-qa_with_context\", \"staging\", version=1)\n",
    "print(\"âœ… Set 'staging' alias â†’ version 1\")\n",
    "\n",
    "# Load prompt via alias - this is what production code should use!\n",
    "print(\"\\nğŸ“¥ Loading prompt via alias...\")\n",
    "prod_prompt = mlflow.genai.load_prompt(\"prompts:/tutorial-qa_with_context@production\")\n",
    "print(f\"   Loaded production prompt (version {prod_prompt.version})\")\n",
    "\n",
    "# Use the production prompt\n",
    "filled = prod_prompt.format(\n",
    "    context=\"MLflow is an open source platform for the ML lifecycle.\",\n",
    "    question=\"What is MLflow?\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\"role\": \"user\", \"content\": filled}],\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ¤– Production response: {response.choices[0].message.content}\")\n",
    "print(\"\\nğŸ’¡ Tip: Update the alias to point to a new version for safe rollouts!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versioning with Commit Messages\n",
    "\n",
    "Register improved versions with descriptive commit messages to track changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register an improved version of the RAG Q&A prompt\n",
    "print(\"ğŸ“ Creating a new version with improvements...\\n\")\n",
    "\n",
    "improved_template = \"\"\"You are a helpful AI assistant. Use ONLY the provided context to answer.\n",
    "\n",
    "Context:\n",
    "{{ context }}\n",
    "\n",
    "Question: {{ question }}\n",
    "\n",
    "Instructions:\n",
    "- Answer based strictly on the context above\n",
    "- If the answer is not in the context, say \"I don't have enough information\"\n",
    "- Be concise (2-3 sentences max)\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Register as version 2\n",
    "updated_prompt = mlflow.genai.register_prompt(\n",
    "    name=\"tutorial-qa_with_context\",\n",
    "    template=improved_template,\n",
    "    commit_message=\"Added instructions for stricter context adherence and conciseness\",\n",
    "    tags={\n",
    "        \"use_case\": \"RAG Q&A\",\n",
    "        \"recommended_temp\": \"0.1\",\n",
    "        \"variables\": \"context,question\",\n",
    "        \"author\": \"jules\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"âœ… Registered version {updated_prompt.version}\")\n",
    "print(\"   Commit: Added instructions for stricter context adherence\")\n",
    "\n",
    "# Now promote to staging for testing\n",
    "mlflow.genai.set_prompt_alias(\"tutorial-qa_with_context\", \"staging\", version=updated_prompt.version)\n",
    "print(f\"\\nğŸ·ï¸ Promoted version {updated_prompt.version} to 'staging' alias\")\n",
    "print(\"   Production still points to version 1 (safe rollout!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching Prompts\n",
    "\n",
    "Discover prompts across your organization using tag-based search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Searching prompts in the Registry...\n",
      "\n",
      "Found 5 prompts by author 'jules':\n",
      "   - tutorial-classification\n",
      "   - tutorial-creative_writing\n",
      "   - tutorial-qa_simple\n",
      "   - tutorial-qa_with_context\n",
      "   - tutorial-summarization\n",
      "\n",
      "ğŸ’¡ Search tips:\n",
      "   - Filter by author: tags.author='jules'\n",
      "   - Filter by use case: tags.use_case='RAG Q&A'\n",
      "   - Combine filters: tags.author='jules' AND tags.use_case='RAG Q&A'\n"
     ]
    }
   ],
   "source": [
    "# Search for prompts by tags\n",
    "print(\"ğŸ” Searching prompts in the Registry...\\n\")\n",
    "\n",
    "# Search all prompts by author\n",
    "all_prompts = mlflow.genai.search_prompts(filter_string=\"tags.author='jules'\")\n",
    "\n",
    "print(f\"Found {len(all_prompts)} prompts by author 'jules':\")\n",
    "for p in all_prompts:\n",
    "    print(f\"   - {p.name}\")\n",
    "\n",
    "# You can also filter by use_case, or any other tag\n",
    "print(\"\\nğŸ’¡ Search tips:\")\n",
    "print(\"   - Filter by author: tags.author='jules'\")\n",
    "print(\"   - Filter by use case: tags.use_case='RAG Q&A'\")\n",
    "print(\"   - Combine filters: tags.author='jules' AND tags.use_case='RAG Q&A'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Best Practices\n",
    "\n",
    "Let's review prompt management best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘         Prompt Management Best Practices                     â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "ğŸ“ CREATION:\n",
      "\n",
      "1. Use descriptive names\n",
      "   âœ… \"rag_qa_with_context_v2\"\n",
      "   âŒ \"prompt1\"\n",
      "\n",
      "2. Document purpose and use case\n",
      "   - What task is this for?\n",
      "   - What model works best?\n",
      "   - What temperature is recommended?\n",
      "\n",
      "3. Define clear variables\n",
      "   - Use {descriptive_names}\n",
      "   - Document expected format\n",
      "   - Validate inputs\n",
      "\n",
      "4. Include examples in metadata\n",
      "   - Show good inputs\n",
      "   - Show expected outputs\n",
      "\n",
      "ğŸ”„ VERSIONING:\n",
      "\n",
      "1. Use semantic versioning\n",
      "   - v1.0: Major version (breaking changes)\n",
      "   - v1.1: Minor version (improvements)\n",
      "   - v1.1.1: Patch (bug fixes)\n",
      "\n",
      "2. Document changes\n",
      "   - What changed?\n",
      "   - Why did it change?\n",
      "   - What's the expected impact?\n",
      "\n",
      "3. Test before deploying\n",
      "   - Compare with previous version\n",
      "   - Measure quality metrics\n",
      "   - Get stakeholder approval\n",
      "\n",
      "ğŸ“Š TRACKING:\n",
      "\n",
      "1. Link prompts to experiments\n",
      "   - Log prompt version in every run\n",
      "   - Enable filtering by prompt\n",
      "\n",
      "2. Track performance\n",
      "   - Quality metrics\n",
      "   - Token usage\n",
      "   - Latency\n",
      "   - Cost\n",
      "\n",
      "3. Monitor in production\n",
      "   - Which prompts are used most?\n",
      "   - Are there quality issues?\n",
      "   - When to update?\n",
      "\n",
      "ğŸ‘¥ COLLABORATION:\n",
      "\n",
      "1. Centralize prompt storage\n",
      "   - One source of truth\n",
      "   - MLflow artifacts or dedicated system\n",
      "\n",
      "2. Implement review process\n",
      "   - Peer review new prompts\n",
      "   - Test thoroughly\n",
      "   - Document decisions\n",
      "\n",
      "3. Share learnings\n",
      "   - What works well?\n",
      "   - What to avoid?\n",
      "   - Common patterns?\n",
      "\n",
      "ğŸš€ DEPLOYMENT:\n",
      "\n",
      "1. Gradual rollout\n",
      "   - Test with small percentage\n",
      "   - Monitor closely\n",
      "   - Be ready to rollback\n",
      "\n",
      "2. Have fallback prompts\n",
      "   - If new prompt fails\n",
      "   - Automatic revert?\n",
      "   - Monitoring alerts?\n",
      "\n",
      "3. Document production prompts\n",
      "   - Which version is live?\n",
      "   - When was it deployed?\n",
      "   - What's the rollback plan?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘         Prompt Management Best Practices                     â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ğŸ“ CREATION:\n",
    "\n",
    "1. Use descriptive names\n",
    "   âœ… \"rag_qa_with_context_v2\"\n",
    "   âŒ \"prompt1\"\n",
    "\n",
    "2. Document purpose and use case\n",
    "   - What task is this for?\n",
    "   - What model works best?\n",
    "   - What temperature is recommended?\n",
    "\n",
    "3. Define clear variables\n",
    "   - Use {descriptive_names}\n",
    "   - Document expected format\n",
    "   - Validate inputs\n",
    "\n",
    "4. Include examples in metadata\n",
    "   - Show good inputs\n",
    "   - Show expected outputs\n",
    "\n",
    "ğŸ”„ VERSIONING:\n",
    "\n",
    "1. Use semantic versioning\n",
    "   - v1.0: Major version (breaking changes)\n",
    "   - v1.1: Minor version (improvements)\n",
    "   - v1.1.1: Patch (bug fixes)\n",
    "\n",
    "2. Document changes\n",
    "   - What changed?\n",
    "   - Why did it change?\n",
    "   - What's the expected impact?\n",
    "\n",
    "3. Test before deploying\n",
    "   - Compare with previous version\n",
    "   - Measure quality metrics\n",
    "   - Get stakeholder approval\n",
    "\n",
    "ğŸ“Š TRACKING:\n",
    "\n",
    "1. Link prompts to experiments\n",
    "   - Log prompt version in every run\n",
    "   - Enable filtering by prompt\n",
    "\n",
    "2. Track performance\n",
    "   - Quality metrics\n",
    "   - Token usage\n",
    "   - Latency\n",
    "   - Cost\n",
    "\n",
    "3. Monitor in production\n",
    "   - Which prompts are used most?\n",
    "   - Are there quality issues?\n",
    "   - When to update?\n",
    "\n",
    "ğŸ‘¥ COLLABORATION:\n",
    "\n",
    "1. Centralize prompt storage\n",
    "   - One source of truth\n",
    "   - MLflow artifacts or dedicated system\n",
    "\n",
    "2. Implement review process\n",
    "   - Peer review new prompts\n",
    "   - Test thoroughly\n",
    "   - Document decisions\n",
    "\n",
    "3. Share learnings\n",
    "   - What works well?\n",
    "   - What to avoid?\n",
    "   - Common patterns?\n",
    "\n",
    "ğŸš€ DEPLOYMENT:\n",
    "\n",
    "1. Gradual rollout\n",
    "   - Test with small percentage\n",
    "   - Monitor closely\n",
    "   - Be ready to rollback\n",
    "\n",
    "2. Have fallback prompts\n",
    "   - If new prompt fails\n",
    "   - Automatic revert?\n",
    "   - Monitoring alerts?\n",
    "\n",
    "3. Document production prompts\n",
    "   - Which version is live?\n",
    "   - When was it deployed?\n",
    "   - What's the rollback plan?\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. âœ… Why prompt management is crucial for GenAI applications\n",
    "2. âœ… How to log prompts as artifacts in MLflow\n",
    "3. âœ… Creating flexible prompt templates with variables\n",
    "4. âœ… Systematic prompt versioning\n",
    "5. âœ… Building a reusable prompt library\n",
    "6. âœ… **Using MLflow Prompt Registry** for production prompt management\n",
    "7. âœ… **Registering and versioning prompts** with commit messages\n",
    "8. âœ… **Using aliases** (production/staging) for safe deployments\n",
    "9. âœ… **Searching prompts** by tags for discovery\n",
    "10. âœ… Best practices for team collaboration\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Treat prompts as code**: Version, test, review\n",
    "- **Use Prompt Registry**: Centralized, versioned, searchable\n",
    "- **Leverage aliases**: Safe deployments with production/staging\n",
    "- **Track everything**: Link prompts to experiments\n",
    "- **Iterate systematically**: A/B test versions\n",
    "- **Document thoroughly**: Purpose, use case, commit messages\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**ğŸ““ Notebook 1.6: Framework Integrations**\n",
    "\n",
    "Learn how to:\n",
    "- Work with different GenAI frameworks\n",
    "- Compare OpenAI, LangChain, LlamaIndex\n",
    "- Choose the right tool for your use case\n",
    "- Integrate MLflow with your framework\n",
    "- Best practices for each framework"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
