{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1.8: Prompt Optimization with GEPA\n",
    "\n",
    "![](images/9_Prompt-Optimization-with-GEPA.png)\n",
    "\n",
    "## Automatically Improve Prompts Using MLflow's GEPA Integration\n",
    "\n",
    "In Tutorial 1.5, we manually iterated on prompts — writing better versions by hand and versioning them in the Prompt Registry. But what if an algorithm could do this automatically?\n",
    "\n",
    "This notebook demonstrates **GEPA (Genetic-Pareto)**, an automatic prompt optimization algorithm integrated into MLflow via `mlflow.genai.optimize_prompts()`.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- How GEPA automatically improves prompts\n",
    "- Using `mlflow.genai.optimize_prompts()` with the Prompt Registry\n",
    "- Evaluating prompt quality with `Correctness` scorer\n",
    "- Comparing original vs. optimized prompts\n",
    "\n",
    "### Prerequisites\n",
    "- Completed Notebook 1.5 (Prompt Management) and 1.7 (Evaluating Agents)\n",
    "- Understanding of the Prompt Registry and evaluation scorers\n",
    "\n",
    "### Estimated Time: 10-15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: How GEPA Works\n",
    "\n",
    "**GEPA (Genetic-Pareto)** optimizes prompts through an iterative cycle:\n",
    "\n",
    "```\n",
    "1. EVALUATE  →  Run the prompt on training examples, score with a judge\n",
    "2. REFLECT   →  Use an LLM to analyze failures and propose improvements\n",
    "3. MUTATE    →  Generate improved prompt variations\n",
    "4. SELECT    →  Keep the best-performing candidates (Pareto-optimal)\n",
    "5. REPEAT    →  Continue until budget exhausted or convergence\n",
    "```\n",
    "\n",
    "### Manual vs. Automatic Optimization\n",
    "\n",
    "| Approach | Method | Effort | Consistency |\n",
    "|----------|--------|--------|-------------|\n",
    "| **Manual** (Notebook 1.5) | Human writes better prompts | High | Variable |\n",
    "| **GEPA** (This notebook) | Algorithm evolves prompts | Low | Systematic |\n",
    "\n",
    "### Integration with Prompt Registry\n",
    "\n",
    "GEPA works directly with MLflow's Prompt Registry:\n",
    "- **Reads** your registered prompt as the starting point\n",
    "- **Optimizes** it through the evaluate-reflect-mutate cycle\n",
    "- **Registers** the improved version automatically as a new version\n",
    "\n",
    "> **Note:** GEPA requires the `gepa` package. Install it with: `pip install gepa`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "from utils.clnt_utils import is_databricks_ai_gateway_client, get_databricks_ai_gateway_client, get_openai_client, get_ai_gateway_model_names\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"09-prompt-optimization\")\n",
    "\n",
    "# Configure client and model based on provider\n",
    "use_databricks_provider = is_databricks_ai_gateway_client()\n",
    "if use_databricks_provider:\n",
    "    client = get_databricks_ai_gateway_client()\n",
    "    model_name = get_ai_gateway_model_names()[0]\n",
    "    optimizer_model = f\"databricks:/{model_name}\"\n",
    "else:\n",
    "    client = get_openai_client()\n",
    "    model_name = \"gpt-5-mini\"\n",
    "    optimizer_model = f\"openai:/{model_name}\"\n",
    "\n",
    "# Enable autologging\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "print(\"\\u2705 Environment configured\")\n",
    "print(f\"   Provider: {'Databricks AI Gateway' if use_databricks_provider else 'OpenAI'}\")\n",
    "print(f\"   Model: {model_name}\")\n",
    "print(f\"   Optimizer model: {optimizer_model}\")\n",
    "print(f\"   Tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Register a Baseline Prompt\n",
    "\n",
    "We'll use the same basic Q&A prompt from Notebook 1.5's Prompt Library (`qa_simple`). We register it fresh here so this notebook is self-contained.\n",
    "\n",
    "This minimal prompt is an ideal optimization target — it has maximum room for GEPA to improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the baseline prompt (same template as qa_simple from Notebook 1.5)\n",
    "baseline_prompt = mlflow.genai.register_prompt(\n",
    "    name=\"gepa-qa-simple\",\n",
    "    template=\"Answer this question: {{ question }}\",\n",
    "    commit_message=\"Baseline prompt for GEPA optimization\",\n",
    "    tags={\"author\": \"jules\", \"use_case\": \"Simple Q&A\", \"status\": \"baseline\"}\n",
    ")\n",
    "\n",
    "print(\"\\u2705 Baseline prompt registered\")\n",
    "print(f\"   Name: {baseline_prompt.name}\")\n",
    "print(f\"   Version: {baseline_prompt.version}\")\n",
    "print(f\"   URI: {baseline_prompt.uri}\")\n",
    "print(f\"   Template: '{baseline_prompt.template}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Prepare Training Data and Predict Function\n",
    "\n",
    "GEPA needs two things:\n",
    "1. **Training data** — example input/output pairs so it can evaluate prompt quality\n",
    "2. **Predict function** — a callable that loads the prompt, fills it, and calls the LLM\n",
    "\n",
    "### Why Training Data Design Matters\n",
    "\n",
    "GEPA improves prompts by finding gaps between actual outputs and expected responses.\n",
    "If the baseline prompt already produces near-perfect answers (common with powerful LLMs),\n",
    "GEPA has **no signal to improve** and will register the original template unchanged.\n",
    "\n",
    "To give GEPA room to work, our training examples require **specific structure and detail**\n",
    "that the bare-bones prompt `\"Answer this question: ...\"` won't naturally produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai import optimize_prompts\n",
    "from mlflow.genai.optimize.optimizers import GepaPromptOptimizer\n",
    "from mlflow.genai.scorers import Correctness\n",
    "\n",
    "# Training data: questions that require STRUCTURED, DETAILED responses.\n",
    "# The bare-bones baseline prompt won't guide the LLM to produce these formats,\n",
    "# giving GEPA a clear signal to add structure/instructions to the prompt.\n",
    "train_data = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Explain MLflow Tracking to a beginner in exactly 3 sentences.\"},\n",
    "        \"expectations\": {\n",
    "            \"expected_response\": (\n",
    "                \"MLflow Tracking is a component that automatically logs your machine learning experiments. \"\n",
    "                \"It records parameters, metrics, and model artifacts so you can compare different runs side by side. \"\n",
    "                \"You can view all your experiments through the MLflow UI dashboard.\"\n",
    "            )\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"List the top 3 benefits of using vector embeddings. Number each benefit.\"},\n",
    "        \"expectations\": {\n",
    "            \"expected_response\": (\n",
    "                \"1. Semantic similarity — embeddings capture meaning, so related concepts like 'happy' and 'joyful' are close in vector space.\\n\"\n",
    "                \"2. Efficiency — they compress high-dimensional sparse data into compact dense vectors, making computation faster.\\n\"\n",
    "                \"3. Transfer learning — pre-trained embeddings can be fine-tuned for specific downstream tasks without training from scratch.\"\n",
    "            )\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Compare RAG and fine-tuning in exactly 2 sentences.\"},\n",
    "        \"expectations\": {\n",
    "            \"expected_response\": (\n",
    "                \"RAG retrieves relevant documents at inference time to augment the LLM's context, \"\n",
    "                \"while fine-tuning modifies the model's weights on domain-specific training data. \"\n",
    "                \"RAG allows dynamic knowledge updates without retraining, while fine-tuning creates a \"\n",
    "                \"specialized model that may lose some general capabilities.\"\n",
    "            )\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is prompt engineering? Answer with a definition followed by 2 best practices.\"},\n",
    "        \"expectations\": {\n",
    "            \"expected_response\": (\n",
    "                \"Prompt engineering is the practice of designing and refining instructions given to LLMs \"\n",
    "                \"to produce desired outputs reliably.\\n\\n\"\n",
    "                \"Best practices:\\n\"\n",
    "                \"1. Be specific and explicit — clearly state the format, length, and style you expect.\\n\"\n",
    "                \"2. Provide examples — include one or two input/output examples to guide the model's behavior.\"\n",
    "            )\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Describe the MLflow Model Registry in one paragraph of 3-4 sentences.\"},\n",
    "        \"expectations\": {\n",
    "            \"expected_response\": (\n",
    "                \"The MLflow Model Registry is a centralized store for managing the full lifecycle of ML models. \"\n",
    "                \"It provides model versioning, so you can track how models evolve over time. \"\n",
    "                \"Teams can transition models through stages like Staging and Production using aliases. \"\n",
    "                \"It also supports annotations and approval workflows for collaborative model governance.\"\n",
    "            )\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is a vector database? Structure your answer as: Definition, then Use Cases (2 bullet points).\"},\n",
    "        \"expectations\": {\n",
    "            \"expected_response\": (\n",
    "                \"A vector database is a specialized database designed to store, index, and efficiently query \"\n",
    "                \"high-dimensional vector embeddings.\\n\\n\"\n",
    "                \"Use cases:\\n\"\n",
    "                \"• Semantic search — finding documents or products by meaning rather than keyword matching.\\n\"\n",
    "                \"• RAG pipelines — retrieving relevant context for LLM generation to improve answer accuracy.\"\n",
    "            )\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# Predict function: GEPA calls this repeatedly during optimization.\n",
    "# During optimization, GEPA patches PromptVersion.template so that\n",
    "# load_prompt() returns the MUTATED template instead of the original.\n",
    "def predict_qa(question: str) -> str:\n",
    "    \"\"\"Load the prompt from the registry, fill it, and call the LLM.\"\"\"\n",
    "    prompt = mlflow.genai.load_prompt(baseline_prompt.uri)\n",
    "    filled = prompt.format(question=question)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": filled}],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "print(f\"\\u2705 Training data prepared: {len(train_data)} examples\")\n",
    "print(\"   (Questions require specific structure/format the baseline prompt can't guide)\")\n",
    "print(\"\\u2705 Predict function defined\")\n",
    "print(f\"   Loads prompt from: {baseline_prompt.uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Run GEPA Optimization\n",
    "\n",
    "Now we run the optimization. GEPA will:\n",
    "1. Evaluate the baseline prompt using the `Correctness` scorer\n",
    "2. Reflect on failures and generate improved variations\n",
    "3. Select the best candidates and repeat\n",
    "4. Register the optimized prompt as a new version in the Prompt Registry\n",
    "\n",
    "> **Note:** This may take 3-5 minutes. The training examples require structured responses\n",
    "> that the bare-bones prompt can't produce well, giving GEPA a clear optimization signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import logging\nfrom ipykernel.iostream import OutStream\n\n# === Fix for GEPA Unicode surrogate characters ===\n# GEPA's internal output contains Unicode surrogates that crash Jupyter's\n# ZMQ/tornado JSON encoder. We fix at three levels:\n\ndef _sanitize_surrogates(obj):\n    \"\"\"Recursively replace Unicode surrogates in an object tree.\"\"\"\n    if isinstance(obj, str):\n        return obj.encode(\"utf-8\", errors=\"replace\").decode(\"utf-8\")\n    elif isinstance(obj, bytes):\n        return obj\n    elif isinstance(obj, dict):\n        return {_sanitize_surrogates(k): _sanitize_surrogates(v) for k, v in obj.items()}\n    elif isinstance(obj, (list, tuple)):\n        return type(obj)(_sanitize_surrogates(item) for item in obj)\n    return obj\n\n# Level 1: Patch OutStream.write at the CLASS level to sanitize all output.\n# This ensures surrogates are stripped before they reach ipykernel's buffer,\n# regardless of how write() is called (instance, class, or thread).\n_orig_outstream_write = OutStream.write\n\ndef _safe_outstream_write(self, string):\n    if isinstance(string, str):\n        string = string.encode(\"utf-8\", errors=\"replace\").decode(\"utf-8\")\n    return _orig_outstream_write(self, string)\n\nOutStream.write = _safe_outstream_write\n\n# Level 2: Patch the kernel session's pack function to handle surrogates\n# in ZMQ/IOPub messages. This is where the serialization error actually\n# occurs — session.pack calls orjson_packer/json_packer which choke on\n# surrogate characters. We catch the error and sanitize on retry.\ntry:\n    _kernel = get_ipython().kernel\n    _orig_pack = _kernel.session.pack\n\n    def _safe_pack(obj):\n        try:\n            return _orig_pack(obj)\n        except (UnicodeEncodeError, TypeError):\n            return _orig_pack(_sanitize_surrogates(obj))\n\n    _kernel.session.pack = _safe_pack\nexcept Exception:\n    pass  # Not in a Jupyter kernel context\n\n# Level 3: Suppress async/tornado error log messages for any surrogates\n# that slip through to non-stdout kernel messages (non-fatal noise).\nfor _logger_name in (\"tornado.general\", \"tornado.application\", \"asyncio\"):\n    logging.getLogger(_logger_name).setLevel(logging.CRITICAL)\n\n# Run GEPA prompt optimization\nprint(\"\\U0001f504 Running GEPA prompt optimization...\\n\")\nprint(\"   This will iterate through evaluate \\u2192 reflect \\u2192 mutate \\u2192 select cycles.\")\nprint(\"   Budget: 100 metric calls (may take 3-5 minutes)\\n\")\n\nresult = optimize_prompts(\n    predict_fn=predict_qa,\n    train_data=train_data,\n    prompt_uris=[baseline_prompt.uri],\n    optimizer=GepaPromptOptimizer(\n        reflection_model=optimizer_model,\n        max_metric_calls=100,\n        display_progress_bar=False,\n    ),\n    scorers=[Correctness(model=optimizer_model)],\n)\n\nprint(\"\\n\\u2705 GEPA optimization complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Compare Original vs. Optimized Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe(s):\n",
    "    \"\"\"Strip Unicode surrogates for safe display in Jupyter.\"\"\"\n",
    "    if isinstance(s, str):\n",
    "        return s.encode(\"utf-8\", errors=\"replace\").decode(\"utf-8\")\n",
    "    return str(s)\n",
    "\n",
    "# Display before/after comparison\n",
    "print(\"=\" * 70)\n",
    "print(\"\\ud83d\\udcca GEPA Optimization Results\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n\\ud83d\\udcc8 Score Improvement:\")\n",
    "if result.initial_eval_score is not None:\n",
    "    print(f\"   Initial score: {result.initial_eval_score:.3f}\")\n",
    "else:\n",
    "    print(\"   Initial score: N/A\")\n",
    "if result.final_eval_score is not None:\n",
    "    print(f\"   Final score:   {result.final_eval_score:.3f}\")\n",
    "else:\n",
    "    print(\"   Final score:   N/A\")\n",
    "if result.initial_eval_score is not None and result.final_eval_score is not None:\n",
    "    improvement = result.final_eval_score - result.initial_eval_score\n",
    "    print(f\"   Improvement:   {improvement:+.3f}\")\n",
    "\n",
    "# Load the optimized prompt directly from the registry to ensure we\n",
    "# see the actual registered version (not just the in-memory object)\n",
    "optimized = result.optimized_prompts[0]\n",
    "registry_prompt = mlflow.genai.load_prompt(f\"prompts:/{optimized.name}/{optimized.version}\")\n",
    "\n",
    "print(f\"\\n\\ud83d\\udcdd Original Prompt (version {baseline_prompt.version}):\")\n",
    "print(f\"   '{baseline_prompt.template}'\")\n",
    "\n",
    "print(f\"\\n\\ud83d\\ude80 Optimized Prompt (version {optimized.version}):\")\n",
    "print(f\"   '{_safe(registry_prompt.template)}'\")\n",
    "\n",
    "if baseline_prompt.template.strip() == _safe(registry_prompt.template).strip():\n",
    "    print(\"\\n\\u26a0\\ufe0f  Note: The optimized template is identical to the baseline.\")\n",
    "    print(\"   This can happen when the baseline already scores well on the\")\n",
    "    print(\"   training data. Try adding harder examples or increasing the budget.\")\n",
    "\n",
    "print(\"\\n\\ud83d\\udd17 The optimized prompt has been automatically registered\")\n",
    "print(f\"   as version {optimized.version} in the Prompt Registry!\")\n",
    "print(f\"   View it in MLflow UI \\u2192 Prompt Registry \\u2192 {_safe(optimized.name)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\n\\ud83d\\udca1 Key Takeaway:\")\n",
    "print(\"   GEPA automatically learned to add structure, instructions,\")\n",
    "print(\"   and constraints that we would normally write by hand.\")\n",
    "print(\"   Combined with the Prompt Registry, optimized prompts are\")\n",
    "print(\"   versioned and ready for deployment via aliases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. How **GEPA** automatically optimizes prompts through evaluate-reflect-mutate cycles\n",
    "2. Using `mlflow.genai.optimize_prompts()` with the **Prompt Registry**\n",
    "3. Preparing **training data** and a **predict function** for optimization\n",
    "4. Comparing **before/after** prompt quality with the `Correctness` scorer\n",
    "5. Optimized prompts are **automatically versioned** in the registry\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Automate what you can**: GEPA systematically improves prompts that would take many manual iterations\n",
    "- **Data-driven optimization**: Training examples define what \"good\" looks like for the algorithm\n",
    "- **Registry integration**: Optimized prompts flow directly into the Prompt Registry for versioning and deployment\n",
    "- **Combine approaches**: Use GEPA for initial optimization, then fine-tune manually if needed\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**\\ud83d\\udcd3 Notebook 1.9: Complete RAG Application**\n",
    "\n",
    "Learn how to:\n",
    "- Build a full RAG pipeline with end-to-end tracing\n",
    "- Evaluate RAG quality with RAGAS metrics\n",
    "- Track performance, cost, and retrieval quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}