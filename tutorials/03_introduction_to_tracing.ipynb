{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1.3: Introduction to Tracing\n",
    "\n",
    "![](images/4_Notebook-13-Introduction-to-Tracing.png)\n",
    "\n",
    "## Observability for GenAI Applications\n",
    "\n",
    "Welcome to the tracing section! This is where MLflow really shines for GenAI development. You'll learn how to gain complete visibility into your LLM applications.\n",
    "\n",
    "### What You'll Learn\n",
    "- What is tracing and why it matters for GenAI\n",
    "- Understanding the trace data model (traces, spans, hierarchy)\n",
    "- Automatic tracing with autologging\n",
    "- Tracing multiple frameworks (OpenAI, LangChain)\n",
    "- Viewing and analyzing traces in MLflow UI\n",
    "- Debugging with traces\n",
    "\n",
    "### Prerequisites\n",
    "- Completed Notebooks 1.1 and 1.2\n",
    "- MLflow UI running\n",
    "- OpenAI API key configured\n",
    "- Or keys for your foundational model provider, e.g., Databricks\n",
    "\n",
    "### Estimated Time: 30-35 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: What is Tracing?\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Traditional experimenting tracking or logging isn't enough for LLM applications:\n",
    "\n",
    "```python\n",
    "# Traditional logging - hard to debug\n",
    "print(\"Calling LLM...\")\n",
    "response = llm.generate(prompt)\n",
    "print(f\"Response: {response}\")\n",
    "# What happened inside? How long did it take? What was sent or received?\n",
    "```\n",
    "\n",
    "### The Solution: Distributed Tracing\n",
    "\n",
    "Tracing captures the **complete execution flow** of your application:\n",
    "\n",
    "```\n",
    "Trace: RAG Application\n",
    "â”œâ”€â”€ Span 1: Embed Query [0ms - 200ms]\n",
    "â”‚   Input: \"What is MLflow?\"\n",
    "â”‚   Output: [0.123, 0.456, ...]\n",
    "â”‚   \n",
    "â”œâ”€â”€ Span 2: Retrieve Documents [200ms - 350ms]\n",
    "â”‚   Input: [0.123, 0.456, ...]\n",
    "â”‚   Output: [\"MLflow is...\", \"The platform...\"]\n",
    "â”‚   \n",
    "â””â”€â”€ Span 3: Generate Response [350ms - 1500ms]\n",
    "    Input: {query, documents}\n",
    "    Output: \"MLflow is an open source platform...\"\n",
    "    LLM: gpt-5-mini\n",
    "    Tokens: 150\n",
    "```\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "1. **Visibility**: See every step in your LLM workflow\n",
    "2. **Performance**: Identify bottlenecks and latency issues\n",
    "3. **Debugging**: Trace errors to their exact source\n",
    "4. **Cost**: Track token usage per operation\n",
    "5. **Quality**: Inspect inputs/outputs at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Understanding the Trace Data Model\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "**Trace**: A complete execution of an operation\n",
    "- Represents one request or workflow\n",
    "- Contains one or more spans\n",
    "- Has a root span\n",
    "\n",
    "**Span**: A single operation within a trace\n",
    "- Has a start and end time\n",
    "- Contains inputs and outputs\n",
    "- Has metadata (model, tokens, etc.)\n",
    "- Can have parent-child relationships\n",
    "\n",
    "**Span Attributes**: Additional metadata\n",
    "- Model name\n",
    "- Token counts\n",
    "- Temperature\n",
    "- Custom attributes\n",
    "\n",
    "### Span Types\n",
    "\n",
    "MLflow defines standard span types:\n",
    "\n",
    "- `CHAIN`: A sequence of operations\n",
    "- `LLM`: Language model call\n",
    "- `RETRIEVER`: Document retrieval\n",
    "- `EMBEDDING`: Text embedding\n",
    "- `TOOL`: Tool/function execution\n",
    "- `AGENT`: Agent reasoning\n",
    "- `PARSER`: Output parsing\n",
    "\n",
    "### Hierarchy Example\n",
    "\n",
    "```\n",
    "TRACE (root)\n",
    "â”‚\n",
    "â””â”€ SPAN: Agent Executor (AGENT)\n",
    "   â”‚\n",
    "   â”œâ”€ SPAN: Planning Step (LLM)\n",
    "   â”‚  â””â”€ attributes: {model: gpt-5, tokens: 50}\n",
    "   â”‚\n",
    "   â”œâ”€ SPAN: Tool Execution (TOOL)\n",
    "   â”‚  â””â”€ attributes: {tool: search, query: \"...\"}\n",
    "   â”‚\n",
    "   â””â”€ SPAN: Final Response (LLM)\n",
    "      â””â”€ attributes: {model: gpt-5, tokens: 150}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment configured: using Databricks client\n",
      "   MLflow version: 3.9.0\n",
      "   Tracking URI: http://localhost:5000\n",
      "   Using model: jsd-gpt-5-2\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "from utils.clnt_utils import is_databricks_ai_gateway_client, get_databricks_ai_gateway_client, get_openai_client, get_ai_gateway_model_names\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "use_databricks_provider = is_databricks_ai_gateway_client()\n",
    "if use_databricks_provider:\n",
    "    client = get_databricks_ai_gateway_client()\n",
    "    model_name = get_ai_gateway_model_names()[0]\n",
    "else:\n",
    "    # Initialize OpenAI\n",
    "    client = get_openai_client()\n",
    "    model_name = \"gpt-5-2\"\n",
    "\n",
    "print(\"âœ… Environment configured: using\", \"Databricks\" if use_databricks_provider else \"OpenAI\", \"client\")\n",
    "print(f\"   MLflow version: {mlflow.__version__}\")\n",
    "print(f\"   Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"   Using model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Your First Trace - Manual Example\n",
    "\n",
    "Before we use autologging, let's understand what tracing captures by manually logging the information. This way we understand the difference and benefits of using autlogging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Making LLM call WITHOUT tracing...\n",
      "\n",
      "Response: Distributed tracing is a way to track and correlate a single request as it flows through multiple services and components in a distributed system, recording timing and metadata for each step to help diagnose performance issues and failures.\n",
      "\n",
      "âŒ What we DON'T know:\n",
      "   - Exact timing of the call\n",
      "   - Detailed request/response structure\n",
      "   - Easy way to correlate with other operations\n",
      "   - Visual representation of execution\n"
     ]
    }
   ],
   "source": [
    "# Create experiment for tracing examples\n",
    "mlflow.set_experiment(\"06-tracing-introduction\")\n",
    "\n",
    "# Without tracing - basic call\n",
    "print(\"\\nğŸ“ Making LLM call WITHOUT tracing...\\n\")\n",
    "\n",
    "prompt = \"Explain what distributed tracing is in one sentence.\"\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=1.0,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(f\"Response: {response.choices[0].message.content}\")\n",
    "print(\"\\nâŒ What we DON'T know:\")\n",
    "print(\"   - Exact timing of the call\")\n",
    "print(\"   - Detailed request/response structure\")\n",
    "print(\"   - Easy way to correlate with other operations\")\n",
    "print(\"   - Visual representation of execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Automatic Tracing with OpenAI Autologging\n",
    "\n",
    "Now let's enable tracing with a single line of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI autologging enabled\n",
      "   All OpenAI API calls will now be automatically traced!\n"
     ]
    }
   ],
   "source": [
    "# Enable OpenAI autologging - THIS IS THE MAGIC LINE!\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "print(\"âœ… OpenAI autologging enabled\")\n",
    "print(\"   All OpenAI API calls will now be automatically traced!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Making LLM call WITH tracing...\n",
      "\n",
      "Response: Distributed tracing is a way to track and visualize a single request as it flows through multiple services and components in a distributed system, linking related operations together to diagnose latency and failures.\n",
      "\n",
      "âœ… What we NOW know:\n",
      "   âœ“ Complete request details (model, messages, parameters)\n",
      "   âœ“ Response content and metadata\n",
      "   âœ“ Token usage (prompt, completion, total)\n",
      "   âœ“ Timing information (latency)\n",
      "   âœ“ All captured automatically!\n",
      "\n",
      "ğŸ”— View trace in MLflow UI: http://localhost:5000\n",
      "   Navigate to: Traces tab\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-6e7dc2a97971aff86ceb1e3513d234f7&amp;experiment_id=6&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-6e7dc2a97971aff86ceb1e3513d234f7)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make the same call - now it's automatically traced!\n",
    "print(\"\\nğŸ” Making LLM call WITH tracing...\\n\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=1.0,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(f\"Response: {response.choices[0].message.content}\")\n",
    "print(\"\\nâœ… What we NOW know:\")\n",
    "print(\"   âœ“ Complete request details (model, messages, parameters)\")\n",
    "print(\"   âœ“ Response content and metadata\")\n",
    "print(\"   âœ“ Token usage (prompt, completion, total)\")\n",
    "print(\"   âœ“ Timing information (latency)\")\n",
    "print(\"   âœ“ All captured automatically!\")\n",
    "print(\"\\nğŸ”— View trace in MLflow UI: http://localhost:5000\")\n",
    "print(\"   Navigate to: Traces tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ‰ What Just Happened?\n",
    "\n",
    "With `mlflow.openai.autolog()`, MLflow automatically:\n",
    "\n",
    "1. **Intercepted** the OpenAI API call\n",
    "2. **Created** a trace with a unique ID\n",
    "3. **Captured** all inputs (messages, model, parameters)\n",
    "4. **Captured** all outputs (response, tokens, timing)\n",
    "5. **Stored** everything in a structured format\n",
    "6. **Made it available** in the MLflow UI\n",
    "\n",
    "**Zero code changes required!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Exploring Trace Details\n",
    "\n",
    "Let's make a more complex call and examine what gets traced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¬ Multi-turn conversation with tracing...\n",
      "\n",
      "Response: ...\n",
      "\n",
      "ğŸ“Š Trace captured:\n",
      "   Model: gpt-5.2-2025-12-11\n",
      "   Prompt tokens: 66\n",
      "   Completion tokens: 200\n",
      "   Total tokens: 266\n",
      "\n",
      "ğŸ” In the trace, you can see:\n",
      "   - All 4 messages (system + conversation history)\n",
      "   - Model configuration (temperature, max_tokens)\n",
      "   - Complete response\n",
      "   - Token breakdown\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-f315b641fc7048a87050841ed9b5f1d5&amp;experiment_id=6&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-f315b641fc7048a87050841ed9b5f1d5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Multi-turn conversation with system prompt\n",
    "print(\"\\nğŸ’¬ Multi-turn conversation with tracing...\\n\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful AI expert who explains concepts clearly and concisely.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is MLflow?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"MLflow is an open-source platform for managing the machine learning lifecycle.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What are its main components and what are they used for?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    temperature=1.0,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(f\"Response: {response.choices[0].message.content[:200]}...\")\n",
    "print(\"\\nğŸ“Š Trace captured:\")\n",
    "print(f\"   Model: {response.model}\")\n",
    "print(f\"   Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"   Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"   Total tokens: {response.usage.total_tokens}\")\n",
    "print(\"\\nğŸ” In the trace, you can see:\")\n",
    "print(\"   - All 4 messages (system + conversation history)\")\n",
    "print(\"   - Model configuration (temperature, max_tokens)\")\n",
    "print(\"   - Complete response\")\n",
    "print(\"   - Token breakdown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Tracing Multiple Sequential Calls\n",
    "\n",
    "Let's see how tracing helps with multi-step workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Multi-step workflow with automatic tracing...\n",
      "\n",
      "Step 1: Generating topic...\n",
      "  Topic: **Topic:** *AI Agents That Use Tools (Beyond Chatbots): How â€œLLM + Toolsâ€ Systems Work*\n",
      "\n",
      "Write about how modern AI goes beyond generating text by **calling external tools**â€”like web search, databases, calculators, code execution, calendars, or APIsâ€”to complete tasks. Cover:\n",
      "\n",
      "- What an â€œAI agentâ€ is vs. a standard chatbot  \n",
      "- How tool use works (function calling, planning, feedback loops)  \n",
      "- Real-world examples (customer support automation, data analysis, scheduling)  \n",
      "- Key risks and design challenges (hallucinations, permissions, security, reliability)  \n",
      "- Where this trend is headed (multi-agent systems, long-term memory, autonomy limits)\n",
      "\n",
      "Step 2: Creating outline...\n",
      "  Outline: 1) **From Chatbots to AI Agents: What Changes and Why It Matters**  \n",
      "   - Define a *standard chatbot...\n",
      "\n",
      "Step 3: Writing introduction...\n",
      "  Introduction: Modern AI is increasingly moving beyond standard chatbots that only generate text, into â€œAI agentsâ€ that can take goal-directed actions by calling external tools such as web search, databases, calculators, code execution, calendars, and APIs. In an LLM+tools setup, the model plans steps, selects the right function to call, executes it, observes results, and iterates in feedback loops until it can deliver a verified outcome rather than a best-guess response. This approach already powers practical workflows like automating customer support with account lookups and ticket updates, running data analysis by querying datasets and executing code, and handling scheduling by checking availability and creating calendar events. But tool-using agents introduce new risks and design challengesâ€”hallucinated tool calls, over-broad permissions, prompt injection and data leakage, brittle integrations, and reliability under uncertaintyâ€”so the next wave is focused on safer autonomy via stronger guardrails, multi-agent coordination, long-term memory, and clear limits on what agents can do without human approval.\n",
      "\n",
      "âœ… All three steps completed!\n",
      "\n",
      "ğŸ“Š Total tokens used:\n",
      "   1198 tokens\n",
      "\n",
      "ğŸ” View in MLflow UI:\n",
      "   You'll see THREE separate traces, one for each call\n",
      "   Each trace shows timing, tokens, and complete I/O\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-12994f2cd5b56d61ccb159750daf0bfa&amp;experiment_id=6&amp;trace_id=tr-1c7e18e0a79f24e2b0d603b99cf5f71d&amp;experiment_id=6&amp;trace_id=tr-5f0abe6d85b4816d2e3bbadbad453f1e&amp;experiment_id=6&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-12994f2cd5b56d61ccb159750daf0bfa), Trace(trace_id=tr-1c7e18e0a79f24e2b0d603b99cf5f71d), Trace(trace_id=tr-5f0abe6d85b4816d2e3bbadbad453f1e)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple multi-step workflow\n",
    "print(\"\\nğŸ”„ Multi-step workflow with automatic tracing...\\n\")\n",
    "\n",
    "# Step 1: Generate a topic\n",
    "print(\"Step 1: Generating topic...\")\n",
    "topic_response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful AI expert who explains concepts clearly and concisely.\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Suggest one interesting AI topic for a blog post.\"\n",
    "    }],\n",
    "    temperature=1.0,\n",
    "    max_tokens=200\n",
    ")\n",
    "topic = topic_response.choices[0].message.content\n",
    "print(f\"  Topic: {topic}\")\n",
    "\n",
    "# Step 2: Generate an outline\n",
    "print(\"\\nStep 2: Creating outline...\")\n",
    "outline_response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Create a 3-point outline for a blog post about: {topic}\"\n",
    "    }],\n",
    "    temperature=1.0,\n",
    "    max_tokens=500\n",
    ")\n",
    "outline = outline_response.choices[0].message.content\n",
    "print(f\"  Outline: {outline[:100]}...\")\n",
    "\n",
    "# Step 3: Write the introduction\n",
    "print(\"\\nStep 3: Writing introduction...\")\n",
    "intro_response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Write a 4-sentence introduction paragraph for: {topic}\"\n",
    "    }],\n",
    "    temperature=1.0,\n",
    "    max_tokens=2000\n",
    ")\n",
    "intro = intro_response.choices[0].message.content\n",
    "print(f\"  Introduction: {intro}\")\n",
    "\n",
    "print(\"\\nâœ… All three steps completed!\")\n",
    "print(\"\\nğŸ“Š Total tokens used:\")\n",
    "total_tokens = (topic_response.usage.total_tokens +\n",
    "                outline_response.usage.total_tokens +\n",
    "                intro_response.usage.total_tokens)\n",
    "print(f\"   {total_tokens} tokens\")\n",
    "\n",
    "print(\"\\nğŸ” View in MLflow UI:\")\n",
    "print(\"   You'll see THREE separate traces, one for each call\")\n",
    "print(\"   Each trace shows timing, tokens, and complete I/O\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Observation\n",
    "\n",
    "Each OpenAI call creates a **separate trace**. In the next notebook, we'll learn how to group related calls into a **single hierarchical trace** using manual instrumentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Tracing with Different Models\n",
    "\n",
    "Let's compare traces across different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¬ Comparing models with tracing...\n",
      "\n",
      "Testing jsd-gpt-5-2...\n",
      "  Latency: 1.07s\n",
      "  Tokens: 42\n",
      "  Response: LLM temperature controls randomness: low = more deterministic, high = more varied and creative but less predictable outputs.\n",
      "\n",
      "Testing jsd-gpt-5-mini...\n",
      "  Latency: 1.26s\n",
      "  Tokens: 49\n",
      "  Response: Temperature controls randomness in an LLM's token sampling: low = deterministic, high = more diverse and unpredictable outputs.\n",
      "\n",
      "âœ… Model comparison complete!\n",
      "\n",
      "ğŸ” In MLflow UI Traces tab:\n",
      "   - Filter by Execution time (ms)\n",
      "   - Compare latencies\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-47456ea6c54d0d92dd2fbbc92845b5c1&amp;experiment_id=6&amp;trace_id=tr-c91678e54408c7cc6d2b939da10e63fb&amp;experiment_id=6&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-47456ea6c54d0d92dd2fbbc92845b5c1), Trace(trace_id=tr-c91678e54408c7cc6d2b939da10e63fb)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare different models\n",
    "prompt = \"Explain LLM temperature in 20 words or less.\"\n",
    "models = [\"jsd-gpt-5-2\", \"jsd-gpt-5-mini\"] if use_databricks_provider else [\"gpt-5-2\", \"gpt-5-mini\"]\n",
    "\n",
    "\n",
    "print(\"\\nğŸ”¬ Comparing models with tracing...\\n\")\n",
    "\n",
    "for model in models:\n",
    "    print(f\"Testing {model}...\")\n",
    "    \n",
    "    start = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=1.0,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    latency = time.time() - start\n",
    "    \n",
    "    print(f\"  Latency: {latency:.2f}s\")\n",
    "    print(f\"  Tokens: {response.usage.total_tokens}\")\n",
    "    print(f\"  Response: {response.choices[0].message.content}\\n\")\n",
    "\n",
    "print(\"âœ… Model comparison complete!\")\n",
    "print(\"\\nğŸ” In MLflow UI Traces tab:\")\n",
    "print(\"   - Filter by Execution time (ms)\")\n",
    "print(\"   - Compare latencies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Understanding the MLflow Traces UI\n",
    "\n",
    "Let's explore what the Traces tab shows you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘           MLflow Traces UI Guide                             â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "1. ğŸ” TRACES TAB LOCATION:\n",
      "   Navigate to: http://localhost:5000\n",
      "   Click on: \"Traces\" tab (Under Observability)\n",
      "\n",
      "2. ğŸ“‹ TRACE LIST VIEW:\n",
      "   - Timestamp: When the trace was created\n",
      "   - Request ID: Unique identifier\n",
      "   - Latency: Total execution time\n",
      "   - Status: Success/Error\n",
      "   - Tags: Custom metadata\n",
      "\n",
      "3. ğŸ“Š TRACE DETAILS VIEW (click on a trace):\n",
      "\n",
      "   Timeline Tab:\n",
      "   - Waterfall visualization of spans\n",
      "   - Duration of each operation\n",
      "   - Parent-child relationships\n",
      "\n",
      "   Spans Tab:\n",
      "   - List of all spans in the trace\n",
      "   - Inputs and outputs for each span\n",
      "   - Metadata (model, tokens, etc.)\n",
      "\n",
      "   Attributes Tab:\n",
      "   - Custom attributes you've logged\n",
      "   - System attributes (timestamps, status)\n",
      "\n",
      "4. ğŸ” SPAN DETAILS (click on a span):\n",
      "   - Input: What was sent to the LLM\n",
      "   - Output: What the LLM returned\n",
      "   - Attributes: Model config, tokens, etc.\n",
      "   - Timing: Start time, end time, duration\n",
      "\n",
      "5. ğŸ’¡ USEFUL FEATURES:\n",
      "   - Search traces by content\n",
      "   - Filter by date range\n",
      "   - Filter by status (success/error)\n",
      "   - Compare multiple traces\n",
      "   - Export trace data\n",
      "\n",
      "\n",
      "âœ… Trace created! Go to the UI and explore it.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-2996e88a5115dac8270553a2ffed862b&amp;experiment_id=6&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-2996e88a5115dac8270553a2ffed862b)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a trace with interesting attributes\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘           MLflow Traces UI Guide                             â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "1. ğŸ” TRACES TAB LOCATION:\n",
    "   Navigate to: http://localhost:5000\n",
    "   Click on: \"Traces\" tab (Under Observability)\n",
    "\n",
    "2. ğŸ“‹ TRACE LIST VIEW:\n",
    "   - Timestamp: When the trace was created\n",
    "   - Request ID: Unique identifier\n",
    "   - Latency: Total execution time\n",
    "   - Status: Success/Error\n",
    "   - Tags: Custom metadata\n",
    "\n",
    "3. ğŸ“Š TRACE DETAILS VIEW (click on a trace):\n",
    "   \n",
    "   Timeline Tab:\n",
    "   - Waterfall visualization of spans\n",
    "   - Duration of each operation\n",
    "   - Parent-child relationships\n",
    "   \n",
    "   Spans Tab:\n",
    "   - List of all spans in the trace\n",
    "   - Inputs and outputs for each span\n",
    "   - Metadata (model, tokens, etc.)\n",
    "   \n",
    "   Attributes Tab:\n",
    "   - Custom attributes you've logged\n",
    "   - System attributes (timestamps, status)\n",
    "\n",
    "4. ğŸ” SPAN DETAILS (click on a span):\n",
    "   - Input: What was sent to the LLM\n",
    "   - Output: What the LLM returned\n",
    "   - Attributes: Model config, tokens, etc.\n",
    "   - Timing: Start time, end time, duration\n",
    "\n",
    "5. ğŸ’¡ USEFUL FEATURES:\n",
    "   - Search traces by content\n",
    "   - Filter by date range\n",
    "   - Filter by status (success/error)\n",
    "   - Compare multiple traces\n",
    "   - Export trace data\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# Make a traced call for demonstration\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Explain the MLflow Traces UI in one sentence.\"\n",
    "    }],\n",
    "    temperature=1.0,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "print(\"âœ… Trace created! Go to the UI and explore it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: Tracing with LangChain (Framework Integration)\n",
    "\n",
    "MLflow supports automatic tracing for many frameworks. Let's try LangChain!\n",
    "\n",
    "LangChain is a Python/JS framework for building LLM-powered applications by providing reusable building blocks for prompts, models, tools/function-calling, retrieval (RAG), memory/state, and multi-step chains/agents.\n",
    "\n",
    "Why/when to use it\n",
    "Use LangChain when you want to:\n",
    "\n",
    "**Standardize an LLM app**: consistent interfaces for prompt templates, chat models (e.g., ChatOpenAI), outputs/parsers, retries, and tracing.\n",
    "\n",
    "**Build RAG**: connect documents â†’ embeddings â†’ vector store â†’ retriever â†’ prompt, with common patterns already implemented.\n",
    "\n",
    "**Use tools/agents**: let the model call functions/tools and orchestrate multi-step workflows.\n",
    "\n",
    "**Compose pipelines**: easily chain steps (prompt â†’ model â†’ parser) and swap components without rewriting glue code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install LangChain if needed\n",
    "!pip install langchain langchain-openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LangChain autologging enabled\n",
      "âœ… Using Databricks AI Gateway as provider\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "databricks_provider = is_databricks_ai_gateway_client()\n",
    "# Enable LangChain autologging\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "print(\"âœ… LangChain autologging enabled\")\n",
    "print(\"âœ… Using\", \"Databricks AI Gateway\" if databricks_provider else \"OpenAI\", \"as provider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”— Creating and running LangChain chain with tracing...\n",
      "\n",
      "Response: Tracing in GenAI applications (often via distributed tracing/telemetry) means recording structured â€œspansâ€ for each step of a requestâ€”prompt construction, retrieval, model calls, tool/function calls, parsing, and post-processingâ€”so you can see exactly what happened and why. Key benefits:\n",
      "\n",
      "- **End-to-end observability**: See the full path of a user request across services (API, RAG pipeline, LLM, tools) with timing and metadata.\n",
      "- **Faster debugging & RCA**: Pinpoint where failures occur (bad prompt, retrieval miss, tool error, parsing bug) and reproduce issues with the exact inputs/outputs.\n",
      "- **Performance & latency optimization**: Identify bottlenecks (slow vector search, long model latency, tool delays), measure tokenization/streaming effects, and reduce tail latency.\n",
      "- **Quality improvement**: Correlate output quality with prompt versions, retrieved documents, tool choices, and model parameters; detect where hallucinations or omissions originate.\n",
      "- **Cost control**: Track token usage, number of model/tool calls, retries, and context size per request; attribute costs to features/tenants and optimize spend.\n",
      "- **Reliability & monitoring**: Build alerts on error rates, timeouts, degradation, and unusual patterns (spikes in retries, retrieval failures).\n",
      "- **Experimentation & evaluation**: Compare model versions, prompts, or retrieval strategies using consistent trace data; support A/B tests and offline evals with rich context.\n",
      "- **Auditability & governance**: Provide an audit trail of what data was used (e.g., retrieved docs), which model/version ran, and what actions/tools were invoked.\n",
      "- **Security & compliance support**: Detect sensitive-data leakage paths, enforce redaction, and monitor policy violations (with appropriate PII handling).\n",
      "- **Better collaboration & maintenance**: Shared traces make it easier for engineers, data scientists, and product teams to understand behavior and iterate safely.\n",
      "\n",
      "In practice, tracing is especially valuable for RAG and agentic workflows where multiple steps and external tools make failures and quality issues hard to diagnose without a detailed execution record.\n",
      "\n",
      "âœ… LangChain execution traced!\n",
      "\n",
      "ğŸ” In the trace, you'll see:\n",
      "   - Prompt template construction\n",
      "   - Variable substitution\n",
      "   - LLM invocation\n",
      "   - All as separate spans in a hierarchy!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-ff4f224c7ebd24c77024da091f96b220&amp;experiment_id=6&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-ff4f224c7ebd24c77024da091f96b220)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.clnt_utils import get_langchain_chat_openai_client, get_databricks_ai_gateway_langchain_client\n",
    "\n",
    "# Create a simple LangChain chain\n",
    "print(\"\\nğŸ”— Creating and running LangChain chain with tracing...\\n\")\n",
    "\n",
    "# Define prompt template\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"You are a {role}. Answer the following question: {question}\"\n",
    ")\n",
    "\n",
    "# Create LLM\n",
    "if databricks_provider:\n",
    "    llm = get_databricks_ai_gateway_langchain_client(model_name, temperature=1.0)\n",
    "else:\n",
    "    llm = get_langchain_chat_openai_client(mode=model_name, temperature=1.0)\n",
    "\n",
    "# Create chain\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# Run chain\n",
    "response = chain.invoke({\n",
    "    \"role\": \"helpful AI assistant\",\n",
    "    \"question\": \"What are the benefits of tracing in GenAI applications?\"\n",
    "})\n",
    "\n",
    "print(f\"Response: {response.content}\")\n",
    "print(\"\\nâœ… LangChain execution traced!\")\n",
    "print(\"\\nğŸ” In the trace, you'll see:\")\n",
    "print(\"   - Prompt template construction\")\n",
    "print(\"   - Variable substitution\")\n",
    "print(\"   - LLM invocation\")\n",
    "print(\"   - All as separate spans in a hierarchy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ LangChain Tracing Benefits\n",
    "\n",
    "With LangChain autologging, MLflow automatically traces:\n",
    "\n",
    "- **Chain execution**: See how data flows through the chain\n",
    "- **Prompt construction**: View how templates are filled\n",
    "- **LLM calls**: Standard OpenAI tracing\n",
    "- **Retrieval steps**: If using RAG components\n",
    "- **Tool usage**: If using agents with tools\n",
    "\n",
    "**All without manual instrumentation!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11: Error Tracing and Debugging\n",
    "\n",
    "Tracing is especially valuable when things go wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ› Demonstrating error tracing...\n",
      "\n",
      "âŒ Error occurred: NotFoundError\n",
      "   Message: Error code: 404 - {'error_code': 'NOT_FOUND', 'message': \"AI Gateway endpoint 'gpt-does-not-exist' d...\n",
      "\n",
      "âœ… Error was captured in trace!\n",
      "\n",
      "ğŸ” In the MLflow UI:\n",
      "   - Trace marked with error status\n",
      "   - Error message visible\n",
      "   - Stack trace preserved\n",
      "   - Easy to identify failure point\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-5774aacae296e2d051497c1613a13605&amp;experiment_id=6&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-5774aacae296e2d051497c1613a13605)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Intentionally cause an error to see how it's traced\n",
    "print(\"\\nğŸ› Demonstrating error tracing...\\n\")\n",
    "\n",
    "try:\n",
    "    # This will fail - invalid model name\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-does-not-exist\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error occurred: {type(e).__name__}\")\n",
    "    print(f\"   Message: {str(e)[:100]}...\")\n",
    "    print(\"\\nâœ… Error was captured in trace!\")\n",
    "    print(\"\\nğŸ” In the MLflow UI:\")\n",
    "    print(\"   - Trace marked with error status\")\n",
    "    print(\"   - Error message visible\")\n",
    "    print(\"   - Stack trace preserved\")\n",
    "    print(\"   - Easy to identify failure point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ› Debugging with Traces\n",
    "\n",
    "When your LLM application fails:\n",
    "\n",
    "1. **Find the trace** in the UI (filter by error status)\n",
    "2. **Identify the failing span** (marked red)\n",
    "3. **Inspect inputs** that caused the error\n",
    "4. **View error details** (message, type, stack trace)\n",
    "5. **Fix and re-run** with full history preserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 12: Disabling Autologging\n",
    "\n",
    "Sometimes you want to turn tracing off temporarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Autologging disabled\n",
      "\n",
      "âœ… Call completed without tracing\n",
      "\n",
      "âœ… Autologging re-enabled\n"
     ]
    }
   ],
   "source": [
    "# Disable autologging\n",
    "mlflow.openai.autolog(disable=True)\n",
    "mlflow.langchain.autolog(disable=True)\n",
    "\n",
    "print(\"âŒ Autologging disabled\")\n",
    "\n",
    "# This call won't be traced\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"This won't be traced.\"}]\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Call completed without tracing\")\n",
    "\n",
    "# Re-enable for future calls\n",
    "mlflow.openai.autolog()\n",
    "print(\"\\nâœ… Autologging re-enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 13: Best Practices for Tracing\n",
    "\n",
    "Let's review key best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘           Tracing Best Practices                             â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "âœ… DO:\n",
      "\n",
      "1. Enable autologging early in development\n",
      "   - Easier to debug from the start\n",
      "   - Understand performance characteristics early\n",
      "\n",
      "2. Use tracing in all environments\n",
      "   - Development: Full tracing for debugging\n",
      "   - Staging: Trace all requests\n",
      "   - Production: Sample or trace all (depending on volume)\n",
      "\n",
      "3. Combine tracing with experiment tracking\n",
      "   - Traces give you the \"how\"\n",
      "   - Experiments give you the \"what\" and \"why\"\n",
      "\n",
      "4. Review traces regularly\n",
      "   - Look for performance bottlenecks\n",
      "   - Identify expensive operations\n",
      "   - Understand failure patterns\n",
      "\n",
      "5. Use multiple frameworks\n",
      "   - MLflow supports 30+ integrations\n",
      "   - Pick the best tool for each task\n",
      "\n",
      "âŒ DON'T:\n",
      "\n",
      "1. Ignore traces until there's a problem\n",
      "   - Proactive monitoring prevents issues\n",
      "\n",
      "2. Log sensitive data in traces\n",
      "   - PII, credentials, confidential info\n",
      "   - Use data masking if needed\n",
      "\n",
      "3. Assume autologging captures everything\n",
      "   - Custom operations need manual instrumentation\n",
      "   - We'll cover this in the next notebook!\n",
      "\n",
      "4. Over-rely on individual traces\n",
      "   - Look at aggregate patterns\n",
      "   - Use traces + metrics together\n",
      "\n",
      "ğŸ’¡ PRO TIPS:\n",
      "\n",
      "- Set up trace sampling in high-volume production\n",
      "- Archive old traces to manage storage\n",
      "- Export important traces for documentation\n",
      "- Share trace links with your team for debugging\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘           Tracing Best Practices                             â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "âœ… DO:\n",
    "\n",
    "1. Enable autologging early in development\n",
    "   - Easier to debug from the start\n",
    "   - Understand performance characteristics early\n",
    "\n",
    "2. Use tracing in all environments\n",
    "   - Development: Full tracing for debugging\n",
    "   - Staging: Trace all requests\n",
    "   - Production: Sample or trace all (depending on volume)\n",
    "\n",
    "3. Combine tracing with experiment tracking\n",
    "   - Traces give you the \"how\"\n",
    "   - Experiments give you the \"what\" and \"why\"\n",
    "\n",
    "4. Review traces regularly\n",
    "   - Look for performance bottlenecks\n",
    "   - Identify expensive operations\n",
    "   - Understand failure patterns\n",
    "\n",
    "5. Use multiple frameworks\n",
    "   - MLflow supports 30+ integrations\n",
    "   - Pick the best tool for each task\n",
    "\n",
    "âŒ DON'T:\n",
    "\n",
    "1. Ignore traces until there's a problem\n",
    "   - Proactive monitoring prevents issues\n",
    "\n",
    "2. Log sensitive data in traces\n",
    "   - PII, credentials, confidential info\n",
    "   - Use data masking if needed\n",
    "\n",
    "3. Assume autologging captures everything\n",
    "   - Custom operations need manual instrumentation\n",
    "   - We'll cover this in the next notebook!\n",
    "\n",
    "4. Over-rely on individual traces\n",
    "   - Look at aggregate patterns\n",
    "   - Use traces + metrics together\n",
    "\n",
    "ğŸ’¡ PRO TIPS:\n",
    "\n",
    "- Set up trace sampling in high-volume production\n",
    "- Archive old traces to manage storage\n",
    "- Export important traces for documentation\n",
    "- Share trace links with your team for debugging\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. âœ… What tracing is and why it's critical for GenAI\n",
    "2. âœ… The trace data model (traces, spans, hierarchy)\n",
    "3. âœ… Automatic tracing with `mlflow.openai.autolog()`\n",
    "4. âœ… How to view and analyze traces in the MLflow UI\n",
    "5. âœ… Tracing multi-step workflows\n",
    "6. âœ… Framework integration (LangChain example)\n",
    "7. âœ… Error tracing and debugging\n",
    "8. âœ… Best practices for production use\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **One line of code** enables complete observability\n",
    "- **Automatic capture** of inputs, outputs, and metadata\n",
    "- **Framework agnostic** - works with OpenAI, LangChain, LlamaIndex, etc.\n",
    "- **Essential for debugging** complex LLM workflows\n",
    "- **Production-ready** with sampling and filtering\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**ğŸ““ Notebook 1.4: Manual Tracing and Advanced Observability**\n",
    "\n",
    "Learn how to:\n",
    "- Create custom spans with `@mlflow.trace`\n",
    "- Add custom attributes to spans\n",
    "- Build hierarchical traces for complex workflows\n",
    "- Trace RAG pipelines with retrieval and generation steps\n",
    "- Trace agentic workflows with tool usage\n",
    "- Debug production issues with advanced techniques\n",
    "\n",
    "### ğŸ¯ Practice Exercise\n",
    "\n",
    "Before moving on, try this:\n",
    "\n",
    "1. Enable tracing\n",
    "2. Build a simple chatbot that:\n",
    "   - Takes user input\n",
    "   - Generates a response\n",
    "   - Evaluates the response quality\n",
    "3. View all three operations in a single trace\n",
    "4. Compare performance across multiple runs\n",
    "\n",
    "Ready? Let's continue to manual tracing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
