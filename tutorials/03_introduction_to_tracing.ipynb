{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1.3: Introduction to Tracing\n",
    "\n",
    "## Observability for GenAI Applications\n",
    "\n",
    "Welcome to the tracing section! This is where MLflow really shines for GenAI development. You'll learn how to gain complete visibility into your LLM applications.\n",
    "\n",
    "### What You'll Learn\n",
    "- What is tracing and why it matters for GenAI\n",
    "- Understanding the trace data model (traces, spans, hierarchy)\n",
    "- Automatic tracing with autologging\n",
    "- Tracing multiple frameworks (OpenAI, LangChain)\n",
    "- Viewing and analyzing traces in MLflow UI\n",
    "- Debugging with traces\n",
    "\n",
    "### Prerequisites\n",
    "- Completed Notebooks 1.1 and 1.2\n",
    "- MLflow UI running\n",
    "- OpenAI API key configured\n",
    "- Or keys for your foundational model provider, e.g., Databricks\n",
    "\n",
    "### Estimated Time: 30-35 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: What is Tracing?\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Traditional experimenting tracking or logging isn't enough for LLM applications:\n",
    "\n",
    "```python\n",
    "# Traditional logging - hard to debug\n",
    "print(\"Calling LLM...\")\n",
    "response = llm.generate(prompt)\n",
    "print(f\"Response: {response}\")\n",
    "# What happened inside? How long did it take? What was sent or received?\n",
    "```\n",
    "\n",
    "### The Solution: Distributed Tracing\n",
    "\n",
    "Tracing captures the **complete execution flow** of your application:\n",
    "\n",
    "```\n",
    "Trace: RAG Application\n",
    "â”œâ”€â”€ Span 1: Embed Query [0ms - 200ms]\n",
    "â”‚   Input: \"What is MLflow?\"\n",
    "â”‚   Output: [0.123, 0.456, ...]\n",
    "â”‚   \n",
    "â”œâ”€â”€ Span 2: Retrieve Documents [200ms - 350ms]\n",
    "â”‚   Input: [0.123, 0.456, ...]\n",
    "â”‚   Output: [\"MLflow is...\", \"The platform...\"]\n",
    "â”‚   \n",
    "â””â”€â”€ Span 3: Generate Response [350ms - 1500ms]\n",
    "    Input: {query, documents}\n",
    "    Output: \"MLflow is an open source platform...\"\n",
    "    LLM: gpt-5-mini\n",
    "    Tokens: 150\n",
    "```\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "1. **Visibility**: See every step in your LLM workflow\n",
    "2. **Performance**: Identify bottlenecks and latency issues\n",
    "3. **Debugging**: Trace errors to their exact source\n",
    "4. **Cost**: Track token usage per operation\n",
    "5. **Quality**: Inspect inputs/outputs at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Understanding the Trace Data Model\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "**Trace**: A complete execution of an operation\n",
    "- Represents one request or workflow\n",
    "- Contains one or more spans\n",
    "- Has a root span\n",
    "\n",
    "**Span**: A single operation within a trace\n",
    "- Has a start and end time\n",
    "- Contains inputs and outputs\n",
    "- Has metadata (model, tokens, etc.)\n",
    "- Can have parent-child relationships\n",
    "\n",
    "**Span Attributes**: Additional metadata\n",
    "- Model name\n",
    "- Token counts\n",
    "- Temperature\n",
    "- Custom attributes\n",
    "\n",
    "### Span Types\n",
    "\n",
    "MLflow defines standard span types:\n",
    "\n",
    "- `CHAIN`: A sequence of operations\n",
    "- `LLM`: Language model call\n",
    "- `RETRIEVER`: Document retrieval\n",
    "- `EMBEDDING`: Text embedding\n",
    "- `TOOL`: Tool/function execution\n",
    "- `AGENT`: Agent reasoning\n",
    "- `PARSER`: Output parsing\n",
    "\n",
    "### Hierarchy Example\n",
    "\n",
    "```\n",
    "TRACE (root)\n",
    "â”‚\n",
    "â””â”€ SPAN: Agent Executor (AGENT)\n",
    "   â”‚\n",
    "   â”œâ”€ SPAN: Planning Step (LLM)\n",
    "   â”‚  â””â”€ attributes: {model: gpt-5, tokens: 50}\n",
    "   â”‚\n",
    "   â”œâ”€ SPAN: Tool Execution (TOOL)\n",
    "   â”‚  â””â”€ attributes: {tool: search, query: \"...\"}\n",
    "   â”‚\n",
    "   â””â”€ SPAN: Final Response (LLM)\n",
    "      â””â”€ attributes: {model: gpt-5, tokens: 150}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment configured: using Databricks client\n",
      "   MLflow version: 3.9.0\n",
      "   Tracking URI: http://localhost:5000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "from utils import is_databricks_client, get_databricks_client, get_openai_client\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "use_databricks_provider = is_databricks_client()\n",
    "if use_databricks_provider:\n",
    "    client = get_databricks_client()\n",
    "else:\n",
    "    # Initialize OpenAI\n",
    "    client = get_openai_client()\n",
    "\n",
    "print(\"âœ… Environment configured: using\", \"Databricks\" if use_databricks_provider else \"OpenAI\", \"client\")\n",
    "print(f\"   MLflow version: {mlflow.__version__}\")\n",
    "print(f\"   Tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Your First Trace - Manual Example\n",
    "\n",
    "Before we use autologging, let's understand what tracing captures by manually logging the information. This way we understand the difference and benefits of using autlogging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Making LLM call WITHOUT tracing...\n",
      "\n",
      "Using Model: databricks-gpt-5-mini\n",
      "Response: Distributed tracing is a method for tracking and visualizing the flow of a single request as it passes through multiple services or components in a distributed system by recording timed, correlated events (spans) across those services.\n",
      "\n",
      "âŒ What we DON'T know:\n",
      "   - Exact timing of the call\n",
      "   - Detailed request/response structure\n",
      "   - Easy way to correlate with other operations\n",
      "   - Visual representation of execution\n"
     ]
    }
   ],
   "source": [
    "# Create experiment for tracing examples\n",
    "mlflow.set_experiment(\"06-tracing-introduction\")\n",
    "\n",
    "# Without tracing - basic call\n",
    "print(\"\\nğŸ“ Making LLM call WITHOUT tracing...\\n\")\n",
    "\n",
    "prompt = \"Explain what distributed tracing is in one sentence.\"\n",
    "model_name = \"databricks-gpt-5-mini\" if use_databricks_provider else \"gpt-5-mini\"\n",
    "print(f\"Using Model: {model_name}\")\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=1.0,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(f\"Response: {response.choices[0].message.content}\")\n",
    "print(\"\\nâŒ What we DON'T know:\")\n",
    "print(\"   - Exact timing of the call\")\n",
    "print(\"   - Detailed request/response structure\")\n",
    "print(\"   - Easy way to correlate with other operations\")\n",
    "print(\"   - Visual representation of execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Automatic Tracing with OpenAI Autologging\n",
    "\n",
    "Now let's enable tracing with a single line of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI autologging enabled\n",
      "   All OpenAI API calls will now be automatically traced!\n"
     ]
    }
   ],
   "source": [
    "# Enable OpenAI autologging - THIS IS THE MAGIC LINE!\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "print(\"âœ… OpenAI autologging enabled\")\n",
    "print(\"   All OpenAI API calls will now be automatically traced!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Making LLM call WITH tracing...\n",
      "\n",
      "Response: Distributed tracing is a method for tracking and visualizing the flow of a single request as it travels across multiple services and components in a distributed system, by recording time-stamped, correlated events (spans) that capture latency and causal relationships.\n",
      "\n",
      "âœ… What we NOW know:\n",
      "   âœ“ Complete request details (model, messages, parameters)\n",
      "   âœ“ Response content and metadata\n",
      "   âœ“ Token usage (prompt, completion, total)\n",
      "   âœ“ Timing information (latency)\n",
      "   âœ“ All captured automatically!\n",
      "\n",
      "ğŸ”— View trace in MLflow UI: http://localhost:5000\n",
      "   Navigate to: Traces tab\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-979cefb57f343d258b596264baeb4d64&amp;experiment_id=6&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-979cefb57f343d258b596264baeb4d64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make the same call - now it's automatically traced!\n",
    "print(\"\\nğŸ” Making LLM call WITH tracing...\\n\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=1.0,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(f\"Response: {response.choices[0].message.content}\")\n",
    "print(\"\\nâœ… What we NOW know:\")\n",
    "print(\"   âœ“ Complete request details (model, messages, parameters)\")\n",
    "print(\"   âœ“ Response content and metadata\")\n",
    "print(\"   âœ“ Token usage (prompt, completion, total)\")\n",
    "print(\"   âœ“ Timing information (latency)\")\n",
    "print(\"   âœ“ All captured automatically!\")\n",
    "print(\"\\nğŸ”— View trace in MLflow UI: http://localhost:5000\")\n",
    "print(\"   Navigate to: Traces tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ‰ What Just Happened?\n",
    "\n",
    "With `mlflow.openai.autolog()`, MLflow automatically:\n",
    "\n",
    "1. **Intercepted** the OpenAI API call\n",
    "2. **Created** a trace with a unique ID\n",
    "3. **Captured** all inputs (messages, model, parameters)\n",
    "4. **Captured** all outputs (response, tokens, timing)\n",
    "5. **Stored** everything in a structured format\n",
    "6. **Made it available** in the MLflow UI\n",
    "\n",
    "**Zero code changes required!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Exploring Trace Details\n",
    "\n",
    "Let's make a more complex call and examine what gets traced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¬ Multi-turn conversation with tracing...\n",
      "\n",
      "Response: ...\n",
      "\n",
      "ğŸ“Š Trace captured:\n",
      "   Model: gpt-5-mini-2025-08-07\n",
      "   Prompt tokens: 66\n",
      "   Completion tokens: 200\n",
      "   Total tokens: 266\n",
      "\n",
      "ğŸ” In the trace, you can see:\n",
      "   - All 4 messages (system + conversation history)\n",
      "   - Model configuration (temperature, max_tokens)\n",
      "   - Complete response\n",
      "   - Token breakdown\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-8d75fdee824d2e55e668d7c53e2c7b35&amp;experiment_id=6&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-8d75fdee824d2e55e668d7c53e2c7b35)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Multi-turn conversation with system prompt\n",
    "print(\"\\nğŸ’¬ Multi-turn conversation with tracing...\\n\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful AI expert who explains concepts clearly and concisely.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is MLflow?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"MLflow is an open-source platform for managing the machine learning lifecycle.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What are its main components and what are they used for?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    temperature=1.0,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(f\"Response: {response.choices[0].message.content[:200]}...\")\n",
    "print(\"\\nğŸ“Š Trace captured:\")\n",
    "print(f\"   Model: {response.model}\")\n",
    "print(f\"   Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"   Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"   Total tokens: {response.usage.total_tokens}\")\n",
    "print(\"\\nğŸ” In the trace, you can see:\")\n",
    "print(\"   - All 4 messages (system + conversation history)\")\n",
    "print(\"   - Model configuration (temperature, max_tokens)\")\n",
    "print(\"   - Complete response\")\n",
    "print(\"   - Token breakdown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Tracing Multiple Sequential Calls\n",
    "\n",
    "Let's see how tracing helps with multi-step workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Multi-step workflow with automatic tracing...\n",
      "\n",
      "Step 1: Generating topic...\n",
      "  Topic: **Topic:** *How â€œAgenticâ€ AI Works: Turning LLMs into Goal-Driven Assistants (Without Letting Them Run Wild)*\n",
      "\n",
      "A blog post on this can cover:\n",
      "- What AI agents are (vs. standard chatbots)\n",
      "- Core building blocks: planning, tool use (APIs), memory, and feedback loops\n",
      "- Real examples: scheduling, research assistants, customer support triage, code maintenance\n",
      "- Key risks: hallucinations, runaway actions, security/privacy, cost blow-ups\n",
      "- Practical safeguards: permissions, sandboxing, human-in-the-loop, rate limits, audit logs, evals\n",
      "\n",
      "Step 2: Creating outline...\n",
      "  Outline: 1) What â€œagenticâ€ AI is and why it matters\n",
      "- Define agentic AI vs. standard chatbots: persistent goa...\n",
      "\n",
      "Step 3: Writing introduction...\n",
      "  Introduction: Agentic AI refers to systems built on large language models that donâ€™t just respond to prompts but autonomously pursue goals by planning, using tools, remembering context, and iterating on outcomes. Unlike standard chatbots that provide reactive answers, these goal-driven assistants can call APIs, schedule tasks, fetch documents, and take multi-step actions to complete complex workflows. Real-world uses range from scheduling and research assistants to customer-support triage and automated code maintenance, but with those capabilities come serious risks like hallucinations, runaway actions, security and privacy breaches, and unexpected cost spikes. This post explains the core components that make agentic behavior possible and outlines practical safeguardsâ€”permissions, sandboxing, human-in-the-loop controls, rate limits, and audit logsâ€”to keep powerful assistants useful and safe.\n",
      "\n",
      "âœ… All three steps completed!\n",
      "\n",
      "ğŸ“Š Total tokens used:\n",
      "   967 tokens\n",
      "\n",
      "ğŸ” View in MLflow UI:\n",
      "   You'll see THREE separate traces, one for each call\n",
      "   Each trace shows timing, tokens, and complete I/O\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-e4209f87bf336a3fb2df386cce1a502f&amp;experiment_id=6&amp;trace_id=tr-f4f82c3f62408e3856158363d9086702&amp;experiment_id=6&amp;trace_id=tr-d4ba8791d7c76c81e839fa3978236a05&amp;experiment_id=6&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-e4209f87bf336a3fb2df386cce1a502f), Trace(trace_id=tr-f4f82c3f62408e3856158363d9086702), Trace(trace_id=tr-d4ba8791d7c76c81e839fa3978236a05)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple multi-step workflow\n",
    "print(\"\\nğŸ”„ Multi-step workflow with automatic tracing...\\n\")\n",
    "\n",
    "# Step 1: Generate a topic\n",
    "print(\"Step 1: Generating topic...\")\n",
    "topic_response = client.chat.completions.create(\n",
    "    model=\"databricks-gpt-5-2\" if use_databricks_provider else \"gpt-5-2i\",\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful AI expert who explains concepts clearly and concisely.\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Suggest one interesting AI topic for a blog post.\"\n",
    "    }],\n",
    "    temperature=1.0,\n",
    "    max_tokens=200\n",
    ")\n",
    "topic = topic_response.choices[0].message.content\n",
    "print(f\"  Topic: {topic}\")\n",
    "\n",
    "# Step 2: Generate an outline\n",
    "print(\"\\nStep 2: Creating outline...\")\n",
    "outline_response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Create a 3-point outline for a blog post about: {topic}\"\n",
    "    }],\n",
    "    temperature=1.0,\n",
    "    max_tokens=500\n",
    ")\n",
    "outline = outline_response.choices[0].message.content\n",
    "print(f\"  Outline: {outline[:100]}...\")\n",
    "\n",
    "# Step 3: Write the introduction\n",
    "print(\"\\nStep 3: Writing introduction...\")\n",
    "intro_response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Write a 4-sentence introduction paragraph for: {topic}\"\n",
    "    }],\n",
    "    temperature=1.0,\n",
    "    max_tokens=2000\n",
    ")\n",
    "intro = intro_response.choices[0].message.content\n",
    "print(f\"  Introduction: {intro}\")\n",
    "\n",
    "print(\"\\nâœ… All three steps completed!\")\n",
    "print(\"\\nğŸ“Š Total tokens used:\")\n",
    "total_tokens = (topic_response.usage.total_tokens +\n",
    "                outline_response.usage.total_tokens +\n",
    "                intro_response.usage.total_tokens)\n",
    "print(f\"   {total_tokens} tokens\")\n",
    "\n",
    "print(\"\\nğŸ” View in MLflow UI:\")\n",
    "print(\"   You'll see THREE separate traces, one for each call\")\n",
    "print(\"   Each trace shows timing, tokens, and complete I/O\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Observation\n",
    "\n",
    "Each OpenAI call creates a **separate trace**. In the next notebook, we'll learn how to group related calls into a **single hierarchical trace** using manual instrumentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Tracing with Different Models\n",
    "\n",
    "Let's compare traces across different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¬ Comparing models with tracing...\n",
      "\n",
      "Testing databricks-gpt-5-2...\n",
      "  Latency: 1.59s\n",
      "  Tokens: 42\n",
      "  Response: LLM temperature controls randomness: lower is more deterministic, higher is more varied and creative, increasing unexpected outputs.\n",
      "\n",
      "Testing databricks-gpt-5-mini...\n",
      "  Latency: 2.11s\n",
      "  Tokens: 55\n",
      "  Response: Temperature controls randomness in an LLMâ€™s output: low = more deterministic/likely tokens, high = more random/creative/varied choices.\n",
      "\n",
      "âœ… Model comparison complete!\n",
      "\n",
      "ğŸ” In MLflow UI Traces tab:\n",
      "   - Filter by Execution time (ms)\n",
      "   - Compare latencies\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-11503bf6d9bc2fee005b912809ae55cb&amp;experiment_id=6&amp;trace_id=tr-35e92c400670bf4a06ffccb61f8e6eac&amp;experiment_id=6&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-11503bf6d9bc2fee005b912809ae55cb), Trace(trace_id=tr-35e92c400670bf4a06ffccb61f8e6eac)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare different models\n",
    "prompt = \"Explain LLM temperature in 20 words or less.\"\n",
    "models = [\"databricks-gpt-5-2\", \"databricks-gpt-5-mini\"] if use_databricks_provider else [\"gpt-5-mini\", \"gpt-5-2\"]\n",
    "\n",
    "\n",
    "print(\"\\nğŸ”¬ Comparing models with tracing...\\n\")\n",
    "\n",
    "for model in models:\n",
    "    print(f\"Testing {model}...\")\n",
    "    \n",
    "    start = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=1.0,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    latency = time.time() - start\n",
    "    \n",
    "    print(f\"  Latency: {latency:.2f}s\")\n",
    "    print(f\"  Tokens: {response.usage.total_tokens}\")\n",
    "    print(f\"  Response: {response.choices[0].message.content}\\n\")\n",
    "\n",
    "print(\"âœ… Model comparison complete!\")\n",
    "print(\"\\nğŸ” In MLflow UI Traces tab:\")\n",
    "print(\"   - Filter by Execution time (ms)\")\n",
    "print(\"   - Compare latencies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Understanding the MLflow Traces UI\n",
    "\n",
    "Let's explore what the Traces tab shows you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘           MLflow Traces UI Guide                             â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "1. ğŸ” TRACES TAB LOCATION:\n",
      "   Navigate to: http://localhost:5000\n",
      "   Click on: \"Traces\" tab (Under Observability)\n",
      "\n",
      "2. ğŸ“‹ TRACE LIST VIEW:\n",
      "   - Timestamp: When the trace was created\n",
      "   - Request ID: Unique identifier\n",
      "   - Latency: Total execution time\n",
      "   - Status: Success/Error\n",
      "   - Tags: Custom metadata\n",
      "\n",
      "3. ğŸ“Š TRACE DETAILS VIEW (click on a trace):\n",
      "\n",
      "   Timeline Tab:\n",
      "   - Waterfall visualization of spans\n",
      "   - Duration of each operation\n",
      "   - Parent-child relationships\n",
      "\n",
      "   Spans Tab:\n",
      "   - List of all spans in the trace\n",
      "   - Inputs and outputs for each span\n",
      "   - Metadata (model, tokens, etc.)\n",
      "\n",
      "   Attributes Tab:\n",
      "   - Custom attributes you've logged\n",
      "   - System attributes (timestamps, status)\n",
      "\n",
      "4. ğŸ” SPAN DETAILS (click on a span):\n",
      "   - Input: What was sent to the LLM\n",
      "   - Output: What the LLM returned\n",
      "   - Attributes: Model config, tokens, etc.\n",
      "   - Timing: Start time, end time, duration\n",
      "\n",
      "5. ğŸ’¡ USEFUL FEATURES:\n",
      "   - Search traces by content\n",
      "   - Filter by date range\n",
      "   - Filter by status (success/error)\n",
      "   - Compare multiple traces\n",
      "   - Export trace data\n",
      "\n",
      "\n",
      "âœ… Trace created! Go to the UI and explore it.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-e4d4f3bd52439457ab108cd1f4c704b5&amp;experiment_id=6&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-e4d4f3bd52439457ab108cd1f4c704b5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a trace with interesting attributes\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘           MLflow Traces UI Guide                             â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "1. ğŸ” TRACES TAB LOCATION:\n",
    "   Navigate to: http://localhost:5000\n",
    "   Click on: \"Traces\" tab (Under Observability)\n",
    "\n",
    "2. ğŸ“‹ TRACE LIST VIEW:\n",
    "   - Timestamp: When the trace was created\n",
    "   - Request ID: Unique identifier\n",
    "   - Latency: Total execution time\n",
    "   - Status: Success/Error\n",
    "   - Tags: Custom metadata\n",
    "\n",
    "3. ğŸ“Š TRACE DETAILS VIEW (click on a trace):\n",
    "   \n",
    "   Timeline Tab:\n",
    "   - Waterfall visualization of spans\n",
    "   - Duration of each operation\n",
    "   - Parent-child relationships\n",
    "   \n",
    "   Spans Tab:\n",
    "   - List of all spans in the trace\n",
    "   - Inputs and outputs for each span\n",
    "   - Metadata (model, tokens, etc.)\n",
    "   \n",
    "   Attributes Tab:\n",
    "   - Custom attributes you've logged\n",
    "   - System attributes (timestamps, status)\n",
    "\n",
    "4. ğŸ” SPAN DETAILS (click on a span):\n",
    "   - Input: What was sent to the LLM\n",
    "   - Output: What the LLM returned\n",
    "   - Attributes: Model config, tokens, etc.\n",
    "   - Timing: Start time, end time, duration\n",
    "\n",
    "5. ğŸ’¡ USEFUL FEATURES:\n",
    "   - Search traces by content\n",
    "   - Filter by date range\n",
    "   - Filter by status (success/error)\n",
    "   - Compare multiple traces\n",
    "   - Export trace data\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# Make a traced call for demonstration\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Explain the MLflow Traces UI in one sentence.\"\n",
    "    }],\n",
    "    temperature=1.0,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "print(\"âœ… Trace created! Go to the UI and explore it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: Tracing with LangChain (Framework Integration)\n",
    "\n",
    "MLflow supports automatic tracing for many frameworks. Let's try LangChain!\n",
    "\n",
    "LangChain is a Python/JS framework for building LLM-powered applications by providing reusable building blocks for prompts, models, tools/function-calling, retrieval (RAG), memory/state, and multi-step chains/agents.\n",
    "\n",
    "Why/when to use it\n",
    "Use LangChain when you want to:\n",
    "\n",
    "**Standardize an LLM app**: consistent interfaces for prompt templates, chat models (e.g., ChatOpenAI), outputs/parsers, retries, and tracing.\n",
    "\n",
    "**Build RAG**: connect documents â†’ embeddings â†’ vector store â†’ retriever â†’ prompt, with common patterns already implemented.\n",
    "\n",
    "**Use tools/agents**: let the model call functions/tools and orchestrate multi-step workflows.\n",
    "\n",
    "**Compose pipelines**: easily chain steps (prompt â†’ model â†’ parser) and swap components without rewriting glue code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install LangChain if needed\n",
    "!pip install langchain langchain-openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LangChain autologging enabled\n",
      "âœ… Using Databricks as provider\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "databricks_provider = is_databricks_client()\n",
    "# Enable LangChain autologging\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "print(\"âœ… LangChain autologging enabled\")\n",
    "print(\"âœ… Using\", \"Databricks\" if databricks_provider else \"OpenAI\", \"as provider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”— Creating and running LangChain chain with tracing...\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_databricks_langchain_chat_client() got an unexpected keyword argument 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Create LLM\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m databricks_provider:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     llm = \u001b[43mget_databricks_langchain_chat_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     16\u001b[39m     llm = get_langchain_chat_openai_client(model_name, temperature=\u001b[32m1.0\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: get_databricks_langchain_chat_client() got an unexpected keyword argument 'model'"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from tutorials.utils import get_langchain_chat_openai_client, get_databricks_langchain_chat_client\n",
    "\n",
    "# Create a simple LangChain chain\n",
    "print(\"\\nğŸ”— Creating and running LangChain chain with tracing...\\n\")\n",
    "\n",
    "# Define prompt template\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"You are a {role}. Answer the following question: {question}\"\n",
    ")\n",
    "\n",
    "# Create LLM\n",
    "if databricks_provider:\n",
    "    llm = get_databricks_langchain_chat_client(model_name, temperature=1.0)\n",
    "else:\n",
    "    llm = get_langchain_chat_openai_client(mode=model_name, temperature=1.0)\n",
    "\n",
    "# Create chain\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# Run chain\n",
    "response = chain.invoke({\n",
    "    \"role\": \"helpful AI assistant\",\n",
    "    \"question\": \"What are the benefits of tracing in GenAI applications?\"\n",
    "})\n",
    "\n",
    "print(f\"Response: {response.content}\")\n",
    "print(\"\\nâœ… LangChain execution traced!\")\n",
    "print(\"\\nğŸ” In the trace, you'll see:\")\n",
    "print(\"   - Prompt template construction\")\n",
    "print(\"   - Variable substitution\")\n",
    "print(\"   - LLM invocation\")\n",
    "print(\"   - All as separate spans in a hierarchy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ LangChain Tracing Benefits\n",
    "\n",
    "With LangChain autologging, MLflow automatically traces:\n",
    "\n",
    "- **Chain execution**: See how data flows through the chain\n",
    "- **Prompt construction**: View how templates are filled\n",
    "- **LLM calls**: Standard OpenAI tracing\n",
    "- **Retrieval steps**: If using RAG components\n",
    "- **Tool usage**: If using agents with tools\n",
    "\n",
    "**All without manual instrumentation!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11: Error Tracing and Debugging\n",
    "\n",
    "Tracing is especially valuable when things go wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intentionally cause an error to see how it's traced\n",
    "print(\"\\nğŸ› Demonstrating error tracing...\\n\")\n",
    "\n",
    "try:\n",
    "    # This will fail - invalid model name\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-does-not-exist\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error occurred: {type(e).__name__}\")\n",
    "    print(f\"   Message: {str(e)[:100]}...\")\n",
    "    print(\"\\nâœ… Error was captured in trace!\")\n",
    "    print(\"\\nğŸ” In the MLflow UI:\")\n",
    "    print(\"   - Trace marked with error status\")\n",
    "    print(\"   - Error message visible\")\n",
    "    print(\"   - Stack trace preserved\")\n",
    "    print(\"   - Easy to identify failure point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ› Debugging with Traces\n",
    "\n",
    "When your LLM application fails:\n",
    "\n",
    "1. **Find the trace** in the UI (filter by error status)\n",
    "2. **Identify the failing span** (marked red)\n",
    "3. **Inspect inputs** that caused the error\n",
    "4. **View error details** (message, type, stack trace)\n",
    "5. **Fix and re-run** with full history preserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 12: Disabling Autologging\n",
    "\n",
    "Sometimes you want to turn tracing off temporarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable autologging\n",
    "mlflow.openai.autolog(disable=True)\n",
    "mlflow.langchain.autolog(disable=True)\n",
    "\n",
    "print(\"âŒ Autologging disabled\")\n",
    "\n",
    "# This call won't be traced\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"This won't be traced.\"}]\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Call completed without tracing\")\n",
    "\n",
    "# Re-enable for future calls\n",
    "mlflow.openai.autolog()\n",
    "print(\"\\nâœ… Autologging re-enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 13: Best Practices for Tracing\n",
    "\n",
    "Let's review key best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘           Tracing Best Practices                             â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "âœ… DO:\n",
    "\n",
    "1. Enable autologging early in development\n",
    "   - Easier to debug from the start\n",
    "   - Understand performance characteristics early\n",
    "\n",
    "2. Use tracing in all environments\n",
    "   - Development: Full tracing for debugging\n",
    "   - Staging: Trace all requests\n",
    "   - Production: Sample or trace all (depending on volume)\n",
    "\n",
    "3. Combine tracing with experiment tracking\n",
    "   - Traces give you the \"how\"\n",
    "   - Experiments give you the \"what\" and \"why\"\n",
    "\n",
    "4. Review traces regularly\n",
    "   - Look for performance bottlenecks\n",
    "   - Identify expensive operations\n",
    "   - Understand failure patterns\n",
    "\n",
    "5. Use multiple frameworks\n",
    "   - MLflow supports 30+ integrations\n",
    "   - Pick the best tool for each task\n",
    "\n",
    "âŒ DON'T:\n",
    "\n",
    "1. Ignore traces until there's a problem\n",
    "   - Proactive monitoring prevents issues\n",
    "\n",
    "2. Log sensitive data in traces\n",
    "   - PII, credentials, confidential info\n",
    "   - Use data masking if needed\n",
    "\n",
    "3. Assume autologging captures everything\n",
    "   - Custom operations need manual instrumentation\n",
    "   - We'll cover this in the next notebook!\n",
    "\n",
    "4. Over-rely on individual traces\n",
    "   - Look at aggregate patterns\n",
    "   - Use traces + metrics together\n",
    "\n",
    "ğŸ’¡ PRO TIPS:\n",
    "\n",
    "- Set up trace sampling in high-volume production\n",
    "- Archive old traces to manage storage\n",
    "- Export important traces for documentation\n",
    "- Share trace links with your team for debugging\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. âœ… What tracing is and why it's critical for GenAI\n",
    "2. âœ… The trace data model (traces, spans, hierarchy)\n",
    "3. âœ… Automatic tracing with `mlflow.openai.autolog()`\n",
    "4. âœ… How to view and analyze traces in the MLflow UI\n",
    "5. âœ… Tracing multi-step workflows\n",
    "6. âœ… Framework integration (LangChain example)\n",
    "7. âœ… Error tracing and debugging\n",
    "8. âœ… Best practices for production use\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **One line of code** enables complete observability\n",
    "- **Automatic capture** of inputs, outputs, and metadata\n",
    "- **Framework agnostic** - works with OpenAI, LangChain, LlamaIndex, etc.\n",
    "- **Essential for debugging** complex LLM workflows\n",
    "- **Production-ready** with sampling and filtering\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**ğŸ““ Notebook 1.4: Manual Tracing and Advanced Observability**\n",
    "\n",
    "Learn how to:\n",
    "- Create custom spans with `@mlflow.trace`\n",
    "- Add custom attributes to spans\n",
    "- Build hierarchical traces for complex workflows\n",
    "- Trace RAG pipelines with retrieval and generation steps\n",
    "- Trace agentic workflows with tool usage\n",
    "- Debug production issues with advanced techniques\n",
    "\n",
    "### ğŸ¯ Practice Exercise\n",
    "\n",
    "Before moving on, try this:\n",
    "\n",
    "1. Enable tracing\n",
    "2. Build a simple chatbot that:\n",
    "   - Takes user input\n",
    "   - Generates a response\n",
    "   - Evaluates the response quality\n",
    "3. View all three operations in a single trace\n",
    "4. Compare performance across multiple runs\n",
    "\n",
    "Ready? Let's continue to manual tracing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
