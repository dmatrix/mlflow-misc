{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1.6: Framework Integrations\n",
    "\n",
    "## Working with Multiple GenAI Frameworks\n",
    "\n",
    "Welcome to framework integrations! MLflow supports 30+ GenAI frameworks, making it the most flexible platform for LLM development. This notebook shows you how to work with the most popular frameworks.\n",
    "\n",
    "### What You'll Learn\n",
    "- Overview of MLflow's framework integrations\n",
    "- Working with OpenAI (direct API)\n",
    "- Working with LangChain (chains and agents)\n",
    "- Working with LlamaIndex (RAG and indexing)\n",
    "- Comparing frameworks and choosing the right one\n",
    "- Best practices for each framework\n",
    "\n",
    "### Prerequisites\n",
    "- Completed previous notebooks (1.1-1.5)\n",
    "- OpenAI API key configured\n",
    "\n",
    "### Estimated Time: 10-15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Framework Overview\n",
    "\n",
    "### MLflow's 30+ Integrations\n",
    "\n",
    "MLflow provides automatic tracing for:\n",
    "\n",
    "**LLM Providers:**\n",
    "- OpenAI, Anthropic, Cohere, Azure OpenAI\n",
    "- AWS Bedrock, Google Vertex AI\n",
    "- Ollama, vLLM, Together AI\n",
    "\n",
    "**Frameworks:**\n",
    "- LangChain, LlamaIndex, Haystack\n",
    "- DSPy, AutoGen, CrewAI\n",
    "- Guardrails AI, Phoenix\n",
    "\n",
    "### Comparison Matrix\n",
    "\n",
    "| Feature | OpenAI | LangChain | LlamaIndex |\n",
    "|---------|--------|-----------|------------|\n",
    "| **Complexity** | Low | Medium | Medium |\n",
    "| **Learning Curve** | Easy | Medium | Medium |\n",
    "| **Use Case** | Direct calls | Workflows | Doc Q&A |\n",
    "| **Tracing** | âœ… Auto | âœ… Auto | âœ… Auto |\n",
    "| **Agents** | Manual | âœ… Built-in | âœ… Built-in |\n",
    "| **RAG** | Manual | âœ… Built-in | âœ… Built-in |\n",
    "| **Customization** | Full | High | High |\n",
    "| **Performance** | Fast | Medium | Medium |\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "**OpenAI Direct API:**\n",
    "- Simple Q&A applications\n",
    "- Maximum control over prompts\n",
    "- Lowest latency\n",
    "- Custom implementations\n",
    "\n",
    "**LangChain:**\n",
    "- Complex multi-step workflows\n",
    "- Agent applications with tools\n",
    "- Need for abstractions\n",
    "- Rapid prototyping\n",
    "\n",
    "**LlamaIndex:**\n",
    "- Document-heavy applications\n",
    "- Advanced indexing strategies\n",
    "- Multiple data sources\n",
    "- Knowledge management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional frameworks\n",
    "!uv add langchain langchain-openai llama-index\n",
    "\n",
    "print(\"âœ… Frameworks installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "from utils.clnt_utils import is_databricks_client, get_databricks_client, get_openai_client\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "use_databricks_provider = is_databricks_client()\n",
    "if use_databricks_provider:\n",
    "    client = get_databricks_client()\n",
    "else:\n",
    "    # Initialize OpenAI\n",
    "    client = get_openai_client()\n",
    "\n",
    "model_name = \"databricks-gpt-5-mini\" if use_databricks_provider else \"gpt-5-mini\"\n",
    "\n",
    "print(\"âœ… Environment configured: using\", \"Databricks\" if use_databricks_provider else \"OpenAI\", \"client\")\n",
    "print(f\"   MLflow version: {mlflow.__version__}\")\n",
    "print(f\"   Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"   Model name: {model_name}\")\n",
    "\n",
    "#Enable OpenAI autologging\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "mlflow.set_experiment(\"09-framework-integrations\")\n",
    "\n",
    "print(\"âœ… Environment configured\")\n",
    "print(\"   OpenAI autologging: ENABLED\")\n",
    "\n",
    "print(f\"   MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: OpenAI Framework (Already Covered)\n",
    "\n",
    "We've been using OpenAI throughout this tutorial series.\n",
    "\n",
    "### Quick Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Strengths\n",
    "\n",
    "- âœ… **Simplicity**: Direct API calls\n",
    "- âœ… **Performance**: No abstraction overhead\n",
    "- âœ… **Control**: Full control over prompts\n",
    "- âœ… **Latest features**: Immediate access to new models\n",
    "- âœ… **Documentation**: Extensive official docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: LangChain Framework\n",
    "\n",
    "LangChain provides abstractions for building LLM applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Enable LangChain autologging\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "print(\"âœ… LangChain autologging enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.clnt_utils import get_databricks_langchain_chat_client, get_langchain_chat_openai_client\n",
    "# Simple LangChain chain\n",
    "print(\"\\nðŸ”— LangChain Example 1: Simple Chain\\n\")\n",
    "\n",
    "# Create prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a {role}. Answer: {question}\"\n",
    ")\n",
    "\n",
    "# Create LLM based on provider\n",
    "\n",
    "if use_databricks_provider:\n",
    "    llm = get_databricks_langchain_chat_client(model_name, temperature=1.0)\n",
    "else:\n",
    "    llm = get_langchain_chat_openai_client(mode=model_name, temperature=1.0)\n",
    "\n",
    "# Create chain using LCEL (LangChain Expression Language)\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run chain (automatically traced!)\n",
    "result = chain.invoke({\n",
    "    \"role\": \"MLflow expert\",\n",
    "    \"question\": \"What makes LangChain different from using OpenAI directly?\"\n",
    "})\n",
    "\n",
    "print(result)\n",
    "print(\"\\nâœ… Chain execution fully traced!\")\n",
    "print(\"   - Prompt construction\")\n",
    "print(\"   - LLM call\")\n",
    "print(\"   - Output parsing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex chain with multiple steps\n",
    "print(\"\\nðŸ”— LangChain Example 2: Multi-Step Chain\\n\")\n",
    "\n",
    "\n",
    "# Step 1: Generate topic\n",
    "topic_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate a technical topic about {domain}\"\n",
    ")\n",
    "topic_chain = topic_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Step 2: Create outline\n",
    "outline_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Create a 3-point outline for: {topic}\"\n",
    ")\n",
    "outline_chain = outline_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Execute pipeline\n",
    "topic = topic_chain.invoke({\"domain\": \"MLOps\"})\n",
    "print(f\"Topic: {topic}\\n\")\n",
    "\n",
    "outline = outline_chain.invoke({\"topic\": topic})\n",
    "print(f\"Outline:\\n{outline}\")\n",
    "\n",
    "print(\"\\nâœ… Multi-step chain traced!\")\n",
    "print(\"   Each chain creates separate spans\")\n",
    "print(\"   Full execution visible in MLflow UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Strengths\n",
    "\n",
    "- âœ… **Abstractions**: Reusable components\n",
    "- âœ… **Chains**: Complex multi-step workflows\n",
    "- âœ… **Agents**: Built-in agent patterns\n",
    "- âœ… **Tools**: Easy tool integration\n",
    "- âœ… **Community**: Large ecosystem\n",
    "- âœ… **RAG**: Built-in retrieval components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: LlamaIndex Framework\n",
    "\n",
    "LlamaIndex specializes in document indexing and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, Document\n",
    "\n",
    "# Enable LlamaIndex autologging\n",
    "mlflow.llama_index.autolog()\n",
    "\n",
    "print(\"âœ… LlamaIndex autologging enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents\n",
    "print(\"\\nðŸ“š LlamaIndex Example: Document Q&A\\n\")\n",
    "\n",
    "documents = [\n",
    "    Document(text=\"MLflow is an open source platform for the complete ML lifecycle. It provides experiment tracking, model registry, and deployment capabilities.\"),\n",
    "    Document(text=\"MLflow Tracing captures the complete execution of GenAI applications, including LLM calls, retrieval steps, and tool usage.\"),\n",
    "    Document(text=\"MLflow integrates with 30+ frameworks including OpenAI, LangChain, LlamaIndex, and more.\"),\n",
    "    Document(text=\"MLflow supports collaborative development with experiment sharing, prompt management, and model versioning.\"),\n",
    "]\n",
    "\n",
    "# Create index (automatically traced)\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Create query engine\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Query the index (automatically traced)\n",
    "response = query_engine.query(\"What tracing capabilities does MLflow have?\")\n",
    "\n",
    "print(f\"Query: What tracing capabilities does MLflow have?\")\n",
    "print(f\"\\nAnswer: {response}\")\n",
    "\n",
    "print(\"\\nâœ… LlamaIndex execution fully traced!\")\n",
    "print(\"   - Document indexing\")\n",
    "print(\"   - Query embedding\")\n",
    "print(\"   - Retrieval\")\n",
    "print(\"   - Response synthesis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LlamaIndex Strengths\n",
    "\n",
    "- âœ… **Indexing**: Advanced document indexing\n",
    "- âœ… **Data loaders**: 100+ data connectors\n",
    "- âœ… **RAG**: Specialized for retrieval\n",
    "- âœ… **Query engines**: Multiple query strategies\n",
    "- âœ… **Agents**: Data-aware agents\n",
    "- âœ… **Performance**: Optimized for documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Comparing Traces Across Frameworks\n",
    "\n",
    "Let's run the same query using all three frameworks and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "query = \"What is the main benefit of using MLflow for GenAI?\"\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"\\nðŸ”¬ Comparing frameworks...\\n\")\n",
    "\n",
    "# Test 1: OpenAI\n",
    "print(\"Testing OpenAI...\")\n",
    "start = time.time()\n",
    "response1 = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": query}],\n",
    "    temperature=0.3,\n",
    "    max_tokens=100\n",
    ")\n",
    "latency1 = time.time() - start\n",
    "answer1 = response1.choices[0].message.content\n",
    "tokens1 = response1.usage.total_tokens\n",
    "results.append((\"OpenAI\", latency1, tokens1, len(answer1)))\n",
    "print(f\"  âœ“ {latency1:.2f}s, {tokens1} tokens\\n\")\n",
    "\n",
    "# Test 2: LangChain\n",
    "print(\"Testing LangChain...\")\n",
    "start = time.time()\n",
    "chain = ChatPromptTemplate.from_template(\"{query}\") | \\\n",
    "        ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3, max_tokens=100) | \\\n",
    "        StrOutputParser()\n",
    "answer2 = chain.invoke({\"query\": query})\n",
    "latency2 = time.time() - start\n",
    "results.append((\"LangChain\", latency2, \"N/A\", len(answer2)))\n",
    "print(f\"  âœ“ {latency2:.2f}s\\n\")\n",
    "\n",
    "# Test 3: LlamaIndex\n",
    "print(\"Testing LlamaIndex...\")\n",
    "start = time.time()\n",
    "response3 = query_engine.query(query)\n",
    "latency3 = time.time() - start\n",
    "answer3 = str(response3)\n",
    "results.append((\"LlamaIndex\", latency3, \"N/A\", len(answer3)))\n",
    "print(f\"  âœ“ {latency3:.2f}s\\n\")\n",
    "\n",
    "# Display comparison\n",
    "print(\"=\"*60)\n",
    "print(\"COMPARISON RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Framework':<15} {'Latency':<12} {'Tokens':<10} {'Length'}\")\n",
    "print(\"-\"*60)\n",
    "for framework, latency, tokens, length in results:\n",
    "    print(f\"{framework:<15} {latency:<12.2f} {str(tokens):<10} {length}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Observations:\")\n",
    "print(\"   - OpenAI: Lowest latency (direct API)\")\n",
    "print(\"   - LangChain: Slight overhead from abstractions\")\n",
    "print(\"   - LlamaIndex: Extra time for retrieval\")\n",
    "print(\"\\nðŸ” View all traces in MLflow UI to see the details!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Choosing the Right Framework\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "```\n",
    "Do you need document retrieval?\n",
    "â”œâ”€ Yes â†’ Consider LlamaIndex\n",
    "â”‚   â””â”€ Complex indexing strategies? â†’ LlamaIndex\n",
    "â”‚   â””â”€ Simple RAG? â†’ LangChain or OpenAI\n",
    "â”‚\n",
    "â””â”€ No â†’ Do you need agents?\n",
    "    â”œâ”€ Yes â†’ LangChain\n",
    "    â”‚   â””â”€ Complex multi-tool workflows? â†’ LangChain\n",
    "    â”‚   â””â”€ Simple agent? â†’ OpenAI + custom code\n",
    "    â”‚\n",
    "    â””â”€ No â†’ OpenAI (for simplicity and performance)\n",
    "```\n",
    "\n",
    "### Real-World Recommendations\n",
    "\n",
    "**Use OpenAI when:**\n",
    "- Building a simple chatbot\n",
    "- Need maximum performance\n",
    "- Want full control\n",
    "- Prototyping quickly\n",
    "\n",
    "**Use LangChain when:**\n",
    "- Building complex workflows\n",
    "- Need agent capabilities\n",
    "- Want reusable components\n",
    "- Large team needs abstractions\n",
    "\n",
    "**Use LlamaIndex when:**\n",
    "- Document Q&A is primary use case\n",
    "- Multiple data sources\n",
    "- Advanced indexing needed\n",
    "- Knowledge management focus\n",
    "\n",
    "**Mix frameworks when:**\n",
    "- You need strengths of multiple\n",
    "- Different parts of application\n",
    "- Gradual migration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Best Practices by Framework\n",
    "\n",
    "### OpenAI Best Practices\n",
    "\n",
    "```python\n",
    "# DO:\n",
    "âœ… Use structured prompts\n",
    "âœ… Implement retry logic\n",
    "âœ… Handle rate limits\n",
    "âœ… Stream responses for UX\n",
    "âœ… Cache results when possible\n",
    "\n",
    "# DON'T:\n",
    "âŒ Hardcode prompts\n",
    "âŒ Ignore error responses\n",
    "âŒ Skip cost tracking\n",
    "âŒ Use synchronous calls in production\n",
    "```\n",
    "\n",
    "### LangChain Best Practices\n",
    "\n",
    "```python\n",
    "# DO:\n",
    "âœ… Use LCEL for chains\n",
    "âœ… Leverage built-in components\n",
    "âœ… Test chains independently\n",
    "âœ… Use async for better performance\n",
    "âœ… Enable debug mode during development\n",
    "\n",
    "# DON'T:\n",
    "âŒ Over-abstract simple use cases\n",
    "âŒ Ignore performance overhead\n",
    "âŒ Skip error handling in chains\n",
    "âŒ Use deprecated components\n",
    "```\n",
    "\n",
    "### LlamaIndex Best Practices\n",
    "\n",
    "```python\n",
    "# DO:\n",
    "âœ… Choose appropriate index type\n",
    "âœ… Chunk documents thoughtfully\n",
    "âœ… Use metadata for filtering\n",
    "âœ… Cache embeddings\n",
    "âœ… Monitor retrieval quality\n",
    "\n",
    "# DON'T:\n",
    "âŒ Index without preprocessing\n",
    "âŒ Use default settings blindly\n",
    "âŒ Ignore index refresh strategies\n",
    "âŒ Skip evaluation of retrievals\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. âœ… Overview of MLflow's 30+ framework integrations\n",
    "2. âœ… Working with OpenAI (direct API)\n",
    "3. âœ… Working with LangChain (chains and workflows)\n",
    "4. âœ… Working with LlamaIndex (document Q&A)\n",
    "5. âœ… Comparing frameworks side-by-side\n",
    "6. âœ… Choosing the right framework for your use case\n",
    "7. âœ… Best practices for each framework\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **All frameworks** are automatically traced by MLflow\n",
    "- **Choose based on use case**, not hype\n",
    "- **OpenAI** for simplicity and performance\n",
    "- **LangChain** for complex workflows\n",
    "- **LlamaIndex** for document-heavy applications\n",
    "- **Mix frameworks** when it makes sense\n",
    "\n",
    "### Framework Comparison Summary\n",
    "\n",
    "| Aspect | OpenAI | LangChain | LlamaIndex |\n",
    "|--------|--------|-----------|------------|\n",
    "| **Best For** | Simple apps | Workflows | Doc Q&A |\n",
    "| **Performance** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ |\n",
    "| **Ease of Use** | â­â­â­â­â­ | â­â­â­ | â­â­â­ |\n",
    "| **Flexibility** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ |\n",
    "| **RAG Support** | â­â­ | â­â­â­â­ | â­â­â­â­â­ |\n",
    "| **Agents** | â­â­ | â­â­â­â­â­ | â­â­â­â­ |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**ðŸ““ Notebook 1.7: Complete RAG Application**\n",
    "\n",
    "Put everything together:\n",
    "- Build a production-ready RAG system\n",
    "- Combine experiment tracking and tracing\n",
    "- Implement caching and error handling\n",
    "- Add monitoring and alerting\n",
    "- Deploy with confidence!\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [MLflow LangChain Integration](https://mlflow.org/docs/latest/llms/langchain/index.html)\n",
    "- [MLflow LlamaIndex Integration](https://mlflow.org/docs/latest/llms/llama-index/index.html)\n",
    "- [Framework Examples](https://github.com/mlflow/mlflow/tree/master/examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
