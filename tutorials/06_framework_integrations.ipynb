{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1.6: Framework Integrations\n",
    "\n",
    "![](images/7_Framework-Integrations.png)\n",
    "\n",
    "## Working with Multiple GenAI Frameworks\n",
    "\n",
    "Welcome to framework integrations! MLflow supports 30+ GenAI frameworks, making it the most flexible platform for LLM development. This notebook shows you how to work with the most popular frameworks.\n",
    "\n",
    "### What You'll Learn\n",
    "- Overview of MLflow's framework integrations\n",
    "- Working with OpenAI (direct API)\n",
    "- Working with LangChain (chains and agents)\n",
    "- Working with LlamaIndex (document indexing and RAG)\n",
    "- Comparing frameworks and choosing the right one\n",
    "- Best practices for each framework\n",
    "\n",
    "### Prerequisites\n",
    "- Completed previous notebooks (1.1-1.5)\n",
    "- OpenAI API key or Databricks AI Gateway configured\n",
    "\n",
    "### Estimated Time: 15-20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Framework Overview\n",
    "\n",
    "### MLflow's 40+ [Integrations](https://mlflow.org/docs/latest/genai/tracing/integrations/)\n",
    "\n",
    "MLflow provides automatic tracing for:\n",
    "\n",
    "**LLM Providers:**\n",
    "- OpenAI, Anthropic, Cohere, Azure OpenAI\n",
    "- AWS Bedrock, Google Vertex AI\n",
    "- Ollama, vLLM, Together AI\n",
    "\n",
    "**Frameworks:**\n",
    "- LangChain, LlamaIndex, Haystack\n",
    "- DSPy, AutoGen, CrewAI\n",
    "- Guardrails AI, Phoenix\n",
    "\n",
    "### Comparison Matrix\n",
    "\n",
    "| Feature | OpenAI | LangChain | LlamaIndex |\n",
    "|---------|--------|-----------|------------|\n",
    "| **Complexity** | Low | Medium | Medium |\n",
    "| **Learning Curve** | Easy | Medium | Medium |\n",
    "| **Use Case** | Direct calls | Workflows | Doc Q&A |\n",
    "| **Tracing** | ‚úÖ Auto | ‚úÖ Auto | ‚úÖ Auto |\n",
    "| **Agents** | Manual | ‚úÖ Built-in | ‚úÖ Built-in |\n",
    "| **RAG** | Manual | ‚úÖ Built-in | ‚úÖ Built-in |\n",
    "| **Customization** | Full | High | High |\n",
    "| **Performance** | Fast | Medium | Medium |\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "**OpenAI Direct API:**\n",
    "- Simple Q&A applications\n",
    "- Maximum control over prompts\n",
    "- Lowest latency\n",
    "- Custom implementations\n",
    "\n",
    "**LangChain:**\n",
    "- Complex multi-step workflows\n",
    "- Agent applications with tools\n",
    "- Need for abstractions\n",
    "- Rapid prototyping\n",
    "\n",
    "**LlamaIndex:**\n",
    "- Document-heavy applications\n",
    "- Advanced indexing strategies\n",
    "- Multiple data sources\n",
    "- Knowledge management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m355 packages\u001b[0m \u001b[2min 8ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m27 packages\u001b[0m \u001b[2min 70ms\u001b[0m\u001b[0m0.4.1                          \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohttp-retry\u001b[0m\u001b[2m==2.9.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdatabricks-ai-bridge\u001b[0m\u001b[2m==0.13.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdatabricks-langchain\u001b[0m\u001b[2m==0.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdatabricks-vectorsearch\u001b[0m\u001b[2m==0.64\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdeprecation\u001b[0m\u001b[2m==2.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx-sse\u001b[0m\u001b[2m==0.4.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjsonpatch\u001b[0m\u001b[2m==1.33\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain\u001b[0m\u001b[2m==1.2.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-classic\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-community\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-core\u001b[0m\u001b[2m==1.2.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-openai\u001b[0m\u001b[2m==1.1.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-text-splitters\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph\u001b[0m\u001b[2m==1.0.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-checkpoint\u001b[0m\u001b[2m==4.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-prebuilt\u001b[0m\u001b[2m==1.0.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanggraph-sdk\u001b[0m\u001b[2m==0.3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangsmith\u001b[0m\u001b[2m==0.6.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1morjson\u001b[0m\u001b[2m==3.11.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mormsgpack\u001b[0m\u001b[2m==1.12.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrequests-toolbelt\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1munitycatalog-ai\u001b[0m\u001b[2m==0.3.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1munitycatalog-client\u001b[0m\u001b[2m==0.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1munitycatalog-langchain\u001b[0m\u001b[2m==0.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1muuid-utils\u001b[0m\u001b[2m==0.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxxhash\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mzstandard\u001b[0m\u001b[2m==0.25.0\u001b[0m\n",
      "‚úÖ Frameworks installed\n"
     ]
    }
   ],
   "source": [
    "# Install additional frameworks\n",
    "!uv add langchain langchain-openai\n",
    "\n",
    "print(\"‚úÖ Frameworks installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment configured: using OpenAI client\n",
      "   MLflow version: 3.9.0\n",
      "   Tracking URI: http://localhost:5000\n",
      "   Model name: gpt-5-mini\n",
      "‚úÖ OpenAI autologging: ENABLED\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "from utils.clnt_utils import (\n",
    "    is_databricks_client, \n",
    "    is_databricks_ai_gateway_client,\n",
    "    get_databricks_ai_gateway_client,\n",
    "    get_openai_client,\n",
    "    get_ai_gateway_model_names\n",
    ")\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "# Determine provider and initialize client\n",
    "use_databricks_provider = is_databricks_client()\n",
    "use_databricks_ai_gateway = is_databricks_ai_gateway_client()\n",
    "\n",
    "if use_databricks_ai_gateway:\n",
    "    client = get_databricks_ai_gateway_client()\n",
    "    model_name = get_ai_gateway_model_names()[0]\n",
    "    provider_name = \"Databricks AI Gateway\"\n",
    "else:\n",
    "    client = get_openai_client()\n",
    "    model_name = \"gpt-5-mini\"\n",
    "    provider_name = \"OpenAI\"\n",
    "\n",
    "print(f\"‚úÖ Environment configured: using {provider_name} client\")\n",
    "print(f\"   MLflow version: {mlflow.__version__}\")\n",
    "print(f\"   Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"   Model name: {model_name}\")\n",
    "\n",
    "# Enable OpenAI autologging\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "mlflow.set_experiment(\"09-framework-integrations\")\n",
    "\n",
    "print(\"‚úÖ OpenAI autologging: ENABLED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: OpenAI Framework (Already Covered)\n",
    "\n",
    "We've been using OpenAI throughout this tutorial series.\n",
    "\n",
    "### Quick Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Strengths\n",
    "\n",
    "- ‚úÖ **Simplicity**: Direct API calls\n",
    "- ‚úÖ **Performance**: No abstraction overhead\n",
    "- ‚úÖ **Control**: Full control over prompts\n",
    "- ‚úÖ **Latest features**: Immediate access to new models\n",
    "- ‚úÖ **Documentation**: Extensive official docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: LangChain Framework\n",
    "\n",
    "LangChain provides abstractions for building LLM applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangChain autologging enabled\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Enable LangChain autologging\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "print(\"‚úÖ LangChain autologging enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó LangChain Example 1: Simple Chain\n",
      "\n",
      "Short answer\n",
      "- OpenAI is a model API (you send prompts, get completions/embeddings). LangChain is a higher-level framework that wraps LLMs (including OpenAI) and provides reusable building blocks for building real applications: prompt templating, chains, agents, memory, retrievers/vectorstores, document loaders, tool integrations, orchestration, caching, and evaluation.\n",
      "\n",
      "What LangChain gives you that using OpenAI directly does not\n",
      "- Abstractions for composition\n",
      "  - Chains/flows: compose multiple LLM calls and non-LLM steps into reusable pipelines.\n",
      "  - Prompt templates: manage variable substitution, partial prompts, and prompt versioning.\n",
      "- Retrieval & RAG support\n",
      "  - Built-in document loaders, retrievers, and vectorstore connectors (FAISS, Pinecone, Milvus, etc.) so you can easily build retrieval-augmented generation.\n",
      "- Agents and tools\n",
      "  - Agent frameworks that let models call tools (search, calculators, APIs) with orchestration and structured tool interfaces.\n",
      "- Conversation and persistent memory\n",
      "  - Memory components to store conversational state, summaries, or long-term context across calls.\n",
      "- Model-agnostic LLM wrappers\n",
      "  - Swap backend models (OpenAI, Anthropic, Hugging Face, local LLMs) with the same high-level code.\n",
      "- Utilities for production readiness\n",
      "  - Streaming support, caching, rate-limit handling, retrying, instrumentation/callbacks, prompt testing, and evaluation helpers.\n",
      "- Connectors & ecosystem\n",
      "  - Prebuilt connectors for common data sources (Google Drive, Slack, SQL, Notion, S3, etc.) and for vector DBs and embeddings.\n",
      "- Developer ergonomics\n",
      "  - Standardized patterns for common tasks (QA over docs, summarization, extraction), reducing boilerplate and mistakes.\n",
      "\n",
      "What you still get from using OpenAI directly\n",
      "- Full low-level control of every API call, prompt shape, and token accounting.\n",
      "- Fewer dependencies, minimal abstraction overhead, and potentially simpler billing visibility per call.\n",
      "- Useful for trivial/one-off tasks where you just need a single request or tight optimization on token usage.\n",
      "\n",
      "Tradeoffs and when to use which\n",
      "- Use OpenAI directly when:\n",
      "  - Your use case is a single, simple call or you need tight control/optimizations.\n",
      "  - You want to avoid extra dependencies or abstraction overhead.\n",
      "- Use LangChain when:\n",
      "  - You‚Äôre building non-trivial apps: RAG systems, multi-step pipelines, agents that call tools, stateful chatbots, or you want reusable components and connectors.\n",
      "  - You want to experiment with different model backends without rewriting business logic.\n",
      "- Downsides of LangChain:\n",
      "  - Additional dependency and learning curve; sometimes opinionated patterns.\n",
      "  - Can mask low-level behavior (and costs/latency) unless you inspect the underlying calls.\n",
      "  - Occasionally fragmentation between versions and community components.\n",
      "\n",
      "Minimal example (conceptual)\n",
      "- Direct OpenAI:\n",
      "  - Construct prompt ‚Üí call OpenAI API ‚Üí parse result.\n",
      "- LangChain:\n",
      "  - Create PromptTemplate ‚Üí LLM wrapper (OpenAI) ‚Üí Chain that calls retriever or tools ‚Üí run chain and get structured output.\n",
      "\n",
      "Bottom line\n",
      "LangChain is a toolkit that sits on top of model APIs (like OpenAI) to make building robust, composable, production LLM applications much easier. If you only need single model calls, OpenAI alone is enough; if you need pipelines, retrieval, agents, memory, or many integrations, LangChain saves a lot of time and boilerplate.\n",
      "\n",
      "‚úÖ Chain execution fully traced!\n",
      "   - Prompt construction\n",
      "   - LLM call\n",
      "   - Output parsing\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-c8bc4cbda9d902740eff1c64e653fe3d&amp;experiment_id=9&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-c8bc4cbda9d902740eff1c64e653fe3d)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.clnt_utils import (\n",
    "    get_databricks_langchain_chat_client, \n",
    "    get_langchain_chat_openai_client,\n",
    "    get_databricks_ai_gateway_langchain_client\n",
    ")\n",
    "\n",
    "# Simple LangChain chain\n",
    "print(\"\\nüîó LangChain Example 1: Simple Chain\\n\")\n",
    "\n",
    "# Create prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a {role}. Answer: {question}\"\n",
    ")\n",
    "\n",
    "# Create LLM based on provider\n",
    "if use_databricks_ai_gateway:\n",
    "    llm = get_databricks_ai_gateway_langchain_client(model_name, temperature=1.0)\n",
    "elif use_databricks_provider:\n",
    "    llm = get_databricks_langchain_chat_client(model_name, temperature=1.0)\n",
    "else:\n",
    "    llm = get_langchain_chat_openai_client(model_name, temperature=1.0)\n",
    "\n",
    "# Create chain using LCEL (LangChain Expression Language)\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run chain (automatically traced!)\n",
    "result = chain.invoke({\n",
    "    \"role\": \"MLflow expert\",\n",
    "    \"question\": \"What makes LangChain different from using OpenAI directly?\"\n",
    "})\n",
    "\n",
    "print(result)\n",
    "print(\"\\n‚úÖ Chain execution fully traced!\")\n",
    "print(\"   - Prompt construction\")\n",
    "print(\"   - LLM call\")\n",
    "print(\"   - Output parsing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # More complex chain with multiple steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó LangChain Example 2: Multi-Step Chain\n",
      "\n",
      "Topic: Topic: Continuous Model Validation & Certification for Production MLOps Pipelines\n",
      "\n",
      "Short description\n",
      "Design and implement an automated, end-to-end system that continuously validates machine learning models in production across correctness, performance, robustness, fairness, and compliance, and issues machine-readable certification artifacts (model cards, audit logs, rollback triggers). The system should integrate with CI/CD for models, detect and react to data/model drift, and provide reproducible evidence for safe deployment.\n",
      "\n",
      "Why this is important\n",
      "- Models degrade in real-world deployments due to data drift, changing user behavior, or upstream changes.\n",
      "- Regulated domains (finance, healthcare) require auditable validation and reproducible certification of models.\n",
      "- Automated continuous validation shortens mean time to detection/correction and enforces governance without slowing iteration.\n",
      "\n",
      "Key research/engineering questions\n",
      "- Which drift/deterioration detection techniques provide reliable early warning with low false positives?\n",
      "- How to combine statistical tests, performance monitoring, and adversarial/robustness checks into a single actionable verdict?\n",
      "- How to define certification criteria that balance business risk, fairness, and model innovation?\n",
      "- How to produce reproducible, tamper-evident certification artifacts for audits?\n",
      "- What are best practices for automated safe rollout (canary, shadow, multi-armed bandits) tied to validation checks?\n",
      "\n",
      "Core components and responsibilities\n",
      "- Data validation: schema checks, missingness, datatype changes, sample-quality rules (Great Expectations, WhyLogs).\n",
      "- Distribution monitoring: univariate/multivariate drift (KS, PSI, MMD), embedding-space drift, population-stability tests (Evidently, River).\n",
      "- Performance validation: online/offline metrics (accuracy, AUC, MSE), calibration (ECE), confidence monitoring, per-slice metrics.\n",
      "- Fairness and bias checks: group metrics (demographic parity, equalized odds), intersectional slices.\n",
      "- Robustness/attack surface tests: input perturbation, adversarial examples, corrupted-input benchmarks.\n",
      "- Explainability checks: counterfactual consistency, feature importance stability.\n",
      "- Decision logic: rules/thresholds and ML-based meta-detector to decide alerts, partial rollbacks, or full rollback.\n",
      "- Traceability & certification: model cards, provenance (parameters, training data hash, environment), tamper-evident audit logs (immutable storage / signed artifacts).\n",
      "- Orchestration & automation: integrate with CI/CD (Git, MLflow, Argo/ArgoCD, Tekton, Jenkins), deployment platforms (Kubernetes, Seldon, BentoML), and alerting (PagerDuty, Slack).\n",
      "- Observability & dashboards: metrics (Prometheus), visualizations (Grafana), drift/performance dashboards, automated reporting.\n",
      "\n",
      "Possible architecture\n",
      "- CI: Pre-deployment checks run in CI: unit tests, static analysis, offline validation, fairness tests, synthetic robustness tests.\n",
      "- Canary/Shadow: Deploy candidate model to canary/shadow traffic; collect telemetry and run real-time validation operators.\n",
      "- Monitoring: Streaming data validation and aggregation (Kafka + Flink/River/Rust), compute drift and performance metrics continuously.\n",
      "- Decision engine: Rules + ML meta-detector that creates incidents, triggers rollbacks or operator review.\n",
      "- Audit store: Persist model artifacts, metrics and decisions in an immutable store (object storage with signed manifests or blockchain-like ledger).\n",
      "- Reporting: Automatic model card generation and scheduled certification reviews.\n",
      "\n",
      "Evaluation plan / experiments\n",
      "- Benchmarks: Use controlled drift scenarios (synthetic feature shift, label shift, concept shift) to measure detection time and false positives.\n",
      "- Datasets: Tabular‚ÄîAdult, LendingClub, CreditCardFraud; Time-series‚ÄîNYC Taxi; Vision‚ÄîCIFAR-10 corrupted sets; Healthcare‚ÄîMIMIC (with privacy considerations).\n",
      "- Metrics to evaluate the system: detection latency, false-positive rate, precision/recall of deterioration alerts, business KPI impact (loss reduction), rollback frequency, operator workload reduction.\n",
      "- A/B tests: Compare baseline deployment (no automated validation) vs. pipeline with automatic gating/canary + validation to measure mean time to recovery and incident rate.\n",
      "\n",
      "Tools & libraries (examples)\n",
      "- Data validation: Great Expectations, WhyLogs, TFDV\n",
      "- Drift monitoring: Evidently AI, River, Alibi Detect, scikit-multiflow\n",
      "- Experiment & model tracking: MLflow, Weights & Biases, DVC\n",
      "- Orchestration: Argo Workflows, Airflow, Kubeflow Pipelines\n",
      "- Serving & rollout: Seldon Core, BentoML, Cortex, Istio for traffic splitting\n",
      "- Observability: Prometheus, Grafana, OpenTelemetry\n",
      "- Security & provenance: Sigstore, Notary, hashed manifests\n",
      "\n",
      "Deliverables for a project or thesis\n",
      "- A reference implementation (CI ‚Üí canary ‚Üí automated validation ‚Üí rollback) deployable on Kubernetes\n",
      "- A decision-engine module with pluggable detectors and configurable policies\n",
      "- Automated model card and certification artifacts generator\n",
      "- Evaluation report showing detection characteristics on benchmark scenarios\n",
      "- A runbook for operators and compliance/audit documentation\n",
      "\n",
      "Potential extensions\n",
      "- Incorporate privacy-preserving validation (federated checks, DP metrics)\n",
      "- Use causal methods to distinguish concept drift from upstream confounders\n",
      "- Auto-remediation strategies (automated data labeling/active learning to recover model performance)\n",
      "- Marketplace-ready certification for third-party model vendors\n",
      "\n",
      "If you want, I can:\n",
      "- Narrow this into a specific implementation plan (tech stack + architecture diagram + step-by-step milestones).\n",
      "- Produce a sample CI pipeline and policy rules for model certification.\n",
      "- Draft example model card and audit ledger schema. Which would you like next?\n",
      "\n",
      "Outline:\n",
      "1) Purpose, scope, and goals\n",
      "- Short description: automated, end-to-end system that continuously validates production ML models for correctness, performance, robustness, fairness and compliance, and emits machine-readable certification artifacts (model cards, signed audit logs, rollback triggers). Integrates with model CI/CD, detects/reacts to data/model drift, and produces reproducible evidence for safe deployment.\n",
      "- Why it matters: models degrade in production; regulated domains need auditable validation; automation reduces detection-to-repair time and enforces governance without blocking iteration.\n",
      "- High-level objectives: early, low‚Äëfalse‚Äëpositive detection of deterioration; unified actionable verdicts combining statistical, performance and adversarial checks; tamper‚Äëevident certification for audits; safe rollout automation (canary, shadow, bandits).\n",
      "\n",
      "2) Architecture and core components\n",
      "- Pre-deploy CI checks: unit tests, static analysis, offline performance/fairness/robustness tests and synthetic drift scenarios.\n",
      "- Runtime monitoring & validation:\n",
      "  - Data validation: schema, missingness, sample-quality (e.g., Great Expectations, WhyLogs).\n",
      "  - Distribution/drift monitoring: univariate/multivariate drift (KS, PSI, MMD), embedding-space drift, population-stability.\n",
      "  - Performance validation: online/offline metrics (accuracy, AUC, MSE), calibration (ECE), confidence monitoring, per-slice metrics.\n",
      "  - Fairness & bias: group and intersectional metrics (demographic parity, equalized odds).\n",
      "  - Robustness & explainability: adversarial/corrupted-input tests, counterfactual consistency, feature-importance stability.\n",
      "- Decision engine & automation:\n",
      "  - Rules + ML meta-detector that fuses signals into incidents, partial or full rollback, or operator review.\n",
      "  - Safe rollout integrations: canary/shadow traffic, multi-armed bandits, automatic gating with CI/CD (Argo, Tekton, Jenkins).\n",
      "- Traceability & certification:\n",
      "  - Model provenance (params, training-data hashes, environment), machine-readable model cards, tamper-evident audit logs (signed manifests or immutable store / ledger).\n",
      "- Orchestration & observability:\n",
      "  - Streaming telemetry stack (Kafka + Flink/River), model serving and traffic control (Kubernetes, Seldon, BentoML, Istio), alerting (PagerDuty/Slack), dashboards (Prometheus/Grafana).\n",
      "\n",
      "3) Evaluation, deliverables and next steps\n",
      "- Evaluation plan:\n",
      "  - Controlled drift experiments (feature/label/concept shift) to measure detection latency and false positives.\n",
      "  - Benchmarks & datasets: Adult, LendingClub, CreditCardFraud, NYC Taxi, CIFAR-10 corruptions, MIMIC (privacy-aware).\n",
      "  - Metrics: detection latency, FP rate, precision/recall of deterioration alerts, business KPI impact, rollback frequency, operator workload reduction; A/B tests vs. baseline.\n",
      "- Tools & example stack: Great Expectations, WhyLogs, Evidently/Alibi, River, MLflow/W&B, Argo/Kubeflow, Seldon/BentoML, Prometheus/Grafana, Sigstore.\n",
      "- Deliverables for a project/thesis:\n",
      "  - Reference implementation (CI ‚Üí canary ‚Üí real-time validation ‚Üí rollback) runnable on Kubernetes.\n",
      "  - Pluggable decision-engine and policy config, automated model-card + signed certification artifact generator.\n",
      "  - Evaluation report with detection characteristics and runbook/audit documentation.\n",
      "- Extensions & research directions: privacy-preserving validation (federated/DP), causal drift attribution, automated remediation (active learning), marketplace-ready third-party certification.\n",
      "- Next step offer: I can expand any of the three points into a full implementation plan, produce a sample CI/CD policy and pipeline, or draft example model-card + audit ledger schema ‚Äî which would you like?\n",
      "\n",
      "‚úÖ Multi-step chain traced!\n",
      "   Each chain creates separate spans\n",
      "   Full execution visible in MLflow UI\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-06534b1afa01ea0a94b693bcec2f3ac2&amp;experiment_id=9&amp;trace_id=tr-3c9e500b33a8eef8293bed34063d1064&amp;experiment_id=9&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-06534b1afa01ea0a94b693bcec2f3ac2), Trace(trace_id=tr-3c9e500b33a8eef8293bed34063d1064)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# More complex chain with multiple steps\n",
    "print(\"\\nüîó LangChain Example 2: Multi-Step Chain\\n\")\n",
    "\n",
    "\n",
    "# Step 1: Generate topic\n",
    "topic_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate a technical topic about {domain}\"\n",
    ")\n",
    "topic_chain = topic_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Step 2: Create outline\n",
    "outline_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Create a 3-point outline for: {topic}\"\n",
    ")\n",
    "outline_chain = outline_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Execute pipeline\n",
    "topic = topic_chain.invoke({\"domain\": \"MLOps\"})\n",
    "print(f\"Topic: {topic}\\n\")\n",
    "\n",
    "outline = outline_chain.invoke({\"topic\": topic})\n",
    "print(f\"Outline:\\n{outline}\")\n",
    "\n",
    "print(\"\\n‚úÖ Multi-step chain traced!\")\n",
    "print(\"   Each chain creates separate spans\")\n",
    "print(\"   Full execution visible in MLflow UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Strengths\n",
    "\n",
    "- ‚úÖ **Abstractions**: Reusable components\n",
    "- ‚úÖ **Chains**: Complex multi-step workflows\n",
    "- ‚úÖ **Agents**: Built-in agent patterns\n",
    "- ‚úÖ **Tools**: Easy tool integration\n",
    "- ‚úÖ **Community**: Large ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: LlamaIndex Framework\n",
    "\n",
    "LlamaIndex specializes in document indexing and retrieval-augmented generation (RAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LlamaIndex using default OpenAI configuration\n",
      "‚úÖ LlamaIndex autologging enabled\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.core import VectorStoreIndex, Document, Settings\n",
    "from llama_index.llms.openai import OpenAI as LlamaIndexOpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# Enable LlamaIndex autologging\n",
    "mlflow.llama_index.autolog()\n",
    "\n",
    "# Configure LlamaIndex Settings based on provider\n",
    "# LlamaIndex defaults to OpenAI, so we need to explicitly configure it for Databricks AI Gateway\n",
    "if use_databricks_ai_gateway:\n",
    "    # Get Databricks AI Gateway credentials\n",
    "    databricks_token = os.environ.get(\"DATABRICKS_TOKEN\")\n",
    "    ai_gateway_base_url = os.environ.get(\"AI_GATEWAY_BASE_URL\")\n",
    "    \n",
    "    # Configure LLM for Databricks AI Gateway (OpenAI-compatible endpoint)\n",
    "    Settings.llm = LlamaIndexOpenAI(\n",
    "        model=os.environ.get(\"AI_GATEWAY_LLM_MODEL\", \"jsd-gpt-5-2\"),\n",
    "        api_key=databricks_token,\n",
    "        api_base=ai_gateway_base_url\n",
    "    )\n",
    "    \n",
    "    # Configure embedding model for Databricks AI Gateway\n",
    "    # Use model_name (not model) to bypass OpenAIEmbeddingModelType enum validation\n",
    "    Settings.embed_model = OpenAIEmbedding(\n",
    "        model_name=os.environ.get(\"AI_GATEWAY_EMBED_MODEL\", \"jsd-text-embedding-3-small\"),\n",
    "        api_key=databricks_token,\n",
    "        api_base=ai_gateway_base_url\n",
    "    )\n",
    "    print(\"‚úÖ LlamaIndex configured for Databricks AI Gateway\")\n",
    "    print(f\"   LLM: {Settings.llm.model}\")\n",
    "    print(f\"   Embeddings: {Settings.embed_model.model_name}\")\n",
    "else:\n",
    "    # Use default OpenAI settings (requires OPENAI_API_KEY)\n",
    "    print(\"‚úÖ LlamaIndex using default OpenAI configuration\")\n",
    "\n",
    "print(\"‚úÖ LlamaIndex autologging enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìö LlamaIndex Example: Document Q&A\n",
      "\n",
      "Query: What tracing capabilities does MLflow have?\n",
      "\n",
      "Answer: MLflow Tracing captures the complete execution of GenAI applications, including LLM calls, retrieval steps, and tool usage.\n",
      "\n",
      "‚úÖ LlamaIndex execution fully traced!\n",
      "   - Document indexing\n",
      "   - Query embedding\n",
      "   - Retrieval\n",
      "   - Response synthesis\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-c6e8caaf68470d96d1b736e824387f44&amp;experiment_id=9&amp;trace_id=tr-0d76084e1b37c2702ea012d07db23de5&amp;experiment_id=9&amp;trace_id=tr-62b9817b72948ac3ccca14e9a2c7a67c&amp;experiment_id=9&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-c6e8caaf68470d96d1b736e824387f44), Trace(trace_id=tr-0d76084e1b37c2702ea012d07db23de5), Trace(trace_id=tr-62b9817b72948ac3ccca14e9a2c7a67c)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create sample documents\n",
    "print(\"\\nüìö LlamaIndex Example: Document Q&A\\n\")\n",
    "\n",
    "documents = [\n",
    "    Document(text=\"MLflow is an open source platform for the complete ML lifecycle. It provides experiment tracking, model registry, and deployment capabilities.\"),\n",
    "    Document(text=\"MLflow Tracing captures the complete execution of GenAI applications, including LLM calls, retrieval steps, and tool usage.\"),\n",
    "    Document(text=\"MLflow integrates with 30+ frameworks including OpenAI, LangChain, LlamaIndex, and more.\"),\n",
    "    Document(text=\"MLflow supports collaborative development with experiment sharing, prompt management, and model versioning.\"),\n",
    "]\n",
    "\n",
    "# Create index (automatically traced)\n",
    "# Note: This uses the LLM and embedding model configured in Settings above\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Create query engine\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Query the index (automatically traced)\n",
    "response = query_engine.query(\"What tracing capabilities does MLflow have?\")\n",
    "\n",
    "print(\"Query: What tracing capabilities does MLflow have?\")\n",
    "print(f\"\\nAnswer: {response}\")\n",
    "\n",
    "print(\"\\n‚úÖ LlamaIndex execution fully traced!\")\n",
    "print(\"   - Document indexing\")\n",
    "print(\"   - Query embedding\")\n",
    "print(\"   - Retrieval\")\n",
    "print(\"   - Response synthesis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Best Practices\n",
    "\n",
    "### OpenAI Best Practices\n",
    "\n",
    "```python\n",
    "# DO:\n",
    "‚úÖ Use structured prompts\n",
    "‚úÖ Implement retry logic\n",
    "‚úÖ Handle rate limits\n",
    "‚úÖ Stream responses for UX\n",
    "‚úÖ Cache results when possible\n",
    "\n",
    "# DON'T:\n",
    "‚ùå Hardcode prompts\n",
    "‚ùå Ignore error responses\n",
    "‚ùå Skip cost tracking\n",
    "‚ùå Use synchronous calls in production\n",
    "```\n",
    "\n",
    "### LangChain Best Practices\n",
    "\n",
    "```python\n",
    "# DO:\n",
    "‚úÖ Use LCEL for chains\n",
    "‚úÖ Leverage built-in components\n",
    "‚úÖ Test chains independently\n",
    "‚úÖ Use async for better performance\n",
    "‚úÖ Enable debug mode during development\n",
    "\n",
    "# DON'T:\n",
    "‚ùå Over-abstract simple use cases\n",
    "‚ùå Ignore performance overhead\n",
    "‚ùå Skip error handling in chains\n",
    "‚ùå Use deprecated components\n",
    "```\n",
    "\n",
    "### LlamaIndex Best Practices\n",
    "\n",
    "```python\n",
    "# DO:\n",
    "‚úÖ Choose appropriate index type for your use case\n",
    "‚úÖ Chunk documents thoughtfully\n",
    "‚úÖ Use metadata for filtering\n",
    "‚úÖ Cache embeddings when possible\n",
    "‚úÖ Monitor retrieval quality\n",
    "\n",
    "# DON'T:\n",
    "‚ùå Index without preprocessing\n",
    "‚ùå Use default settings blindly\n",
    "‚ùå Ignore index refresh strategies\n",
    "‚ùå Skip evaluation of retrievals\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. ‚úÖ Overview of MLflow's 30+ framework integrations\n",
    "2. ‚úÖ Working with OpenAI (direct API)\n",
    "3. ‚úÖ Working with LangChain (chains and workflows)\n",
    "4. ‚úÖ Working with LlamaIndex (document indexing and RAG)\n",
    "5. ‚úÖ Best practices for each framework\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **All frameworks** are automatically traced by MLflow\n",
    "- **Choose based on use case**, not hype\n",
    "- **OpenAI** for simplicity and performance\n",
    "- **LangChain** for complex workflows and agents\n",
    "- **LlamaIndex** for document-heavy applications\n",
    "- **Mix frameworks** when it makes sense\n",
    "\n",
    "### Framework Comparison Summary\n",
    "\n",
    "| Aspect | OpenAI | LangChain | LlamaIndex |\n",
    "|--------|--------|-----------|------------|\n",
    "| **Best For** | Simple apps | Workflows | Doc Q&A |\n",
    "| **Performance** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **Ease of Use** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n",
    "| **Flexibility** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n",
    "| **RAG Support** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **Agents** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**üìì Notebook 1.7: Evaluating Agents**\n",
    "\n",
    "Learn how to evaluate your GenAI applications:\n",
    "- LLM-as-Judge evaluation patterns\n",
    "- MLflow built-in scorers\n",
    "- Custom scorers with @scorer decorator\n",
    "- DeepEval integration\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [MLflow LangChain Integration](https://mlflow.org/docs/latest/llms/langchain/index.html)\n",
    "- [MLflow LlamaIndex Integration](https://mlflow.org/docs/latest/llms/llama-index/index.html)\n",
    "- [Framework Examples](https://github.com/mlflow/mlflow/tree/master/examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. ‚úÖ Overview of MLflow's integrations\n",
    "2. ‚úÖ Working with OpenAI (direct API)\n",
    "3. ‚úÖ Working with LangChain (chains and workflows)\n",
    "4. ‚úÖ Best practices for each framework\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **All frameworks** are automatically traced by MLflow\n",
    "- **Choose based on use case**, not hype\n",
    "- **OpenAI** for simplicity and performance\n",
    "- **LangChain** for complex workflows and agents\n",
    "- **Mix frameworks** when it makes sense\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**üìì Notebook 1.7: Evaluating Agents**\n",
    "\n",
    "Learn how to evaluate your GenAI applications:\n",
    "- LLM-as-Judge evaluation patterns\n",
    "- MLflow built-in scorers\n",
    "- Custom scorers with @scorer decorator\n",
    "- DeepEval integration\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [MLflow LangChain Integration](https://mlflow.org/docs/latest/llms/langchain/index.html)\n",
    "- [Framework Examples](https://github.com/mlflow/mlflow/tree/master/examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
