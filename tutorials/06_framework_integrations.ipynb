{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1.6: Framework Integrations\n",
    "\n",
    "## Working with Multiple GenAI Frameworks\n",
    "\n",
    "Welcome to framework integrations! MLflow supports 30+ GenAI frameworks, making it the most flexible platform for LLM development. This notebook shows you how to work with the most popular frameworks.\n",
    "\n",
    "### What You'll Learn\n",
    "- Overview of MLflow's framework integrations\n",
    "- Working with OpenAI (direct API)\n",
    "- Working with LangChain (chains and agents)\n",
    "- Working with LlamaIndex (document indexing and RAG)\n",
    "- Comparing frameworks and choosing the right one\n",
    "- Best practices for each framework\n",
    "\n",
    "### Prerequisites\n",
    "- Completed previous notebooks (1.1-1.5)\n",
    "- OpenAI API key or Databricks AI Gateway configured\n",
    "\n",
    "### Estimated Time: 15-20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Framework Overview\n",
    "\n",
    "### MLflow's 40+ Integrations\n",
    "\n",
    "MLflow provides automatic tracing for:\n",
    "\n",
    "**LLM Providers:**\n",
    "- OpenAI, Anthropic, Cohere, Azure OpenAI\n",
    "- AWS Bedrock, Google Vertex AI\n",
    "- Ollama, vLLM, Together AI\n",
    "\n",
    "**Frameworks:**\n",
    "- LangChain, LlamaIndex, Haystack\n",
    "- DSPy, AutoGen, CrewAI\n",
    "- Guardrails AI, Phoenix\n",
    "\n",
    "### Comparison Matrix\n",
    "\n",
    "| Feature | OpenAI | LangChain | LlamaIndex |\n",
    "|---------|--------|-----------|------------|\n",
    "| **Complexity** | Low | Medium | Medium |\n",
    "| **Learning Curve** | Easy | Medium | Medium |\n",
    "| **Use Case** | Direct calls | Workflows | Doc Q&A |\n",
    "| **Tracing** | ‚úÖ Auto | ‚úÖ Auto | ‚úÖ Auto |\n",
    "| **Agents** | Manual | ‚úÖ Built-in | ‚úÖ Built-in |\n",
    "| **RAG** | Manual | ‚úÖ Built-in | ‚úÖ Built-in |\n",
    "| **Customization** | Full | High | High |\n",
    "| **Performance** | Fast | Medium | Medium |\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "**OpenAI Direct API:**\n",
    "- Simple Q&A applications\n",
    "- Maximum control over prompts\n",
    "- Lowest latency\n",
    "- Custom implementations\n",
    "\n",
    "**LangChain:**\n",
    "- Complex multi-step workflows\n",
    "- Agent applications with tools\n",
    "- Need for abstractions\n",
    "- Rapid prototyping\n",
    "\n",
    "**LlamaIndex:**\n",
    "- Document-heavy applications\n",
    "- Advanced indexing strategies\n",
    "- Multiple data sources\n",
    "- Knowledge management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional frameworks\n",
    "!uv add langchain langchain-openai\n",
    "\n",
    "print(\"‚úÖ Frameworks installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "from utils.clnt_utils import (\n",
    "    is_databricks_client, \n",
    "    is_databricks_ai_gateway_client,\n",
    "    get_databricks_ai_gateway_client,\n",
    "    get_openai_client,\n",
    "    get_ai_gateway_model_names\n",
    ")\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "# Determine provider and initialize client\n",
    "use_databricks_provider = is_databricks_client()\n",
    "use_databricks_ai_gateway = is_databricks_ai_gateway_client()\n",
    "\n",
    "if use_databricks_ai_gateway:\n",
    "    client = get_databricks_ai_gateway_client()\n",
    "    model_name = get_ai_gateway_model_names()[0]\n",
    "    provider_name = \"Databricks AI Gateway\"\n",
    "else:\n",
    "    client = get_openai_client()\n",
    "    model_name = \"gpt-5-mini\"\n",
    "    provider_name = \"OpenAI\"\n",
    "\n",
    "print(f\"‚úÖ Environment configured: using {provider_name} client\")\n",
    "print(f\"   MLflow version: {mlflow.__version__}\")\n",
    "print(f\"   Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"   Model name: {model_name}\")\n",
    "\n",
    "# Enable OpenAI autologging\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "mlflow.set_experiment(\"09-framework-integrations\")\n",
    "\n",
    "print(\"‚úÖ OpenAI autologging: ENABLED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: OpenAI Framework (Already Covered)\n",
    "\n",
    "We've been using OpenAI throughout this tutorial series.\n",
    "\n",
    "### Quick Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Strengths\n",
    "\n",
    "- ‚úÖ **Simplicity**: Direct API calls\n",
    "- ‚úÖ **Performance**: No abstraction overhead\n",
    "- ‚úÖ **Control**: Full control over prompts\n",
    "- ‚úÖ **Latest features**: Immediate access to new models\n",
    "- ‚úÖ **Documentation**: Extensive official docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: LangChain Framework\n",
    "\n",
    "LangChain provides abstractions for building LLM applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Enable LangChain autologging\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "print(\"‚úÖ LangChain autologging enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.clnt_utils import (\n",
    "    get_databricks_langchain_chat_client, \n",
    "    get_langchain_chat_openai_client,\n",
    "    get_databricks_ai_gateway_langchain_client\n",
    ")\n",
    "\n",
    "# Simple LangChain chain\n",
    "print(\"\\nüîó LangChain Example 1: Simple Chain\\n\")\n",
    "\n",
    "# Create prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a {role}. Answer: {question}\"\n",
    ")\n",
    "\n",
    "# Create LLM based on provider\n",
    "if use_databricks_ai_gateway:\n",
    "    llm = get_databricks_ai_gateway_langchain_client(model_name, temperature=1.0)\n",
    "elif use_databricks_provider:\n",
    "    llm = get_databricks_langchain_chat_client(model_name, temperature=1.0)\n",
    "else:\n",
    "    llm = get_langchain_chat_openai_client(model_name, temperature=1.0)\n",
    "\n",
    "# Create chain using LCEL (LangChain Expression Language)\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run chain (automatically traced!)\n",
    "result = chain.invoke({\n",
    "    \"role\": \"MLflow expert\",\n",
    "    \"question\": \"What makes LangChain different from using OpenAI directly?\"\n",
    "})\n",
    "\n",
    "print(result)\n",
    "print(\"\\n‚úÖ Chain execution fully traced!\")\n",
    "print(\"   - Prompt construction\")\n",
    "print(\"   - LLM call\")\n",
    "print(\"   - Output parsing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # More complex chain with multiple steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex chain with multiple steps\n",
    "print(\"\\nüîó LangChain Example 2: Multi-Step Chain\\n\")\n",
    "\n",
    "\n",
    "# Step 1: Generate topic\n",
    "topic_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate a technical topic about {domain}\"\n",
    ")\n",
    "topic_chain = topic_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Step 2: Create outline\n",
    "outline_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Create a 3-point outline for: {topic}\"\n",
    ")\n",
    "outline_chain = outline_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Execute pipeline\n",
    "topic = topic_chain.invoke({\"domain\": \"MLOps\"})\n",
    "print(f\"Topic: {topic}\\n\")\n",
    "\n",
    "outline = outline_chain.invoke({\"topic\": topic})\n",
    "print(f\"Outline:\\n{outline}\")\n",
    "\n",
    "print(\"\\n‚úÖ Multi-step chain traced!\")\n",
    "print(\"   Each chain creates separate spans\")\n",
    "print(\"   Full execution visible in MLflow UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Strengths\n",
    "\n",
    "- ‚úÖ **Abstractions**: Reusable components\n",
    "- ‚úÖ **Chains**: Complex multi-step workflows\n",
    "- ‚úÖ **Agents**: Built-in agent patterns\n",
    "- ‚úÖ **Tools**: Easy tool integration\n",
    "- ‚úÖ **Community**: Large ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: LlamaIndex Framework\n",
    "\n",
    "LlamaIndex specializes in document indexing and retrieval-augmented generation (RAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import VectorStoreIndex, Document, Settings\n",
    "from llama_index.llms.openai import OpenAI as LlamaIndexOpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# Enable LlamaIndex autologging\n",
    "mlflow.llama_index.autolog()\n",
    "\n",
    "# Configure LlamaIndex Settings based on provider\n",
    "# LlamaIndex defaults to OpenAI, so we need to explicitly configure it for Databricks AI Gateway\n",
    "if use_databricks_ai_gateway:\n",
    "    # Get Databricks AI Gateway credentials\n",
    "    databricks_token = os.environ.get(\"DATABRICKS_TOKEN\")\n",
    "    ai_gateway_base_url = os.environ.get(\"AI_GATEWAY_BASE_URL\")\n",
    "    \n",
    "    # Configure LLM for Databricks AI Gateway (OpenAI-compatible endpoint)\n",
    "    Settings.llm = LlamaIndexOpenAI(\n",
    "        model=os.environ.get(\"AI_GATEWAY_LLM_MODEL\", \"jsd-gpt-5-2\"),\n",
    "        api_key=databricks_token,\n",
    "        api_base=ai_gateway_base_url\n",
    "    )\n",
    "    \n",
    "    # Configure embedding model for Databricks AI Gateway\n",
    "    # Use model_name (not model) to bypass OpenAIEmbeddingModelType enum validation\n",
    "    Settings.embed_model = OpenAIEmbedding(\n",
    "        model_name=os.environ.get(\"AI_GATEWAY_EMBED_MODEL\", \"jsd-text-embedding-3-small\"),\n",
    "        api_key=databricks_token,\n",
    "        api_base=ai_gateway_base_url\n",
    "    )\n",
    "    print(\"‚úÖ LlamaIndex configured for Databricks AI Gateway\")\n",
    "    print(f\"   LLM: {Settings.llm.model}\")\n",
    "    print(f\"   Embeddings: {Settings.embed_model.model_name}\")\n",
    "else:\n",
    "    # Use default OpenAI settings (requires OPENAI_API_KEY)\n",
    "    print(\"‚úÖ LlamaIndex using default OpenAI configuration\")\n",
    "\n",
    "print(\"‚úÖ LlamaIndex autologging enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents\n",
    "print(\"\\nüìö LlamaIndex Example: Document Q&A\\n\")\n",
    "\n",
    "documents = [\n",
    "    Document(text=\"MLflow is an open source platform for the complete ML lifecycle. It provides experiment tracking, model registry, and deployment capabilities.\"),\n",
    "    Document(text=\"MLflow Tracing captures the complete execution of GenAI applications, including LLM calls, retrieval steps, and tool usage.\"),\n",
    "    Document(text=\"MLflow integrates with 30+ frameworks including OpenAI, LangChain, LlamaIndex, and more.\"),\n",
    "    Document(text=\"MLflow supports collaborative development with experiment sharing, prompt management, and model versioning.\"),\n",
    "]\n",
    "\n",
    "# Create index (automatically traced)\n",
    "# Note: This uses the LLM and embedding model configured in Settings above\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Create query engine\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Query the index (automatically traced)\n",
    "response = query_engine.query(\"What tracing capabilities does MLflow have?\")\n",
    "\n",
    "print(f\"Query: What tracing capabilities does MLflow have?\")\n",
    "print(f\"\\nAnswer: {response}\")\n",
    "\n",
    "print(\"\\n‚úÖ LlamaIndex execution fully traced!\")\n",
    "print(\"   - Document indexing\")\n",
    "print(\"   - Query embedding\")\n",
    "print(\"   - Retrieval\")\n",
    "print(\"   - Response synthesis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Best Practices\n",
    "\n",
    "### OpenAI Best Practices\n",
    "\n",
    "```python\n",
    "# DO:\n",
    "‚úÖ Use structured prompts\n",
    "‚úÖ Implement retry logic\n",
    "‚úÖ Handle rate limits\n",
    "‚úÖ Stream responses for UX\n",
    "‚úÖ Cache results when possible\n",
    "\n",
    "# DON'T:\n",
    "‚ùå Hardcode prompts\n",
    "‚ùå Ignore error responses\n",
    "‚ùå Skip cost tracking\n",
    "‚ùå Use synchronous calls in production\n",
    "```\n",
    "\n",
    "### LangChain Best Practices\n",
    "\n",
    "```python\n",
    "# DO:\n",
    "‚úÖ Use LCEL for chains\n",
    "‚úÖ Leverage built-in components\n",
    "‚úÖ Test chains independently\n",
    "‚úÖ Use async for better performance\n",
    "‚úÖ Enable debug mode during development\n",
    "\n",
    "# DON'T:\n",
    "‚ùå Over-abstract simple use cases\n",
    "‚ùå Ignore performance overhead\n",
    "‚ùå Skip error handling in chains\n",
    "‚ùå Use deprecated components\n",
    "```\n",
    "\n",
    "### LlamaIndex Best Practices\n",
    "\n",
    "```python\n",
    "# DO:\n",
    "‚úÖ Choose appropriate index type for your use case\n",
    "‚úÖ Chunk documents thoughtfully\n",
    "‚úÖ Use metadata for filtering\n",
    "‚úÖ Cache embeddings when possible\n",
    "‚úÖ Monitor retrieval quality\n",
    "\n",
    "# DON'T:\n",
    "‚ùå Index without preprocessing\n",
    "‚ùå Use default settings blindly\n",
    "‚ùå Ignore index refresh strategies\n",
    "‚ùå Skip evaluation of retrievals\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. ‚úÖ Overview of MLflow's 30+ framework integrations\n",
    "2. ‚úÖ Working with OpenAI (direct API)\n",
    "3. ‚úÖ Working with LangChain (chains and workflows)\n",
    "4. ‚úÖ Working with LlamaIndex (document indexing and RAG)\n",
    "5. ‚úÖ Best practices for each framework\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **All frameworks** are automatically traced by MLflow\n",
    "- **Choose based on use case**, not hype\n",
    "- **OpenAI** for simplicity and performance\n",
    "- **LangChain** for complex workflows and agents\n",
    "- **LlamaIndex** for document-heavy applications\n",
    "- **Mix frameworks** when it makes sense\n",
    "\n",
    "### Framework Comparison Summary\n",
    "\n",
    "| Aspect | OpenAI | LangChain | LlamaIndex |\n",
    "|--------|--------|-----------|------------|\n",
    "| **Best For** | Simple apps | Workflows | Doc Q&A |\n",
    "| **Performance** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **Ease of Use** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n",
    "| **Flexibility** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n",
    "| **RAG Support** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **Agents** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**üìì Notebook 1.7: Evaluating Agents**\n",
    "\n",
    "Learn how to evaluate your GenAI applications:\n",
    "- LLM-as-Judge evaluation patterns\n",
    "- MLflow built-in scorers\n",
    "- Custom scorers with @scorer decorator\n",
    "- DeepEval integration\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [MLflow LangChain Integration](https://mlflow.org/docs/latest/llms/langchain/index.html)\n",
    "- [MLflow LlamaIndex Integration](https://mlflow.org/docs/latest/llms/llama-index/index.html)\n",
    "- [Framework Examples](https://github.com/mlflow/mlflow/tree/master/examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. ‚úÖ Overview of MLflow's integrations\n",
    "2. ‚úÖ Working with OpenAI (direct API)\n",
    "3. ‚úÖ Working with LangChain (chains and workflows)\n",
    "4. ‚úÖ Best practices for each framework\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **All frameworks** are automatically traced by MLflow\n",
    "- **Choose based on use case**, not hype\n",
    "- **OpenAI** for simplicity and performance\n",
    "- **LangChain** for complex workflows and agents\n",
    "- **Mix frameworks** when it makes sense\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**üìì Notebook 1.7: Evaluating Agents**\n",
    "\n",
    "Learn how to evaluate your GenAI applications:\n",
    "- LLM-as-Judge evaluation patterns\n",
    "- MLflow built-in scorers\n",
    "- Custom scorers with @scorer decorator\n",
    "- DeepEval integration\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [MLflow LangChain Integration](https://mlflow.org/docs/latest/llms/langchain/index.html)\n",
    "- [Framework Examples](https://github.com/mlflow/mlflow/tree/master/examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
