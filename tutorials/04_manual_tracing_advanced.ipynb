{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1.4: Manual Tracing and Advanced Observability\n",
    "\n",
    "## Custom Instrumentation for Complex Workflows\n",
    "\n",
    "Welcome to advanced tracing! While autologging is powerful, real-world applications often need custom instrumentation. You'll learn how to trace your own functions and build hierarchical observability into complex GenAI workflows.\n",
    "\n",
    "### What You'll Learn\n",
    "- When to use manual tracing vs. autologging\n",
    "- The `@mlflow.trace` decorator\n",
    "- Creating custom spans with proper types\n",
    "- Adding custom attributes to spans\n",
    "- Building hierarchical traces (parent-child relationships)\n",
    "- Tracing RAG pipelines end-to-end\n",
    "- Tracing agentic workflows with tool usage\n",
    "- Advanced debugging techniques\n",
    "\n",
    "### Prerequisites\n",
    "- Completed Notebook 1.3 (Introduction to Tracing)\n",
    "- Understanding of autologging\n",
    "- MLflow UI running\n",
    "\n",
    "### Estimated Time: 30-35 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: When to Use Manual Tracing\n",
    "\n",
    "### Autologging is Great For:\n",
    "- ‚úÖ LLM API calls (OpenAI, Anthropic, etc.)\n",
    "- ‚úÖ Framework chains (LangChain, LlamaIndex)\n",
    "- ‚úÖ Quick prototyping\n",
    "- ‚úÖ Standard workflows\n",
    "\n",
    "### Manual Tracing is Needed For:\n",
    "- ‚úÖ **Custom functions** in your pipeline\n",
    "- ‚úÖ **Domain-specific operations** (parsing, validation, business logic)\n",
    "- ‚úÖ **Custom retrievers** or data sources\n",
    "- ‚úÖ **External API calls** not auto-instrumented\n",
    "- ‚úÖ **Adding context** not captured automatically\n",
    "- ‚úÖ **Organizing operations** into logical groups\n",
    "\n",
    "### Best Practice: Combine Both!\n",
    "```python\n",
    "# Use autologging for LLM calls\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Add manual tracing for custom logic\n",
    "@mlflow.trace\n",
    "def my_custom_retriever(query):\n",
    "    # Your custom code\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment configured: using Databricks client\n",
      "   MLflow version: 3.9.0\n",
      "   Tracking URI: http://localhost:5000\n",
      "‚úÖ Environment configured\n",
      "   OpenAI autologging: ENABLED\n",
      "   Ready for manual tracing!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "from utils.clnt_utils import is_databricks_ai_gateway_client, get_databricks_ai_gateway_client, get_openai_client, get_ai_gateway_model_names\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "use_databricks_provider = is_databricks_ai_gateway_client()\n",
    "if use_databricks_provider:\n",
    "    client = get_databricks_ai_gateway_client()\n",
    "    model_name = get_ai_gateway_model_names()[0]\n",
    "else:\n",
    "    # Initialize OpenAI\n",
    "    client = get_openai_client()\n",
    "    model_name = \"gpt-5-2\"\n",
    "print(\"‚úÖ Environment configured: using\", \"Databricks\" if use_databricks_provider else \"OpenAI\", \"client\")\n",
    "print(f\"   MLflow version: {mlflow.__version__}\")\n",
    "print(f\"   Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "# Create experiment\n",
    "mlflow.set_experiment(\"07-manual-tracing\")\n",
    "\n",
    "print(\"‚úÖ Environment configured\")\n",
    "print(\"   OpenAI autologging: ENABLED\")\n",
    "print(\"   Ready for manual tracing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Your First Manual Trace\n",
    "\n",
    "Let's trace a custom function using the `@mlflow.trace` decorator, giving us the control what specific calls to trace as we might\n",
    "have custom logic that we want trace and debug in case of issues. MLflow privdes a decorator `@mlflow.trace`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing manual tracing...\n",
      "\n",
      "Input: '  What is MLflow?  '\n",
      "Output: what is mlflow?\n",
      "\n",
      "‚úÖ Function traced!\n",
      "\n",
      "üìä In MLflow UI, you'll see:\n",
      "   - Span name: preprocess_query\n",
      "   - Input: original query\n",
      "   - Output: processed query\n",
      "   - Duration: ~100ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-68a72e5fdacfc2da2992cc861ce85e94&amp;experiment_id=7&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-68a72e5fdacfc2da2992cc861ce85e94)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple example: Trace a custom function\n",
    "@mlflow.trace\n",
    "def preprocess_query(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess user query before sending to LLM.\n",
    "    This is a custom function that MLflow doesn't auto-trace.\n",
    "    \"\"\"\n",
    "    # Simulate preprocessing\n",
    "    time.sleep(0.1)  # Simulate work\n",
    "    \n",
    "    processed = user_query.strip().lower()\n",
    "    \n",
    "    # This function is now fully traced!\n",
    "    return processed\n",
    "\n",
    "# Test it\n",
    "print(\"\\nüîç Testing manual tracing...\\n\")\n",
    "\n",
    "result = preprocess_query(\"  What is MLflow?  \")\n",
    "\n",
    "print(\"Input: '  What is MLflow?  '\")\n",
    "print(f\"Output: {result}\")\n",
    "print(\"\\n‚úÖ Function traced!\")\n",
    "print(\"\\nüìä In MLflow UI, you'll see:\")\n",
    "print(\"   - Span name: preprocess_query\")\n",
    "print(\"   - Input: original query\")\n",
    "print(\"   - Output: processed query\")\n",
    "print(\"   - Duration: ~100ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ What Just Happened?\n",
    "\n",
    "The `@mlflow.trace` decorator:\n",
    "1. **Captured** function inputs and outputs\n",
    "2. **Measured** execution time\n",
    "3. **Created** a span in the trace\n",
    "4. **Logged** everything to MLflow\n",
    "\n",
    "**No manual logging code needed!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Span Types and Names\n",
    "\n",
    "You can specify span types to categorize operations, and attach names to it by providing arguments to the decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìö Testing typed spans...\n",
      "\n",
      "Embedding: [0.1, 0.2, 0.3]...\n",
      "\n",
      "Retrieved 2 documents\n",
      "  Doc 1: MLflow is an open source platform for the ML and G...\n",
      "\n",
      "‚úÖ Typed spans created!\n",
      "\n",
      "üîç In MLflow UI:\n",
      "   - embed_query: Type = EMBEDDING\n",
      "   - retrieve_documents: Type = RETRIEVER\n",
      "   - Easy to filter by operation type!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-61aa2e80625ae86b4fd7dfa507facae2&amp;experiment_id=7&amp;trace_id=tr-cb529ba7b6710a0f8429475372245e6d&amp;experiment_id=7&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-61aa2e80625ae86b4fd7dfa507facae2), Trace(trace_id=tr-cb529ba7b6710a0f8429475372245e6d)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import List\n",
    "# Specify span type and custom name\n",
    "@mlflow.trace(name=\"document_retriever\", span_type=\"RETRIEVER\")\n",
    "def retrieve_documents(query: str, top_k: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Simulate document retrieval from a vector database.\n",
    "    \"\"\"\n",
    "    time.sleep(0.2)  # Simulate vector search\n",
    "    \n",
    "    # Simulated documents\n",
    "    docs = [\n",
    "        \"MLflow is an open source platform for the ML and GenAI lifecycle and Observability .\",\n",
    "        \"MLflow Tracing provides observability for GenAI applications.\",\n",
    "        \"MLflow supports experiment tracking, model registry, and deployment.\"\n",
    "    ]\n",
    "    \n",
    "    return docs[:top_k]\n",
    "\n",
    "@mlflow.trace(name=\"query_embedder\", span_type=\"EMBEDDING\")\n",
    "def embed_query(query: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Generate embedding for query.\n",
    "    \"\"\"\n",
    "    time.sleep(0.15)  # Simulate embedding generation\n",
    "    \n",
    "    # Simulated embedding\n",
    "    return [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# Test both functions\n",
    "print(\"\\nüìö Testing typed spans...\\n\")\n",
    "\n",
    "query = \"What is MLflow?\"\n",
    "\n",
    "# These will create different span types\n",
    "embedding = embed_query(query)\n",
    "print(f\"Embedding: {embedding[:3]}...\")\n",
    "\n",
    "documents = retrieve_documents(query, top_k=2)\n",
    "print(f\"\\nRetrieved {len(documents)} documents\")\n",
    "print(f\"  Doc 1: {documents[0][:50]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ Typed spans created!\")\n",
    "print(\"\\nüîç In MLflow UI:\")\n",
    "print(\"   - embed_query: Type = EMBEDDING\")\n",
    "print(\"   - retrieve_documents: Type = RETRIEVER\")\n",
    "print(\"   - Easy to filter by operation type!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìã Standard Span Types\n",
    "\n",
    "MLflow defines these standard types:\n",
    "\n",
    "- `CHAIN`: Sequence of operations\n",
    "- `LLM`: Language model call\n",
    "- `RETRIEVER`: Document retrieval\n",
    "- `EMBEDDING`: Text embedding\n",
    "- `TOOL`: Tool/function execution\n",
    "- `AGENT`: Agent reasoning\n",
    "- `PARSER`: Output parsing\n",
    "\n",
    "Use the appropriate type for better organization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Adding Custom Attributes\n",
    "\n",
    "Enrich spans with additional metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Testing custom attributes...\n",
      "\n",
      "Retrieved 2 documents\n",
      "\n",
      "‚úÖ Span created with custom attributes!\n",
      "\n",
      "üîç Custom attributes visible in UI:\n",
      "   - top_k: 2\n",
      "   - query_length: 24\n",
      "   - search_method: vector_similarity\n",
      "   - num_results: 2\n",
      "   - total_docs_in_db: 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-1711e4424769ca83f524e69f3ddb70b0&amp;experiment_id=7&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-1711e4424769ca83f524e69f3ddb70b0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add custom attributes to spans\n",
    "@mlflow.trace(name=\"enhanced_retriever\", span_type=\"RETRIEVER\")\n",
    "def enhanced_retrieve(query: str, top_k: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retriever with rich metadata logging.\n",
    "    \"\"\"\n",
    "    span = mlflow.get_current_active_span()\n",
    "    # set attributes\n",
    "    span.set_attributes({\"top_k\":top_k,\n",
    "                        \"query_length\":len(query),\n",
    "                        \"search_method\":\"vector_similarity\"})\n",
    "\n",
    "    # Simulate retrieval\n",
    "    time.sleep(0.2)\n",
    "    docs = [\n",
    "        \"MLflow is an open source platform.\",\n",
    "        \"MLflow provides tracing capabilities.\",\n",
    "        \"MLflow supports multiple frameworks.\"\n",
    "    ]\n",
    "    \n",
    "    # set attributes\n",
    "    span.set_attributes({\"num_results\": len(docs[:top_k]),\n",
    "                        \"total_docs_in_db\": 1000})  # Simulated\n",
    "    \n",
    "    return docs[:top_k]\n",
    "\n",
    "# Test it\n",
    "print(\"\\nüìä Testing custom attributes...\\n\")\n",
    "\n",
    "docs = enhanced_retrieve(\"What is MLflow tracing?\", top_k=2)\n",
    "\n",
    "print(f\"Retrieved {len(docs)} documents\")\n",
    "print(\"\\n‚úÖ Span created with custom attributes!\")\n",
    "print(\"\\nüîç Custom attributes visible in UI:\")\n",
    "print(\"   - top_k: 2\")\n",
    "print(\"   - query_length: 24\")\n",
    "print(\"   - search_method: vector_similarity\")\n",
    "print(\"   - num_results: 2\")\n",
    "print(\"   - total_docs_in_db: 1000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° When to Add Custom Attributes\n",
    "\n",
    "Add attributes for:\n",
    "- **Configuration** (top_k, threshold, model_name)\n",
    "- **Performance metrics** (num_results, cache_hit)\n",
    "- **Data characteristics** (query_length, doc_size)\n",
    "- **Business logic** (user_tier, feature_flags)\n",
    "- **Debugging info** (data_source, version)\n",
    "\n",
    "These make traces **searchable and analyzable**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Building Hierarchical Traces\n",
    "\n",
    "Create parent-child relationships between spans, using a CHAIN span type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå≥ Testing hierarchical tracing...\n",
      "\n",
      "Retrieved 2 documents:\n",
      "  1. MLflow is an open source platform.... (score: 0.95)\n",
      "  2. MLflow provides tracing.... (score: 0.89)\n",
      "\n",
      "‚úÖ Hierarchical trace created!\n",
      "\n",
      "üå≥ Trace hierarchy:\n",
      "\n",
      "   rag_retrieval_pipeline (CHAIN)\n",
      "   ‚îú‚îÄ‚îÄ parse_query (PARSER)\n",
      "   ‚îú‚îÄ‚îÄ embed_text (EMBEDDING)\n",
      "   ‚îî‚îÄ‚îÄ vector_search (RETRIEVER)\n",
      "\n",
      "View the timeline in MLflow UI to see the hierarchy!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-aac05ce4ea849093d3fb49718889f9e9&amp;experiment_id=7&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-aac05ce4ea849093d3fb49718889f9e9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "# Build a hierarchical trace\n",
    "@mlflow.trace(name=\"parse_query\", span_type=\"PARSER\")\n",
    "def parse_query(query: str) -> Dict:\n",
    "    \"\"\"Extract intent and entities from query.\"\"\"\n",
    "    time.sleep(0.05)\n",
    "    return {\n",
    "        \"intent\": \"question\",\n",
    "        \"entities\": [\"MLflow\"],\n",
    "        \"cleaned_query\": query.strip()\n",
    "    }\n",
    "\n",
    "@mlflow.trace(name=\"embed_text\", span_type=\"EMBEDDING\")\n",
    "def embed_text(text: str) -> List[float]:\n",
    "    \"\"\"Generate embedding.\"\"\"\n",
    "    time.sleep(0.1)\n",
    "    return [0.1] * 768  # Simulated embedding\n",
    "\n",
    "@mlflow.trace(name=\"vector_search\", span_type=\"RETRIEVER\")\n",
    "def vector_search(embedding: List[float], top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"Search vector database.\"\"\"\n",
    "    span = mlflow.get_current_active_span()\n",
    "    span.set_attributes({\"embedding_dim\": len(embedding),\n",
    "                         \"top_k\":top_k})\n",
    "    \n",
    "    time.sleep(0.15)\n",
    "    return [\n",
    "        {\"text\": \"MLflow is an open source platform.\", \"score\": 0.95},\n",
    "        {\"text\": \"MLflow provides tracing.\", \"score\": 0.89},\n",
    "        {\"text\": \"MLflow supports deployments.\", \"score\": 0.82}\n",
    "    ][:top_k]\n",
    "\n",
    "# Test the hierarchical pipeline with a CHAIN span type\n",
    "@mlflow.trace(name=\"rag_retrieval_pipeline\", span_type=\"CHAIN\")\n",
    "def rag_retrieval_pipeline(user_query: str, top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Complete retrieval pipeline with multiple steps.\n",
    "    This parent span will contain all child spans!\n",
    "    \"\"\"\n",
    "    span = mlflow.get_current_active_span()\n",
    "    span.set_attributes({\"pipeline_version\":\"v1.0\",\n",
    "                         \"top_k\":top_k})\n",
    "    \n",
    "    # Step 1: Parse query (child span)\n",
    "    parsed = parse_query(user_query)\n",
    "    \n",
    "    # Step 2: Embed query (child span)\n",
    "    embedding = embed_text(parsed[\"cleaned_query\"])\n",
    "    \n",
    "    # Step 3: Vector search (child span)\n",
    "    results = vector_search(embedding, top_k=top_k)\n",
    "    span.set_attributes({\"num_results\": len(results)})\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the hierarchical pipeline\n",
    "print(\"\\nüå≥ Testing hierarchical tracing...\\n\")\n",
    "\n",
    "results = rag_retrieval_pipeline(\"What is MLflow?\", top_k=2)\n",
    "\n",
    "print(f\"Retrieved {len(results)} documents:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"  {i}. {result['text'][:40]}... (score: {result['score']})\")\n",
    "\n",
    "print(\"\\n‚úÖ Hierarchical trace created!\")\n",
    "print(\"\\nüå≥ Trace hierarchy:\")\n",
    "print(\"\"\"\n",
    "   rag_retrieval_pipeline (CHAIN)\n",
    "   ‚îú‚îÄ‚îÄ parse_query (PARSER)\n",
    "   ‚îú‚îÄ‚îÄ embed_text (EMBEDDING)\n",
    "   ‚îî‚îÄ‚îÄ vector_search (RETRIEVER)\n",
    "\"\"\")\n",
    "print(\"View the timeline in MLflow UI to see the hierarchy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üå≥ Hierarchical Tracing Benefits\n",
    "\n",
    "**Advantages:**\n",
    "- **Logical organization** of complex workflows\n",
    "- **Performance analysis** per operation\n",
    "- **Easy debugging** - identify which step failed\n",
    "- **Clear dependencies** between operations\n",
    "- **Aggregate metrics** at parent level\n",
    "\n",
    "**The parent span automatically:**\n",
    "- Contains all child spans\n",
    "- Calculates total duration\n",
    "- Preserves execution order\n",
    "- Shows the call stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Complete RAG Pipeline with Tracing\n",
    "\n",
    "Let's build a full RAG pipeline combining manual tracing and autologging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Testing complete RAG system with full tracing...\n",
      "\n",
      "Query: What tracing capabilities does MLflow provide?\n",
      "\n",
      "Context docs used: 3\n",
      "\n",
      "Answer:\n",
      "MLflow provides **MLflow Tracing**, which captures **LLM execution traces with detailed spans**, and it works across integrations like **OpenAI, LangChain, and 30+ other frameworks**.\n",
      "\n",
      "============================================================\n",
      "‚úÖ RAG System Complete with Full Observability!\n",
      "============================================================\n",
      "\n",
      "\n",
      "üîç Trace Hierarchy in MLflow UI:\n",
      "\n",
      "rag_qa_system (CHAIN)\n",
      "‚îú‚îÄ‚îÄ context_retrieval (RETRIEVER)\n",
      "‚îú‚îÄ‚îÄ format_prompt (PARSER)\n",
      "‚îî‚îÄ‚îÄ generate_answer (LLM)\n",
      "    ‚îî‚îÄ‚îÄ OpenAI API call (auto-traced)\n",
      "\n",
      "Each span includes:\n",
      "‚úì Inputs and outputs\n",
      "‚úì Duration and timing\n",
      "‚úì Custom attributes\n",
      "‚úì Error information (if any)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-f88b499d7d60fb95f9370748d96a5aa9&amp;experiment_id=7&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-f88b499d7d60fb95f9370748d96a5aa9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Complete RAG implementation with comprehensive tracing\n",
    "\n",
    "@mlflow.trace(name=\"context_retrieval\", span_type=\"RETRIEVER\")\n",
    "def retrieve_context(query: str, top_k: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieve relevant context documents.\n",
    "    \"\"\"\n",
    "    span = mlflow.get_current_active_span()\n",
    "    span.set_attributes({\"retrieval_method\":\"semantic_search\",\n",
    "                         \"top_k\":top_k})\n",
    "\n",
    "    \n",
    "    # Simulate retrieval\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "    # fake docs retrieved\n",
    "    docs = [\n",
    "        \"MLflow is an open source platform for managing ML and GenAI lifecycle.\",\n",
    "        \"MLflow Tracing captures LLM execution with detailed spans.\",\n",
    "        \"MLflow integrates with OpenAI, LangChain, and 30+ frameworks.\",\n",
    "        \"MLflow provides experiment tracking and model registry.\",\n",
    "        \"MLflow supports deployment to various serving platforms.\"\n",
    "    ]\n",
    "    \n",
    "    retrieved = docs[:top_k]\n",
    "    span.set_attributes({\"num_docs_retrieved\": len(retrieved)})\n",
    "    \n",
    "    return retrieved\n",
    "\n",
    "@mlflow.trace(name=\"format_prompt\", span_type=\"PARSER\")\n",
    "def format_rag_prompt(query: str, context_docs: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Format RAG prompt with context and query.\n",
    "    \"\"\"\n",
    "    span = mlflow.get_current_active_span()\n",
    "    span.set_attributes({\"num_context_docs\": len(context_docs)})\n",
    "    \n",
    "    context = \"\\n\".join([f\"- {doc}\" for doc in context_docs])\n",
    "    \n",
    "    prompt = f\"\"\"Use the following context to answer the question:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer concisely based on the context above:\"\"\"\n",
    "    span.set_attributes({\"prompt_length\": len(prompt)})\n",
    "    span.set_attributes({\"prompt\": prompt})\n",
    "    span.set_attributes({\"query\": query})\n",
    "    span.set_attributes({\"context\": context})\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "@mlflow.trace(name=\"generate_answer\", span_type=\"LLM\")\n",
    "def generate_answer(prompt: str, model: str = \"gpt-5-mini\") -> str:\n",
    "    \"\"\"\n",
    "    Generate answer using LLM.\n",
    "    OpenAI call will be auto-traced!\n",
    "    \"\"\"\n",
    "    span = mlflow.get_current_active_span()\n",
    "    span.set_attributes({\"model\": model})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=1.0,\n",
    "        max_tokens=2000\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    span.set_attributes({\"answer_length\": len(answer),\n",
    "                         \"tokens_used\": response.usage.total_tokens})\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Complete RAG implementation with comprehensive tracing\n",
    "# The entire pipeline is traced manually\n",
    "@mlflow.trace(name=\"rag_qa_system\", span_type=\"CHAIN\")\n",
    "def rag_qa_system(user_query: str, top_k: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Complete RAG question-answering system.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with answer, context, and metadata\n",
    "    \"\"\"\n",
    "    span = mlflow.get_current_active_span()\n",
    "    span.set_attributes({\"system_version\": \"v1.0\",\n",
    "                         \"user_query\": user_query})\n",
    "    \n",
    "    # Step 1: Retrieve context (manually traced)\n",
    "    context_docs = retrieve_context(user_query, top_k=top_k)\n",
    "    \n",
    "    # Step 2: Format prompt (manually traced)\n",
    "    prompt = format_rag_prompt(user_query, context_docs)\n",
    "    \n",
    "    # Step 3: Generate answer (manually + auto traced)\n",
    "    answer = generate_answer(prompt, model_name)\n",
    "    \n",
    "    result = {\n",
    "        \"query\": user_query,\n",
    "        \"answer\": answer,\n",
    "        \"context_docs\": context_docs,\n",
    "        \"num_docs_used\": len(context_docs)\n",
    "    }\n",
    "    span.set_attributes({\"answer_generated\": True})\n",
    "    \n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test the complete RAG system\n",
    "print(\"\\nü§ñ Testing complete RAG system with full tracing...\\n\")\n",
    "\n",
    "result = rag_qa_system(\"What tracing capabilities does MLflow provide?\", top_k=3)\n",
    "\n",
    "print(f\"Query: {result['query']}\")\n",
    "print(f\"\\nContext docs used: {result['num_docs_used']}\")\n",
    "print(f\"\\nAnswer:\\n{result['answer']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ RAG System Complete with Full Observability!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "\\nüîç Trace Hierarchy in MLflow UI:\n",
    "\n",
    "rag_qa_system (CHAIN)\n",
    "‚îú‚îÄ‚îÄ context_retrieval (RETRIEVER)\n",
    "‚îú‚îÄ‚îÄ format_prompt (PARSER)\n",
    "‚îî‚îÄ‚îÄ generate_answer (LLM)\n",
    "    ‚îî‚îÄ‚îÄ OpenAI API call (auto-traced)\n",
    "\n",
    "Each span includes:\n",
    "‚úì Inputs and outputs\n",
    "‚úì Duration and timing\n",
    "‚úì Custom attributes\n",
    "‚úì Error information (if any)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ RAG Pipeline Analysis\n",
    "\n",
    "With this tracing setup, you can:\n",
    "\n",
    "1. **Identify bottlenecks**: Which step takes longest?\n",
    "2. **Debug failures**: Which step failed and why?\n",
    "3. **Optimize retrieval**: Is top_k=3 optimal?\n",
    "4. **Track costs**: How many tokens per query?\n",
    "5. **Monitor quality**: Are retrieved docs relevant?\n",
    "\n",
    "**All visible in the MLflow UI timeline view!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Tracing Agentic Workflows\n",
    "\n",
    "Let's trace a simple agent with tool usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Testing agentic workflow with tool tracing...\n",
      "\n",
      "returned_plan: {'tool': 'get_weather', 'params': {'city': 'San Francisco'}}\n",
      "Query: What's the weather in San Francisco?\n",
      "Answer: In San Francisco, it‚Äôs **72¬∞F** and **sunny**, with **45% humidity**.\n",
      "\n",
      "returned_plan: {'tool': 'calculate', 'params': {'expression': '42*2'}}\n",
      "Query: Calculate 42 times 2\n",
      "Answer: 42 √ó 2 = **84**\n",
      "\n",
      "============================================================\n",
      "‚úÖ Agent Workflow Fully Traced!\n",
      "============================================================\n",
      "\n",
      "\n",
      "üîç Agent Trace Hierarchy:\n",
      "\n",
      "agent_execution (CHAIN) (manula trace)\n",
      "‚îú‚îÄ‚îÄ agent_planning (AGENT) (manula trace)\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ OpenAI API call (auto-traced)\n",
      "‚îú‚îÄ‚îÄ weather_tool or calculator_tool (TOOL) (manula trace)\n",
      "‚îî‚îÄ‚îÄ OpenAI API call for final response (auto-traced)\n",
      "\n",
      "Key insights visible:\n",
      "‚úì Which tool was selected\n",
      "‚úì Tool execution time\n",
      "‚úì Tool parameters and results\n",
      "‚úì Total agent reasoning time\n",
      "‚úì Token usage across all LLM calls\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-0d0aa107d690abd232a66b49e267d46e&amp;experiment_id=7&amp;trace_id=tr-cea93509d6594b27f63c075c07811313&amp;experiment_id=7&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-0d0aa107d690abd232a66b49e267d46e), Trace(trace_id=tr-cea93509d6594b27f63c075c07811313)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Agentic workflow with tool usage\n",
    "import json\n",
    "\n",
    "@mlflow.trace(name=\"weather_tool\", span_type=\"TOOL\")\n",
    "def get_weather(city: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Simulated weather tool.\n",
    "    \"\"\"\n",
    "    span = mlflow.get_current_active_span()\n",
    "    span.set_attributes({\n",
    "        \"tool_name\": \"weather_api\",\n",
    "        \"city\": city,\n",
    "        \"api_version\": \"v2.0\"\n",
    "    })\n",
    "    \n",
    "    time.sleep(0.1)  # Simulate API call\n",
    "    \n",
    "    weather_data = {\n",
    "        \"city\": city,\n",
    "        \"temperature\": \"72¬∞F\",\n",
    "        \"condition\": \"Sunny\",\n",
    "        \"humidity\": \"45%\"\n",
    "    }\n",
    "    \n",
    "    span.set_attributes({   \n",
    "        \"data_retrieved\": True\n",
    "    })\n",
    "    \n",
    "    return weather_data\n",
    "\n",
    "# Another tool to use for the agent\n",
    "@mlflow.trace(name=\"calculator_tool\", span_type=\"TOOL\")\n",
    "def calculate(expression: str) -> float:\n",
    "    \"\"\"\n",
    "    Simulated calculator tool.\n",
    "    \"\"\"\n",
    "    span = mlflow.get_current_active_span()\n",
    "    span.set_attributes({\n",
    "        \"tool_name\": \"calculator\",\n",
    "        \"expression\": expression\n",
    "    })\n",
    "    \n",
    "    time.sleep(0.05)\n",
    "    \n",
    "    # UNSAFE: Don't use eval() in production!\n",
    "    result = eval(expression)\n",
    "    \n",
    "    span.set_attributes({\n",
    "        \"result\": result\n",
    "    })\n",
    "    \n",
    "    return result\n",
    "\n",
    "# The agent planning step in decideing which tool to use\n",
    "# based on the user query\n",
    "@mlflow.trace(name=\"agent_planning\", span_type=\"AGENT\")\n",
    "def agent_plan(user_query: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Agent decides which tool to use.\n",
    "    \"\"\"\n",
    "    span = mlflow.get_current_active_span()\n",
    "    span.set_attributes({\n",
    "        \"user_query\": user_query\n",
    "    })\n",
    "    \n",
    "    # Use LLM to plan (auto-traced)\n",
    "    system_prompt = \"\"\"You are an agent with access to:\n",
    "1. get_weather(city) - Get weather information\n",
    "2. calculate(expression) - Evaluate math expressions\n",
    "\n",
    "Decide which tool to use and extract parameters.\n",
    "Respond with JSON: {\"tool\": \"tool_name\", \"params\": {...}}\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_query}\n",
    "        ],\n",
    "        temperature=1.0\n",
    "    )\n",
    "\n",
    "    # Parse the response\n",
    "    returned_plan = json.loads(response.choices[0].message.content)\n",
    "    span.set_attributes({\n",
    "        \"returned_plan\": returned_plan\n",
    "    }) \n",
    " \n",
    "    print(f\"returned_plan: {returned_plan}\")\n",
    "    \n",
    "    # Simulate parsing (simplified)\n",
    "    if \"weather\" in user_query.lower():\n",
    "        plan = {\"tool\": \"weather\", \"params\": {\"city\": \"San Francisco\"}}\n",
    "    else:\n",
    "        plan = {\"tool\": \"calculator\", \"params\": {\"expression\": \"42 * 2\"}}\n",
    "    \n",
    "    span.set_attributes({\n",
    "        \"selected_tool\": plan[\"tool\"]\n",
    "    })\n",
    "    \n",
    "    return plan\n",
    "\n",
    "# run the agent in a chain\n",
    "@mlflow.trace(name=\"agent_execution\", span_type=\"CHAIN\")\n",
    "def run_agent(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Complete agent execution with planning and tool use.\n",
    "    \"\"\"\n",
    "    span = mlflow.get_current_active_span()\n",
    "    span.set_attributes({\n",
    "        \"agent_version\": \"v1.0\",\n",
    "        \"user_query\": user_query\n",
    "    })\n",
    "    \n",
    "    # Step 1: Plan (uses LLM)\n",
    "    plan = agent_plan(user_query)\n",
    "    \n",
    "    # Step 2: Execute tool\n",
    "    if plan[\"tool\"] == \"weather\":\n",
    "        tool_result = get_weather(plan[\"params\"][\"city\"])\n",
    "    else:\n",
    "        tool_result = calculate(plan[\"params\"][\"expression\"])\n",
    "    \n",
    "    span.set_attributes({\n",
    "        \"tool_executed\": plan[\"tool\"]\n",
    "    })\n",
    "    \n",
    "    # Step 3: Generate final response (uses LLM)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Tool result: {tool_result}. Answer: {user_query}\"\n",
    "        }],\n",
    "        temperature=1.0\n",
    "    )\n",
    "    \n",
    "    final_answer = response.choices[0].message.content\n",
    "    \n",
    "    span.set_attributes({\n",
    "        \"answer_generated\": True\n",
    "    })\n",
    "    \n",
    "    return final_answer\n",
    "\n",
    "# Test the agent\n",
    "print(\"\\nü§ñ Testing agentic workflow with tool tracing...\\n\")\n",
    "\n",
    "query1 = \"What's the weather in San Francisco?\"\n",
    "answer1 = run_agent(query1)\n",
    "\n",
    "print(f\"Query: {query1}\")\n",
    "print(f\"Answer: {answer1}\\n\")\n",
    "\n",
    "query2 = \"Calculate 42 times 2\"\n",
    "answer2 = run_agent(query2)\n",
    "\n",
    "print(f\"Query: {query2}\")\n",
    "print(f\"Answer: {answer2}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Agent Workflow Fully Traced!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "\\nüîç Agent Trace Hierarchy:\n",
    "\n",
    "agent_execution (CHAIN) (manula trace)\n",
    "‚îú‚îÄ‚îÄ agent_planning (AGENT) (manula trace)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ OpenAI API call (auto-traced)\n",
    "‚îú‚îÄ‚îÄ weather_tool or calculator_tool (TOOL) (manula trace)\n",
    "‚îî‚îÄ‚îÄ OpenAI API call for final response (auto-traced)\n",
    "\n",
    "Key insights visible:\n",
    "‚úì Which tool was selected\n",
    "‚úì Tool execution time\n",
    "‚úì Tool parameters and results\n",
    "‚úì Total agent reasoning time\n",
    "‚úì Token usage across all LLM calls\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñ Agent Tracing Benefits\n",
    "\n",
    "For agentic workflows, tracing reveals:\n",
    "\n",
    "1. **Decision Making**: Which tool was chosen and why?\n",
    "2. **Tool Performance**: How long does each tool take?\n",
    "3. **Error Tracking**: Which tool failed?\n",
    "4. **Cost Analysis**: How many LLM calls per agent run?\n",
    "5. **Optimization**: Can we skip unnecessary steps?\n",
    "\n",
    "**Essential for debugging complex agents!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Advanced Debugging Techniques\n",
    "\n",
    "Let's explore how to use traces for debugging, besides inspection and retrospection.\n",
    "Note that we are using manual tracing, expilicty by using the decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üêõ Testing error tracing...\n",
      "\n",
      "‚úÖ Success case: {'processed': True, 'value': 'test'}\n",
      "\n",
      "Testing failure case...\n",
      "‚ùå Error: Missing required_field\n",
      "\n",
      "üîç Error details captured in trace!\n",
      "\n",
      "\n",
      "üí° Debugging with Traces:\n",
      "\n",
      "1. Filter traces by error status in UI\n",
      "2. View the failed span (marked in red)\n",
      "3. Check custom attributes:\n",
      "   - What were the inputs?\n",
      "   - What validation step failed?\n",
      "   - What was the error type?\n",
      "4. Compare with successful traces\n",
      "5. Reproduce the issue with exact inputs\n",
      "6. Use MLflow Assistant to get insights on the failed traces and root cause\n",
      "\n",
      "Attributes logged BEFORE the error help identify root cause!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-f530682c405447351f5a1e365ab7c555&amp;experiment_id=7&amp;trace_id=tr-8b89f966c02cbbe9821e4a3d123c25fa&amp;experiment_id=7&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-f530682c405447351f5a1e365ab7c555), Trace(trace_id=tr-8b89f966c02cbbe9821e4a3d123c25fa)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Debugging example: Trace a buggy function, which can raise an error\n",
    "\n",
    "@mlflow.trace(name=\"buggy_processor\", span_type=\"PARSER\")\n",
    "def process_with_validation(data: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Fake function with potential errors.\n",
    "    \"\"\"\n",
    "    span = mlflow.get_current_active_span()\n",
    "    span.set_attributes({\n",
    "        \"input_keys\": list(data.keys())\n",
    "    })\n",
    "    \n",
    "    # Log intermediate state for debugging\n",
    "    span.set_attributes({\n",
    "        \"validation_step\": \"checking_required_fields\"\n",
    "    })\n",
    "    \n",
    "    # This might raise an error\n",
    "    if \"required_field\" not in data:\n",
    "        span.set_attributes({\n",
    "            \"error_type\": \"missing_field\"\n",
    "        })\n",
    "        raise ValueError(\"Missing required_field\")\n",
    "    span.set_attributes({\n",
    "        \"validation_step\": \"passed\"\n",
    "    })\n",
    "    \n",
    "    # Process data\n",
    "    result = {\"processed\": True, \"value\": data.get(\"required_field\")}\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"\\nüêõ Testing error tracing...\\n\")\n",
    "\n",
    "# Success case\n",
    "try:\n",
    "    result = process_with_validation({\"required_field\": \"test\"})\n",
    "    print(f\"‚úÖ Success case: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Failure case\n",
    "print(\"\\nTesting failure case...\")\n",
    "try:\n",
    "    result = process_with_validation({\"wrong_field\": \"test\"})\n",
    "    print(f\"‚úÖ Success case: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"\\nüîç Error details captured in trace!\")\n",
    "\n",
    "print(\"\"\"\n",
    "\\nüí° Debugging with Traces:\n",
    "\n",
    "1. Filter traces by error status in UI\n",
    "2. View the failed span (marked in red)\n",
    "3. Check custom attributes:\n",
    "   - What were the inputs?\n",
    "   - What validation step failed?\n",
    "   - What was the error type?\n",
    "4. Compare with successful traces\n",
    "5. Reproduce the issue with exact inputs\n",
    "6. Use MLflow Assistant to get insights on the failed traces and root cause\n",
    "\n",
    "Attributes logged BEFORE the error help identify root cause!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: Performance Analysis\n",
    "\n",
    "Use traces to identify performance bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
      "‚ïë         Performance Analysis with Traces                     ‚ïë\n",
      "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
      "\n",
      "In the MLflow UI, you can:\n",
      "\n",
      "1. üìä TIMELINE VIEW:\n",
      "   - See which operations take the most time\n",
      "   - Identify serial vs parallel operations\n",
      "   - Find bottlenecks visually\n",
      "\n",
      "2. üìà AGGREGATE METRICS:\n",
      "   - Average latency per span type\n",
      "   - P50, P95, P99 latencies\n",
      "   - Success rate per operation\n",
      "\n",
      "3. üîç COMPARISON:\n",
      "   - Compare traces before/after optimization\n",
      "   - A/B test different implementations\n",
      "   - Track performance over time\n",
      "\n",
      "4. üí° OPTIMIZATION STRATEGIES:\n",
      "\n",
      "   If retrieval is slow:\n",
      "   - Check embedding generation time\n",
      "   - Optimize vector search\n",
      "   - Consider caching\n",
      "\n",
      "   If LLM calls are slow:\n",
      "   - Reduce max_tokens\n",
      "   - Use streaming responses\n",
      "   - Try smaller models\n",
      "\n",
      "   If overall latency is high:\n",
      "   - Parallelize independent operations\n",
      "   - Cache frequent queries\n",
      "   - Optimize prompt length\n",
      "\n",
      "5. üìä METRICS TO TRACK:\n",
      "   - End-to-end latency\n",
      "   - Per-operation latency\n",
      "   - Token usage\n",
      "   - Cache hit rate\n",
      "   - Error rate\n",
      "\n",
      "\n",
      "Running performance test with 3 queries...\n",
      "\n",
      "Query 1/3: What is MLflow's GenAI offerin...\n",
      "  Latency: 2.00s\n",
      "\n",
      "Query 2/3: How does tracing work?...\n",
      "  Latency: 1.81s\n",
      "\n",
      "Query 3/3: What are the benefits?...\n",
      "  Latency: 2.28s\n",
      "\n",
      "‚úÖ Performance test complete!\n",
      "\n",
      "üîç Compare these traces in the UI:\n",
      "   - Are latencies consistent?\n",
      "   - Which steps vary the most?\n",
      "   - Any outliers to investigate?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-761396cca83a8308fb8b315a1d4438a3&amp;experiment_id=7&amp;trace_id=tr-a7127daf40f75640e9e83d289822e307&amp;experiment_id=7&amp;trace_id=tr-2ef454b8e778200420b508ba9b8c564a&amp;experiment_id=7&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-761396cca83a8308fb8b315a1d4438a3), Trace(trace_id=tr-a7127daf40f75640e9e83d289822e307), Trace(trace_id=tr-2ef454b8e778200420b508ba9b8c564a)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Performance analysis example\n",
    "print(\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë         Performance Analysis with Traces                     ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "In the MLflow UI, you can:\n",
    "\n",
    "1. üìä TIMELINE VIEW:\n",
    "   - See which operations take the most time\n",
    "   - Identify serial vs parallel operations\n",
    "   - Find bottlenecks visually\n",
    "\n",
    "2. üìà AGGREGATE METRICS:\n",
    "   - Average latency per span type\n",
    "   - P50, P95, P99 latencies\n",
    "   - Success rate per operation\n",
    "\n",
    "3. üîç COMPARISON:\n",
    "   - Compare traces before/after optimization\n",
    "   - A/B test different implementations\n",
    "   - Track performance over time\n",
    "\n",
    "4. üí° OPTIMIZATION STRATEGIES:\n",
    "   \n",
    "   If retrieval is slow:\n",
    "   - Check embedding generation time\n",
    "   - Optimize vector search\n",
    "   - Consider caching\n",
    "   \n",
    "   If LLM calls are slow:\n",
    "   - Reduce max_tokens\n",
    "   - Use streaming responses\n",
    "   - Try smaller models\n",
    "   \n",
    "   If overall latency is high:\n",
    "   - Parallelize independent operations\n",
    "   - Cache frequent queries\n",
    "   - Optimize prompt length\n",
    "\n",
    "5. üìä METRICS TO TRACK:\n",
    "   - End-to-end latency\n",
    "   - Per-operation latency\n",
    "   - Token usage\n",
    "   - Cache hit rate\n",
    "   - Error rate\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# Run a simple performance test\n",
    "print(\"Running performance test with 3 queries...\\n\")\n",
    "\n",
    "test_queries = [\n",
    "    \"What is MLflow's GenAI offering?\",\n",
    "    \"How does tracing work?\",\n",
    "    \"What are the benefits?\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"Query {i}/3: {query[:30]}...\")\n",
    "    start = time.time()\n",
    "    result = rag_qa_system(query, top_k=2)\n",
    "    latency = time.time() - start\n",
    "    print(f\"  Latency: {latency:.2f}s\\n\")\n",
    "\n",
    "print(\"‚úÖ Performance test complete!\")\n",
    "print(\"\\nüîç Compare these traces in the UI:\")\n",
    "print(\"   - Are latencies consistent?\")\n",
    "print(\"   - Which steps vary the most?\")\n",
    "print(\"   - Any outliers to investigate?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. ‚úÖ When to use manual tracing vs. autologging\n",
    "2. ‚úÖ The `@mlflow.trace` decorator for custom functions\n",
    "3. ‚úÖ Span types and naming conventions\n",
    "4. ‚úÖ Adding custom attributes with `span = mlflow.get_current_active_span(); span.set_attributes({...})`\n",
    "5. ‚úÖ Building hierarchical traces (parent-child relationships)\n",
    "6. ‚úÖ Tracing complete RAG pipelines end-to-end\n",
    "7. ‚úÖ Tracing agentic workflows with tool usage\n",
    "8. ‚úÖ Advanced debugging techniques with traces\n",
    "9. ‚úÖ Performance analysis and optimization strategies\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Combine** autologging and manual tracing for complete coverage\n",
    "- **Use span types** for better organization and filtering\n",
    "- **Add custom attributes** for debugging and analysis\n",
    "- **Build hierarchies** to represent complex workflows accurately\n",
    "- **Trace everything** from retrieval to generation to validation\n",
    "- **Use traces** for debugging, performance, and optimization\n",
    "\n",
    "### Production Best Practices\n",
    "\n",
    "1. **Always trace** in development and staging\n",
    "2. **Sample traces** in high-volume production (e.g., 1-10%)\n",
    "3. **Monitor key metrics** from traces (latency, errors, costs)\n",
    "4. **Set up alerts** for anomalies\n",
    "5. **Review traces** during incident response\n",
    "6. **Use trace IDs** for cross-system correlation\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**üìì Notebook 1.5: Prompt Management**\n",
    "\n",
    "Learn how to:\n",
    "- Create reusable prompt templates\n",
    "- Version prompts systematically\n",
    "- Link prompts to experiments and traces\n",
    "- Share prompts across your team\n",
    "- Track prompt performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
