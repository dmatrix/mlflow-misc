{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1.7: Evaluating Agents with MLflow\n",
    "\n",
    "## LLM-as-Judge Evaluation for GenAI Applications\n",
    "\n",
    "This notebook teaches you how to systematically evaluate agents and LLM applications using MLflow's evaluation framework. You'll learn to use built-in judges, create custom scorers, and integrate third-party evaluation libraries like DeepEval.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- ‚úÖ Why agent evaluation matters\n",
    "- ‚úÖ MLflow built-in scorers (RelevanceToQuery, Correctness, Guidelines, Safety)\n",
    "- ‚úÖ Creating custom scorers with the `@scorer` decorator\n",
    "- ‚úÖ DeepEval integration for conversational evaluation\n",
    "- ‚úÖ Session-level multi-turn evaluation\n",
    "\n",
    "### Prerequisites\n",
    "- Completed notebooks 1.1-1.6\n",
    "- Understanding of MLflow tracing\n",
    "\n",
    "### Estimated Time: 25-30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Why Evaluate Agents?\n",
    "\n",
    "Agent evaluation is critical for:\n",
    "\n",
    "| Challenge | Solution |\n",
    "|-----------|----------|\n",
    "| Non-deterministic outputs | Statistical evaluation over multiple runs |\n",
    "| Complex reasoning chains | Step-by-step quality assessment |\n",
    "| Multi-turn conversations | Session-level coherence metrics |\n",
    "| Safety and compliance | Automated guardrail checking |\n",
    "| Regression detection | Baseline comparisons |\n",
    "\n",
    "### LLM-as-Judge Pattern\n",
    "\n",
    "Instead of brittle string matching, we use LLMs to evaluate LLM outputs:\n",
    "\n",
    "```\n",
    "Agent Output ‚Üí Judge LLM ‚Üí Score + Reasoning\n",
    "```\n",
    "\n",
    "![LLM as a judge concept](images/llm-as-judge.png)\n",
    "\n",
    "MLflow provides:\n",
    "1. **Built-in scorers** - Pre-configured judges for common metrics\n",
    "2. **Custom scorers** - Define your own evaluation logic\n",
    "3. **Third-party integrations** - DeepEval, RAGAS, Phoenix, and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/07 14:12:45 INFO mlflow.tracking.fluent: Experiment with name '1-agent-evaluation' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment configured for agent evaluation\n",
      "use_databricks_provider: False\n",
      "MLflow tracking: http://localhost:5000\n",
      "Experiment: 1-agent-evaluation\n",
      "Agent model : gpt-5.2\n",
      "Judge model : gpt-5.2\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "from utils.clnt_utils import is_databricks_ai_gateway_client, get_databricks_ai_gateway_client, get_openai_client, get_ai_gateway_model_names\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Configure MLflow\n",
    "EXPERIMENT_NAME = \"1-agent-evaluation\"\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# Check if we are using a Databricks AI Gateway client\n",
    "use_databricks_provider = is_databricks_ai_gateway_client()\n",
    "if use_databricks_provider:\n",
    "    client = get_databricks_ai_gateway_client()\n",
    "    models = get_ai_gateway_model_names()\n",
    "    JUDGE_MODEL = models[2]\n",
    "    AGENT_MODEL = models[0]\n",
    "else:\n",
    "    # Initialize as an OpenAI client\n",
    "    client = get_openai_client()\n",
    "    JUDGE_MODEL = \"gpt-5.2\"\n",
    "    AGENT_MODEL = \"gpt-5.2\"\n",
    "\n",
    "# Enable autologging\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "print(\"‚úÖ Environment configured for agent evaluation\")\n",
    "print(f\"use_databricks_provider: {use_databricks_provider}\")\n",
    "print(f\"MLflow tracking: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"Experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"Agent model : {AGENT_MODEL}\")\n",
    "print(f\"Judge model : {JUDGE_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Create a Simple Agent for Evaluation\n",
    "\n",
    "Let's create a simple Q&A agent that we'll evaluate. This is a good first stepping stone to the next steps in which we use a number of built-in judges, followed by custom scorers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is MLflow for GenAI?\n",
      "\n",
      "Response: MLflow for GenAI is the set of MLflow capabilities designed to **build, evaluate, track, and deploy generative AI applications** (LLMs and agentic systems) with the same rigor you‚Äôd use for traditional ML‚Äîcovering prompts, models, tools, retrieval, and outputs.\n",
      "\n",
      "Key pieces:\n",
      "\n",
      "- **Experiment tracking for LLM apps**: Log prompts, model/provider (e.g., OpenAI, Anthropic, local), parameters (temperature, max tokens), retrieved context, tool calls, and outputs‚Äîso runs are reproducible and comparable.\n",
      "- **Prompt & model management**: Version and register prompts and GenAI ‚Äúmodels‚Äù/chains so you can promote changes through dev ‚Üí staging ‚Üí prod with governance.\n",
      "- **Evaluation (LLM evals)**: Run standardized evaluations on datasets (QA, summarization, RAG, safety), including automated metrics and LLM-as-judge style scoring, plus human review workflows where applicable.\n",
      "- **Tracing/observability**: Capture traces of multi-step chains/agents (LLM calls, retriever hits, tool invocations) to debug latency, cost, and quality issues.\n",
      "- **Deployment**: Package and deploy GenAI applications (including RAG pipelines) behind a serving endpoint, with versioning and rollback.\n",
      "\n",
      "In short, it helps you move GenAI prototypes into production by adding **tracking, evaluation, tracing, and lifecycle management** around LLM-based systems.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-60107dc7c4f3d0724eceac17fbb1b190&amp;experiment_id=1&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-60107dc7c4f3d0724eceac17fbb1b190)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "class SimpleQAAgent:\n",
    "    \"\"\"\n",
    "    A simple Q&A agent for demonstration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, client:Any, model: str = \"gpt-5.2\"):\n",
    "        self.model = model\n",
    "        self.client = client\n",
    "    \n",
    "    @mlflow.trace(name=\"qa_agent\", span_type=\"AGENT\")\n",
    "    def answer(self, question: str) -> str:\n",
    "        \"\"\"Answer a question.\"\"\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\"\"You are a helpful Agent Assistant. Provide concise, accurate answers, \n",
    "                no hallucinations, with a focus on MLflow and GenAI.\"\"\"},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "# Initialize agent\n",
    "agent = SimpleQAAgent(client=client, model=AGENT_MODEL)\n",
    "\n",
    "# Test the agent with a single question\n",
    "test_question = \"What is MLflow for GenAI?\"\n",
    "test_response = agent.answer(test_question)\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"\\nResponse: {test_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Evaluation: Is This Response Good?\n",
    "\n",
    "Now that we have a response, how do we know if it's good? We could read it manually, but that doesn't scale. Instead, let's use MLflow's built-in `RelevanceToQuery` scorer to have an LLM judge evaluate the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/07 14:13:01 INFO mlflow.models.evaluation.utils.trace: Auto tracing is temporarily enabled during the model evaluation for computing some metrics and debugging. To disable tracing, call `mlflow.autolog(disable=True)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Evaluating the test response with RelevanceToQuery scorer...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc5e17b6288461a87daaccccb31ac8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [Elapsed: 00:00, Remaining: ?] "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <title>Evaluation output</title>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <style>\n",
       "        body {\n",
       "            font-family: Arial, sans-serif;\n",
       "        }\n",
       "\n",
       "        .header {\n",
       "            a.button {\n",
       "                padding: 4px 8px;\n",
       "                line-height: 20px;\n",
       "                box-shadow: none;\n",
       "                height: 20px;\n",
       "                display: inline-flex;\n",
       "                align-items: center;\n",
       "                justify-content: center;\n",
       "                vertical-align: middle;\n",
       "                background-color: rgb(34, 114, 180);\n",
       "                color: rgb(255, 255, 255);\n",
       "                text-decoration: none;\n",
       "                animation-duration: 0s;\n",
       "                transition: none 0s ease 0s;\n",
       "                position: relative;\n",
       "                white-space: nowrap;\n",
       "                text-align: center;\n",
       "                border: 1px solid rgb(192, 205, 216);\n",
       "                cursor: pointer;\n",
       "                user-select: none;\n",
       "                touch-action: manipulation;\n",
       "                border-radius: 4px;\n",
       "                gap: 6px;\n",
       "            }\n",
       "\n",
       "            a.button:hover {\n",
       "                background-color: rgb(14, 83, 139) !important;\n",
       "                border-color: transparent !important;\n",
       "                color: rgb(255, 255, 255) !important;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .warnings-section {\n",
       "            margin-top: 8px;\n",
       "\n",
       "            ul {\n",
       "                list-style-type: none;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .instructions-section {\n",
       "            margin-top: 16px;\n",
       "            font-size: 14px;\n",
       "\n",
       "            ul {\n",
       "                margin-top: 0;\n",
       "                margin-bottom: 0;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        code {\n",
       "            font-family: monospace;\n",
       "        }\n",
       "\n",
       "        .note {\n",
       "            color: #666;\n",
       "        }\n",
       "\n",
       "        a {\n",
       "            color: #2272B4;\n",
       "            text-decoration: none;\n",
       "        }\n",
       "\n",
       "        a:hover {\n",
       "            color: #005580;\n",
       "        }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "    <div class=\"header\">\n",
       "        <a href=\"http://localhost:5000/#/experiments/1/evaluation-runs?selectedRunUuid=ccc4645c9d1a49cd8b3fad195d55276e\" class=\"button\">\n",
       "            View evaluation results in MLflow\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1em\" height=\"1em\" fill=\"none\" viewBox=\"0 0 16 16\" aria-hidden=\"true\" focusable=\"false\" class=\"\">\n",
       "                <path fill=\"currentColor\" d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"></path>\n",
       "                <path fill=\"currentColor\" d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"></path>\n",
       "            </svg>\n",
       "        </a>\n",
       "    </div>\n",
       "</div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Quick Evaluation Result:\n",
      "----------------------------------------\n",
      "   Relevance Score: 1.0\n",
      "\n",
      "‚úÖ The LLM judge evaluated our agent's response!\n",
      "   Now let's learn about all the built-in scorers available...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mlflow.genai.scorers import RelevanceToQuery\n",
    "\n",
    "# Configure judge model URI based on provider\n",
    "if use_databricks_provider:\n",
    "    databricks_token = os.environ.get(\"DATABRICKS_TOKEN\")\n",
    "    ai_gateway_base_url = os.environ.get(\"AI_GATEWAY_BASE_URL\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = databricks_token\n",
    "    os.environ[\"OPENAI_API_BASE\"] = ai_gateway_base_url\n",
    "    judge_model_uri = f\"openai:/{JUDGE_MODEL}\"\n",
    "else:\n",
    "    judge_model_uri = f\"openai:/{JUDGE_MODEL}\"\n",
    "\n",
    "# Create a quick evaluation of the single test response\n",
    "quick_eval_data = [{\n",
    "    \"inputs\": {\"question\": test_question},\n",
    "    \"outputs\": {\"response\": test_response}  # Pre-computed output, no predict_fn needed\n",
    "}]\n",
    "\n",
    "# Evaluate with RelevanceToQuery scorer\n",
    "quick_scorer = RelevanceToQuery(model=judge_model_uri)\n",
    "\n",
    "print(\"üîÑ Evaluating the test response with RelevanceToQuery scorer...\\n\")\n",
    "\n",
    "quick_result = mlflow.genai.evaluate(\n",
    "    data=quick_eval_data,\n",
    "    scorers=[quick_scorer]\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "print(\"üìä Quick Evaluation Result:\")\n",
    "print(\"-\" * 40)\n",
    "score = quick_result.metrics.get(\"relevance_to_query/mean\", \"N/A\")\n",
    "print(f\"   Relevance Score: {score}\")\n",
    "print(\"\\n‚úÖ The LLM judge evaluated our agent's response!\")\n",
    "print(\"   Now let's learn about all the built-in scorers available...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: MLflow Built-in Scorers\n",
    "\n",
    "MLflow provides [pre-configured scorers](https://mlflow.org/docs/latest/genai/eval-monitor/scorers/llm-judge/predefined/) for common evaluation needs:\n",
    "\n",
    "| Scorer | Purpose | Inputs |\n",
    "|--------|---------|--------|\n",
    "| `RelevanceToQuery` | Is the response relevant to the question? | inputs, outputs |\n",
    "| `Correctness` | Is the response factually correct? | outputs, expectations |\n",
    "| `Guidelines` | Does the response follow specific guidelines? | outputs |\n",
    "| `Safety` | Is the response safe and appropriate? | outputs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Judge model URI: openai:/gpt-5.2\n",
      "\n",
      "‚úÖ Built-in scorers initialized:\n",
      "   - RelevanceToQuery, Correctness, Safety, Guidelines\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mlflow.genai.scorers import (\n",
    "    RelevanceToQuery,\n",
    "    Correctness,\n",
    "    Guidelines,\n",
    "    Safety\n",
    ")\n",
    "\n",
    "if use_databricks_provider:\n",
    "    databricks_token = os.environ.get(\"DATABRICKS_TOKEN\")\n",
    "    ai_gateway_base_url = os.environ.get(\"AI_GATEWAY_BASE_URL\")\n",
    "    \n",
    "    # Try configuring as OpenAI-compatible endpoint\n",
    "    os.environ[\"OPENAI_API_KEY\"] = databricks_token\n",
    "    os.environ[\"OPENAI_API_BASE\"] = ai_gateway_base_url\n",
    "    \n",
    "    judge_model_uri = f\"databricks:/{JUDGE_MODEL}\"\n",
    "    print(\"üîß Configured for Databricks AI Gateway\")\n",
    "    print(f\"   Base URL: {ai_gateway_base_url}\")\n",
    "    print(f\"   Model: {JUDGE_MODEL}\")\n",
    "else:\n",
    "    judge_model_uri = f\"openai:/{JUDGE_MODEL}\"\n",
    "\n",
    "print(f\"üîß Judge model URI: {judge_model_uri}\")\n",
    "\n",
    "# Initialize built-in scorers\n",
    "relevance_scorer = RelevanceToQuery(model=judge_model_uri)\n",
    "correctness_scorer = Correctness(model=judge_model_uri)\n",
    "safety_scorer = Safety(model=judge_model_uri)\n",
    "guidelines_scorer = Guidelines(\n",
    "    model=judge_model_uri,\n",
    "    guidelines=[ \"\"\"\n",
    "Response should be appropriately detailed:\n",
    "- Simple factual questions: < 200 words  \n",
    "- Technical how-to questions: < 500 words\n",
    "- Complex architectural questions: < 1000 words\n",
    "\"\"\"\n",
    "    ],\n",
    "    name=\"custom_guidelines\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Built-in scorers initialized:\")\n",
    "print(\"   - RelevanceToQuery, Correctness, Safety, Guidelines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Create Evaluation Dataset\n",
    "\n",
    "This comprises three steps:\n",
    " 1. Create an evaluation set that we will use in our evalution of the metrics to judge against the answer\n",
    " 2. Create a prediction function that will take question as in input, and return a LLM response to the question\n",
    " 3. run mlflow evaluation with all the scorers\n",
    "\n",
    "### Why Create an Evaluation Dataset?\n",
    "\n",
    "An evaluation dataset is the foundation of systematic agent testing. Without it, you're essentially \"eyeballing\" outputs‚Äîwhich doesn't scale and misses edge cases. The dataset enables **repeatable, automated evaluation** so you can detect regressions when you change prompts, models, or agent logic.\n",
    "\n",
    "### What Purpose Does It Serve?\n",
    "\n",
    "| Purpose | Benefit |\n",
    "|---------|---------|\n",
    "| **Benchmark Performance** | Measure how well your agent performs on known questions |\n",
    "| **Detect Regressions** | Catch quality drops when updating prompts or models |\n",
    "| **Compare Configurations** | A/B test different models, temperatures, or prompts |\n",
    "| **Validate Edge Cases** | Include tricky questions that previously caused failures |\n",
    "| **Document Expected Behavior** | Serve as living documentation of what your agent should do |\n",
    "\n",
    "### How Many Evaluation Pairs?\n",
    "\n",
    "**Rule of thumb: Start with 20-50 examples for development, scale to 100-500 for production.**\n",
    "\n",
    "- **Minimum viable**: 10-20 pairs to catch obvious issues\n",
    "- **Development**: 20-50 pairs covering main use cases\n",
    "- **Pre-production**: 50-100 pairs including edge cases\n",
    "- **Production monitoring**: 100-500+ pairs for statistical significance\n",
    "\n",
    "The dataset below uses 8 examples for tutorial brevity, but real evaluations need more coverage.\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "- `inputs`: The questions/prompts sent to the agent (dict with your input field names)\n",
    "- `expectations` (optional): Expected answers for correctness checking\n",
    "\n",
    "**Important:** Use `expected_response` as the field name in expectations‚Äîthis is what MLflow's `Correctness` scorer looks for. Alternative: use `expected_facts` for fact-based evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation dataset created with 8 examples\n",
      "\n",
      "üìã Questions cover:\n",
      "   - MLflow Tracing fundamentals\n",
      "   - Prompt management\n",
      "   - Span types and structure\n",
      "   - LLM evaluation methods\n",
      "   - LLM-as-Judge pattern\n",
      "   - Cost and token tracking\n",
      "   - Framework integrations\n",
      "   - Session-level observability\n"
     ]
    }
   ],
   "source": [
    "# Evaluation dataset focused on MLflow for GenAI and Agent Observability\n",
    "# NOTE: Use 'expected_response' - this is the field name that MLflow's Correctness scorer expects\n",
    "from mlflow.genai import create_dataset\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is MLflow Tracing and why is it important for GenAI applications?\"},\n",
    "        \"expectations\": {\"expected_response\": \"MLflow Tracing provides observability for GenAI applications by capturing the complete execution flow including LLM calls, retrieval steps, tool usage, and agent reasoning. It's important because it enables debugging, performance analysis, and understanding of complex AI pipelines.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How does MLflow help with prompt management in GenAI development?\"},\n",
    "        \"expectations\": {\"expected_response\": \"MLflow's Prompt Registry allows you to version control prompts, tag and search prompt versions, link prompts to experiments, and collaborate with teams. This ensures reproducibility and systematic prompt engineering.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What are spans in MLflow Tracing and what types are available?\"},\n",
    "        \"expectations\": {\"expected_response\": \"Spans are units of work captured during tracing. MLflow supports span types including LLM (for model calls), RETRIEVER (for RAG retrieval), TOOL (for function calls), AGENT (for agent orchestration), CHAIN (for sequential operations), and EMBEDDING (for vector operations).\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How can you evaluate LLM outputs using MLflow?\"},\n",
    "        \"expectations\": {\"expected_response\": \"MLflow provides an evaluation framework with built-in scorers like RelevanceToQuery, Correctness, Guidelines, and Safety. You can also create custom scorers using the @scorer decorator or integrate third-party libraries like DeepEval and RAGAS.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the LLM-as-Judge pattern and how does MLflow support it?\"},\n",
    "        \"expectations\": {\"expected_response\": \"LLM-as-Judge uses an LLM to evaluate outputs from another LLM, replacing brittle string matching with intelligent assessment. MLflow supports this through built-in scorers that use configurable judge models to provide scores and reasoning explanations.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How do you track costs and token usage in MLflow for GenAI?\"},\n",
    "        \"expectations\": {\"expected_response\": \"MLflow automatically logs token usage (prompt tokens, completion tokens, total tokens) and can calculate costs based on model pricing. This data is captured in traces and experiment runs, enabling cost analysis and optimization across different models and configurations.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What frameworks does MLflow integrate with for GenAI auto-tracing?\"},\n",
    "        \"expectations\": {\"expected_response\": \"MLflow provides auto-tracing for 40+ frameworks including OpenAI, Anthropic, LangChain, LlamaIndex, AWS Bedrock, Google Vertex AI, Cohere, Ollama, DSPy, AutoGen, and CrewAI. Auto-tracing automatically captures LLM calls without manual instrumentation.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How do you implement session-level tracing for multi-turn conversations?\"},\n",
    "        \"expectations\": {\"expected_response\": \"Key concepts: (1) stable session identifier, (2) tag traces with session_id, (3) filter traces by session, (4) MLflow search capabilities\"}\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create and register the dataset\n",
    "dataset  = create_dataset(\n",
    "    name=\"regression_test_suite\",\n",
    "    experiment_id= mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id,  \n",
    "    tags={\"type\": \"regression\", \"priority\": \"critical\"},\n",
    ")\n",
    "\n",
    "dataset.merge_records(eval_dataset)\n",
    "\n",
    "print(f\"‚úÖ Evaluation dataset created with {len(eval_dataset)} examples\")\n",
    "print(\"\\nüìã Questions cover:\")\n",
    "print(\"   - MLflow Tracing fundamentals\")\n",
    "print(\"   - Prompt management\")\n",
    "print(\"   - Span types and structure\")\n",
    "print(\"   - LLM evaluation methods\")\n",
    "print(\"   - LLM-as-Judge pattern\")\n",
    "print(\"   - Cost and token tracking\")\n",
    "print(\"   - Framework integrations\")\n",
    "print(\"   - Session-level observability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How predict_fn works?\n",
    "\n",
    "#### When and Why to Use predict_fn\n",
    "##### predict_fn is required when:\n",
    "\n",
    " * Your dataset only contains inputs (and optionally expectations)\n",
    " * You need MLflow to call your agent/model to generate the outputs for evaluation\n",
    "\n",
    "##### predict_fn is NOT needed when:\n",
    "\n",
    " * Your data already contains pre-computed outputs\n",
    " * You're passing MLflow traces (which already contain the inputs/outputs)\n",
    "\n",
    "When `mlflow.genai.evaluate()` runs:\n",
    "\n",
    "1. It iterates through each item in `eval_dataset`\n",
    "2. Extracts item[\"inputs\"] and passes it to `predict_fn(inputs)`\n",
    "3. Your function returns outputs (e.g., {\"response\": \"...\"})\n",
    "\n",
    "Scorers then evaluate using inputs, outputs, and expectations\n",
    " `[inputs, outputs responses (from llm), expecttions] --> scorers`\n",
    "\n",
    " - **eval_dataset**: Defines what to test (questions + expected answers)\n",
    " - **predict_fn**: Defines how to get answers (calls your agent)\n",
    " - **scorers**: Define how to judge quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prediction function defined\n",
      "   Signature: predict_fn(question: str) -> dict\n"
     ]
    }
   ],
   "source": [
    "def predict_fn(question: str) -> dict:\n",
    "    \"\"\"\n",
    "    Prediction function wrapper for evaluation.\n",
    "    \n",
    "    Note: mlflow.genai.evaluate() unpacks the 'inputs' dict as keyword arguments,\n",
    "    so the function signature must match the keys in your dataset's 'inputs' field.\n",
    "    \n",
    "    Dataset: {\"inputs\": {\"question\": \"...\"}} \n",
    "    Called as: predict_fn(question=\"...\")\n",
    "    \n",
    "    Args:\n",
    "        question: The question string (unpacked from inputs dict)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with 'response' key\n",
    "    \"\"\"\n",
    "    response = agent.answer(question)\n",
    "    return {\"response\": response}\n",
    "\n",
    "print(\"‚úÖ Prediction function defined\")\n",
    "print(\"   Signature: predict_fn(question: str) -> dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Run Evaluation with Built-in Scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/07 15:23:15 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset. To disable this check, set the MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION environment variable to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Running evaluation with built-in scorers...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d98f5fcf7b646af84d6502e5e50a1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [Elapsed: 00:00, Remaining: ?] "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <title>Evaluation output</title>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <style>\n",
       "        body {\n",
       "            font-family: Arial, sans-serif;\n",
       "        }\n",
       "\n",
       "        .header {\n",
       "            a.button {\n",
       "                padding: 4px 8px;\n",
       "                line-height: 20px;\n",
       "                box-shadow: none;\n",
       "                height: 20px;\n",
       "                display: inline-flex;\n",
       "                align-items: center;\n",
       "                justify-content: center;\n",
       "                vertical-align: middle;\n",
       "                background-color: rgb(34, 114, 180);\n",
       "                color: rgb(255, 255, 255);\n",
       "                text-decoration: none;\n",
       "                animation-duration: 0s;\n",
       "                transition: none 0s ease 0s;\n",
       "                position: relative;\n",
       "                white-space: nowrap;\n",
       "                text-align: center;\n",
       "                border: 1px solid rgb(192, 205, 216);\n",
       "                cursor: pointer;\n",
       "                user-select: none;\n",
       "                touch-action: manipulation;\n",
       "                border-radius: 4px;\n",
       "                gap: 6px;\n",
       "            }\n",
       "\n",
       "            a.button:hover {\n",
       "                background-color: rgb(14, 83, 139) !important;\n",
       "                border-color: transparent !important;\n",
       "                color: rgb(255, 255, 255) !important;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .warnings-section {\n",
       "            margin-top: 8px;\n",
       "\n",
       "            ul {\n",
       "                list-style-type: none;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .instructions-section {\n",
       "            margin-top: 16px;\n",
       "            font-size: 14px;\n",
       "\n",
       "            ul {\n",
       "                margin-top: 0;\n",
       "                margin-bottom: 0;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        code {\n",
       "            font-family: monospace;\n",
       "        }\n",
       "\n",
       "        .note {\n",
       "            color: #666;\n",
       "        }\n",
       "\n",
       "        a {\n",
       "            color: #2272B4;\n",
       "            text-decoration: none;\n",
       "        }\n",
       "\n",
       "        a:hover {\n",
       "            color: #005580;\n",
       "        }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "    <div class=\"header\">\n",
       "        <a href=\"http://localhost:5000/#/experiments/1/evaluation-runs?selectedRunUuid=9fd149dab5b84b17af6428c8f176c599\" class=\"button\">\n",
       "            View evaluation results in MLflow\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1em\" height=\"1em\" fill=\"none\" viewBox=\"0 0 16 16\" aria-hidden=\"true\" focusable=\"false\" class=\"\">\n",
       "                <path fill=\"currentColor\" d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"></path>\n",
       "                <path fill=\"currentColor\" d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"></path>\n",
       "            </svg>\n",
       "        </a>\n",
       "    </div>\n",
       "</div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Evaluation complete!\n",
      "\n",
      "üìä Metrics Summary:\n",
      "--------------------------------------------------\n",
      "  safety/mean: 1.000\n",
      "  relevance_to_query/mean: 1.000\n",
      "  custom_guidelines/mean: 0.571\n",
      "  correctness/mean: 0.429\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "print(\"üîÑ Running evaluation with built-in scorers...\\n\")\n",
    "\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=predict_fn,\n",
    "    scorers=[\n",
    "        relevance_scorer,\n",
    "        correctness_scorer,\n",
    "        safety_scorer,\n",
    "        guidelines_scorer\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete!\")\n",
    "\n",
    "# Display metrics\n",
    "print(\"\\nüìä Metrics Summary:\")\n",
    "print(\"-\" * 50)\n",
    "if results.metrics:\n",
    "    for metric_name, value in results.metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {metric_name}: {value:.3f}\")\n",
    "        else:\n",
    "            print(f\"  {metric_name}: {value}\")\n",
    "else:\n",
    "    print(\"  No metrics returned\")\n",
    "    print(\"\\n‚ö†Ô∏è  Scorers returned None - this usually means the judge model call failed.\")\n",
    "    print(\"  Check the 'error_message' or similar columns above for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Custom Scorers with @scorer Decorator\n",
    "\n",
    "Create your own evaluation logic using the `@scorer` decorator.\n",
    "\n",
    "Beyond using built-in metrics, you can define custom scorers to capture specific subject matter expertise. This is particularly useful when standard scorers cannot effectively gauge unique or nuanced aspects of your model's responses. While this example uses simple scorers for brevity and demonstration, you should tailor your custom metrics to reflect the specialized requirements of your specific domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Custom scorers defined:\n",
      "   - response_length_check\n",
      "   - contains_keywords\n",
      "   - no_hallucination_markers\n"
     ]
    }
   ],
   "source": [
    "from mlflow.genai import scorer\n",
    "\n",
    "@scorer\n",
    "def response_length_check(outputs: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Check if response is within acceptable length.\n",
    "    Returns True if response is between 200 and 500 characters.\n",
    "    \"\"\"\n",
    "    response = outputs.get(\"response\", \"\")\n",
    "    length = len(response)\n",
    "    return 20 <= length <= 500\n",
    "\n",
    "@scorer\n",
    "def contains_keywords(outputs: dict, expectations: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Check if response contains key terms from expected answer.\n",
    "    \"\"\"\n",
    "    response = outputs.get(\"response\", \"\").lower()\n",
    "    # Use 'expected_response' to match the dataset field name\n",
    "    expected = expectations.get(\"expected_response\", \"\").lower()\n",
    "    \n",
    "    # Extract key words (simple approach)\n",
    "    key_words = [word for word in expected.split() if len(word) > 4]\n",
    "    \n",
    "    # Check if at least 30% of key words are present\n",
    "    # If no keywords to check, fail conservatively (may indicate data issue)\n",
    "    if not key_words:\n",
    "        return False\n",
    "    \n",
    "    matches = sum(1 for word in key_words if word in response)\n",
    "    return matches / len(key_words) >= 0.3\n",
    "\n",
    "@scorer\n",
    "def no_hallucination_markers(outputs: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Check for common hallucination markers.\n",
    "    \"\"\"\n",
    "    response = outputs.get(\"response\", \"\").lower()\n",
    "    \n",
    "    hallucination_markers = [\n",
    "        \"i think\",\n",
    "        \"i believe\",\n",
    "        \"probably\",\n",
    "        \"might be\",\n",
    "        \"i'm not sure\",\n",
    "        \"as far as i know\"\n",
    "    ]\n",
    "    \n",
    "    return not any(marker in response for marker in hallucination_markers)\n",
    "\n",
    "print(\"‚úÖ Custom scorers defined:\")\n",
    "print(\"   - response_length_check\")\n",
    "print(\"   - contains_keywords\")\n",
    "print(\"   - no_hallucination_markers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation on our Customer Judges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/07 15:24:00 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset. To disable this check, set the MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION environment variable to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Running evaluation with custom scorers...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4368014644437db5bda14bb5562d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [Elapsed: 00:00, Remaining: ?] "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <title>Evaluation output</title>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <style>\n",
       "        body {\n",
       "            font-family: Arial, sans-serif;\n",
       "        }\n",
       "\n",
       "        .header {\n",
       "            a.button {\n",
       "                padding: 4px 8px;\n",
       "                line-height: 20px;\n",
       "                box-shadow: none;\n",
       "                height: 20px;\n",
       "                display: inline-flex;\n",
       "                align-items: center;\n",
       "                justify-content: center;\n",
       "                vertical-align: middle;\n",
       "                background-color: rgb(34, 114, 180);\n",
       "                color: rgb(255, 255, 255);\n",
       "                text-decoration: none;\n",
       "                animation-duration: 0s;\n",
       "                transition: none 0s ease 0s;\n",
       "                position: relative;\n",
       "                white-space: nowrap;\n",
       "                text-align: center;\n",
       "                border: 1px solid rgb(192, 205, 216);\n",
       "                cursor: pointer;\n",
       "                user-select: none;\n",
       "                touch-action: manipulation;\n",
       "                border-radius: 4px;\n",
       "                gap: 6px;\n",
       "            }\n",
       "\n",
       "            a.button:hover {\n",
       "                background-color: rgb(14, 83, 139) !important;\n",
       "                border-color: transparent !important;\n",
       "                color: rgb(255, 255, 255) !important;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .warnings-section {\n",
       "            margin-top: 8px;\n",
       "\n",
       "            ul {\n",
       "                list-style-type: none;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .instructions-section {\n",
       "            margin-top: 16px;\n",
       "            font-size: 14px;\n",
       "\n",
       "            ul {\n",
       "                margin-top: 0;\n",
       "                margin-bottom: 0;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        code {\n",
       "            font-family: monospace;\n",
       "        }\n",
       "\n",
       "        .note {\n",
       "            color: #666;\n",
       "        }\n",
       "\n",
       "        a {\n",
       "            color: #2272B4;\n",
       "            text-decoration: none;\n",
       "        }\n",
       "\n",
       "        a:hover {\n",
       "            color: #005580;\n",
       "        }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "    <div class=\"header\">\n",
       "        <a href=\"http://localhost:5000/#/experiments/1/evaluation-runs?selectedRunUuid=7ff4e377d36e4656af4790e039764e2e\" class=\"button\">\n",
       "            View evaluation results in MLflow\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1em\" height=\"1em\" fill=\"none\" viewBox=\"0 0 16 16\" aria-hidden=\"true\" focusable=\"false\" class=\"\">\n",
       "                <path fill=\"currentColor\" d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"></path>\n",
       "                <path fill=\"currentColor\" d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"></path>\n",
       "            </svg>\n",
       "        </a>\n",
       "    </div>\n",
       "</div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Custom evaluation complete!\n",
      "\n",
      "Custom Metrics Summary:\n",
      "----------------------------------------\n",
      "  response_length_check/mean: 0.125\n",
      "  no_hallucination_markers/mean: 1.000\n",
      "  contains_keywords/mean: 0.750\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation with custom scorers\n",
    "print(\"üîÑ Running evaluation with custom scorers...\\n\")\n",
    "\n",
    "custom_results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=predict_fn,\n",
    "    scorers=[\n",
    "        response_length_check,\n",
    "        contains_keywords,\n",
    "        no_hallucination_markers\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Custom evaluation complete!\")\n",
    "print(\"\\nCustom Metrics Summary:\")\n",
    "print(\"-\" * 40)\n",
    "for metric_name, value in custom_results.metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric_name}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {metric_name}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: DeepEval Integration\n",
    "\n",
    "MLflow integrates with [DeepEval](https://mlflow.org/docs/latest/genai/eval-monitor/scorers/third-party/deepeval/) for advanced conversational AI evaluation metrics.\n",
    "\n",
    "| DeepEval Scorer | Purpose |\n",
    "|-----------------|----------|\n",
    "| `ConversationCompleteness` | Did the conversation achieve its goal? |\n",
    "| `KnowledgeRetention` | Does the agent remember context? |\n",
    "| `TopicAdherence` | Does the agent stay on topic? |\n",
    "| `Toxicity` | Is the agent response harmful or toxic in tonality|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DeepEval scorers initialized:\n",
      "   - ConversationCompleteness\n",
      "   - KnowledgeRetention\n",
      "   - TopicAdherence\n",
      "   - Toxicity\n",
      "\n",
      " Next, let's evaluate a multi-turn conversation using these DeepEval scorers\n"
     ]
    }
   ],
   "source": [
    "from mlflow.genai.scorers.deepeval import (\n",
    "    ConversationCompleteness,\n",
    "    KnowledgeRetention,\n",
    "    TopicAdherence,\n",
    "    Toxicity,\n",
    ")\n",
    "\n",
    "# Initialize DeepEval scorers\n",
    "jude_model_uri = f\"openai:/{JUDGE_MODEL}\"\n",
    "\n",
    "completeness_scorer = ConversationCompleteness(model=jude_model_uri, threshold=0.7, include_reason=True)\n",
    "retention_scorer = KnowledgeRetention(model=jude_model_uri, threshold=0.7, include_reason=True)\n",
    "toxicity_scorer = Toxicity(model=jude_model_uri, threshold=0.7, include_reason=True)\n",
    "topic_scorer = TopicAdherence(model=jude_model_uri, threshold=0.7, include_reason=True, relevant_topics=[\"MLflow\", \"machine learning\", \"AI\", \"data science\", \"genai\", \"agent\", \"observability\", \"prompt engineering\", \"prompt management\", \"prompt registry\", \"experiment tracking\"])\n",
    "\n",
    "\n",
    "print(\"‚úÖ DeepEval scorers initialized:\")\n",
    "print(\"   - ConversationCompleteness\")\n",
    "print(\"   - KnowledgeRetention\")\n",
    "print(\"   - TopicAdherence\")\n",
    "print(\"   - Toxicity\")\n",
    "print(\"\\n Next, let's evaluate a multi-turn conversation using these DeepEval scorers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Multi-Turn Conversation Agent\n",
    "\n",
    "For session-level evaluation, we need an agent that handles conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ConversationalAgent defined with session tracking\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "class ConversationalAgent:\n",
    "    \"\"\"\n",
    "    An agent that maintains conversation history for multi-turn interactions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, client: Any, model: str = \"gpt-5.2\"):\n",
    "        self.model = model\n",
    "        self.client = client\n",
    "        self.conversation_history = []\n",
    "        self.session_id = str(uuid.uuid4())\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset conversation history and start new session.\"\"\"\n",
    "        self.conversation_history = []\n",
    "        self.session_id = str(uuid.uuid4())\n",
    "    \n",
    "    @mlflow.trace(name=\"conversational_agent\", span_type=\"AGENT\")\n",
    "    def chat(self, user_message: str) -> str:\n",
    "        \"\"\"\n",
    "        Send a message and get a response, maintaining history.\n",
    "        \"\"\"\n",
    "        # Tag trace with session ID for grouping\n",
    "        mlflow.update_current_trace(metadata={\n",
    "            \"mlflow.trace.session\": self.session_id,\n",
    "            \"turn_number\": len(self.conversation_history) // 2 + 1\n",
    "        })\n",
    "        \n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message\n",
    "        })\n",
    "        \n",
    "        # Prepare messages with system prompt. Add previous context to the conversation.\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \n",
    "            \"content\": \"\"\"You are a helpful MLflow expert assistant. Answer questions about MLflow clearly, accurately,\n",
    "                        concisely, without hallucinations, and accurately. Remember previous context in the conversation.\"\"\"}\n",
    "        ] + self.conversation_history\n",
    "        \n",
    "        # Get response from the Agent LLM\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        \n",
    "        assistant_message = response.choices[0].message.content\n",
    "        \n",
    "        # Add to history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": assistant_message\n",
    "        })\n",
    "        \n",
    "        return assistant_message\n",
    "\n",
    "print(\"‚úÖ ConversationalAgent defined with session tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate a multi-turn conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/07 15:25:02 WARNING mlflow.tracing.fluent: Found non-string values in metadata. Non-string values in metadata will automatically be stringified when the trace is logged. Non-string items: {'turn_number': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è Multi-Turn Conversation\n",
      "\n",
      "============================================================\n",
      "\n",
      "[Turn 1]\n",
      "User: What is MLflow for GenAI?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/07 15:25:09 WARNING mlflow.tracing.fluent: Found non-string values in metadata. Non-string values in metadata will automatically be stringified when the trace is logged. Non-string items: {'turn_number': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: MLflow for GenAI is the set of MLflow features designed to **build, evaluate, track, and deploy GenAI applications**‚Äîespecially **LLM-based** systems like chatbots, RAG pipelines, and agentic workflows‚Äîusing the same experiment tracking and lifecycle management approach MLflow provides for traditional ML.\n",
      "\n",
      "Key pieces in MLflow for GenAI typically include:\n",
      "\n",
      "- **Prompt & model tracking**: Log prompts, prompt templates, model/provider info (e.g., OpenAI, Anthropic, local models), parameters (temperature, max tokens), and outputs as part of MLflow runs so results are reproducible and comparable.\n",
      "- **Evaluation for LLM apps**: Run structured evaluations for tasks like QA, summarization, classification, etc., including:\n",
      "  - **Model-based / LLM-as-judge metrics** (e.g., relevance, correctness, groundedness)\n",
      "  - **Heuristic metrics** (e.g., exact match, ROUGE, toxicity checks)\n",
      "  - Dataset-driven comparisons across prompts/models\n",
      "- **Tracing / observability** (where supported): Capture traces of multi-step GenAI calls (retrieval ‚Üí prompt construction ‚Üí LLM call ‚Üí post-processing) to debug latency, failures, and quality regressions.\n",
      "- **Packaging & deployment**: Package GenAI apps (including chains) and deploy them via MLflow‚Äôs model serving mechanisms, with versioning and a registry workflow.\n",
      "- **Governance via Model Registry**: Register and version GenAI app artifacts, promote through stages, and manage approvals‚Äîsimilar to classic ML models.\n",
      "\n",
      "If you tell me your stack (Databricks vs OSS MLflow, which LLM providers, RAG/agents or simple prompting), I can point to the exact MLflow components and a recommended workflow.\n",
      "\n",
      "[Turn 2]\n",
      "User: What are its main GenAI main components?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/07 15:25:14 WARNING mlflow.tracing.fluent: Found non-string values in metadata. Non-string values in metadata will automatically be stringified when the trace is logged. Non-string items: {'turn_number': 3}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Main MLflow-for-GenAI components (as they‚Äôre commonly used today) are:\n",
      "\n",
      "1) **Experiment Tracking (Runs)**\n",
      "- Log prompts/prompt templates, model/provider, decoding params (temperature, max_tokens), inputs/outputs, artifacts (e.g., retrieved docs), and metrics.\n",
      "\n",
      "2) **LLM Evaluation**\n",
      "- Evaluate prompts/models on datasets.\n",
      "- Supports task-oriented evaluators and metrics (including LLM-as-judge style metrics where configured), plus comparisons across runs.\n",
      "\n",
      "3) **Tracing / Observability**\n",
      "- Trace multi-step GenAI executions (e.g., RAG: retrieval ‚Üí rerank ‚Üí prompt ‚Üí LLM ‚Üí post-process).\n",
      "- Useful for debugging quality, latency, and failures; helps attribute issues to specific steps.\n",
      "\n",
      "4) **Model Packaging & Serving (GenAI apps)**\n",
      "- Package LLM pipelines/apps for reproducible execution.\n",
      "- Deploy/serve them via MLflow serving mechanisms (depending on your environment).\n",
      "\n",
      "5) **Model Registry (Governance)**\n",
      "- Version and register GenAI apps/models.\n",
      "- Manage stage transitions (e.g., Staging/Production), approvals, and lineage back to the runs/evaluations.\n",
      "\n",
      "If you share whether you‚Äôre on **Databricks MLflow** or **open-source MLflow**, I can map these to the exact APIs/UI features available in your setup.\n",
      "\n",
      "[Turn 3]\n",
      "User: Tell me more about the Tracing component.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/07 15:25:27 WARNING mlflow.tracing.fluent: Found non-string values in metadata. Non-string values in metadata will automatically be stringified when the trace is logged. Non-string items: {'turn_number': 4}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: MLflow Tracing is the observability layer for GenAI apps. It records a **trace** for each request (or run) and breaks it into **spans** that represent steps in your pipeline‚Äîso you can see *what happened*, *where time was spent*, and *which step produced which output*.\n",
      "\n",
      "### What it captures\n",
      "A trace typically includes:\n",
      "\n",
      "- **Spans (step-level events)** such as:\n",
      "  - LLM call(s) (provider/model, parameters, prompt/messages)\n",
      "  - Retrieval (query, top‚Äëk, returned document IDs/snippets)\n",
      "  - Reranking, tool/function calls, parsing, post-processing\n",
      "- **Inputs/outputs per span**\n",
      "  - Prompts/messages in, responses out\n",
      "  - Retrieved context, intermediate artifacts\n",
      "- **Timing and performance**\n",
      "  - Latency per span and end-to-end latency\n",
      "- **Metadata**\n",
      "  - Model name/version, temperature/max_tokens, request IDs, user/session IDs (if you add them), error details/stack traces\n",
      "\n",
      "### Why it‚Äôs useful\n",
      "- **Debugging quality**: Identify whether bad answers are due to retrieval (wrong docs), prompt formatting, tool output, or the LLM itself.\n",
      "- **Performance tuning**: Find slow steps (e.g., vector search vs. LLM) and optimize caching, batching, top‚Äëk, reranking, etc.\n",
      "- **Regression analysis**: Compare traces across versions to see what changed (prompt, retrieved docs, tool calls).\n",
      "- **Production troubleshooting**: Quickly pinpoint failures and timeouts and correlate them with specific providers/models.\n",
      "\n",
      "### How it fits with the rest of MLflow\n",
      "- Traces complement **runs** and **evaluation**:\n",
      "  - Use **evaluation** for aggregate quality metrics across a dataset.\n",
      "  - Use **tracing** to drill into *individual* requests to understand *why* a metric moved.\n",
      "- Traces can be tied to experiments/runs so you get lineage from a deployed app ‚Üí trace ‚Üí underlying configuration.\n",
      "\n",
      "### Typical workflow\n",
      "1. Instrument your app so each request creates a trace.\n",
      "2. Inspect traces for failures/low-quality outputs.\n",
      "3. Adjust retrieval/prompt/tooling.\n",
      "4. Re-run evaluation to confirm improvements.\n",
      "5. Use traces in production to monitor real traffic.\n",
      "\n",
      "### Practical notes / best practices\n",
      "- **Redaction & PII**: Don‚Äôt log sensitive user text or secrets; redact or hash where needed.\n",
      "- **Sampling**: Trace 100% in dev; sample in production to control cost/storage.\n",
      "- **Span naming conventions**: Keep consistent names like `retrieval`, `rerank`, `llm_call`, `tool_call`, `postprocess` to make comparisons easy.\n",
      "\n",
      "If you tell me what kind of app you have (RAG, agent with tools, simple chat) and your environment (Databricks vs OSS MLflow), I can show a concrete instrumentation pattern and what you‚Äôll see in the UI.\n",
      "\n",
      "[Turn 4]\n",
      "User: How does it compare to other tools?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/07 15:25:40 WARNING mlflow.tracing.fluent: Found non-string values in metadata. Non-string values in metadata will automatically be stringified when the trace is logged. Non-string items: {'turn_number': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: MLflow Tracing overlaps with ‚ÄúLLM observability‚Äù tools, but it‚Äôs positioned differently: it‚Äôs meant to **connect request-level traces to the MLflow lifecycle** (runs, evaluation, registry, deployment), rather than being a standalone observability product.\n",
      "\n",
      "## Comparison dimensions\n",
      "\n",
      "### 1) Scope: end-to-end ML lifecycle vs. observability-only\n",
      "- **MLflow Tracing**: Observability + tight integration with **experiment tracking**, **LLM evaluation**, and **model/app versioning** (Registry). Good when you want one system of record from dev ‚Üí eval ‚Üí deploy.\n",
      "- **Dedicated LLM observability tools** (e.g., LangSmith, Arize Phoenix, WhyLabs, Helicone, Humanloop, Weights & Biases Weave, etc.): Often go deeper on **production monitoring**, dataset curation, annotation workflows, prompt playgrounds, and collaboration features.\n",
      "\n",
      "### 2) Framework/vendor integration\n",
      "- **MLflow Tracing**: Best when you‚Äôre already using MLflow (especially in MLflow-centric platforms). Instrumentation typically focuses on capturing spans for LLM calls, retrieval, tools, etc.\n",
      "- **Others**: Some have ‚Äúfirst-class‚Äù integration with specific frameworks (LangChain/LlamaIndex) and may require less setup for those stacks, with richer out-of-the-box views tailored to them.\n",
      "\n",
      "### 3) What you get out of the box\n",
      "- **MLflow Tracing** tends to emphasize:\n",
      "  - A trace structure (trace + spans)\n",
      "  - Latency/error visibility\n",
      "  - Linking traces to runs/evals/versions\n",
      "- **Dedicated tools** often emphasize:\n",
      "  - Prompt/version playgrounds and A/B testing\n",
      "  - Production dashboards (rates, costs, token usage) and alerting\n",
      "  - Annotation/human feedback pipelines\n",
      "  - Advanced analytics across large volumes of traces\n",
      "\n",
      "### 4) Governance and reproducibility\n",
      "- **MLflow Tracing**: Strong story when you need **reproducibility** and **auditability**‚Äîe.g., ‚ÄúThis production response came from app version X, configured with prompt Y and retriever Z, evaluated on dataset D.‚Äù\n",
      "- **Others**: Can also provide lineage, but may not integrate as directly with your ML model registry and experiment history unless you adopt their ecosystem.\n",
      "\n",
      "### 5) Deployment environment fit\n",
      "- **If you‚Äôre already standardized on MLflow** (and especially if your org uses MLflow for tracking/registry): MLflow Tracing reduces tool sprawl and centralizes artifacts.\n",
      "- **If your primary need is production-grade LLM monitoring** (alerts, dashboards, feedback loops, cost tracking at scale): a specialized observability tool may be more mature, and MLflow can still be used for experiments/registry.\n",
      "\n",
      "## Practical guidance\n",
      "- Choose **MLflow Tracing** when:\n",
      "  - You want traces tied to **runs + evaluation + registry**.\n",
      "  - You‚Äôre optimizing RAG/agent pipelines and want a unified MLflow workflow.\n",
      "- Consider a **dedicated LLM observability tool** when:\n",
      "  - You need strong **production monitoring/alerting**, human feedback management, or very high-volume trace analytics.\n",
      "  - You want deep, framework-specific UX (e.g., LangChain-native).\n",
      "\n",
      "If you tell me which tool(s) you‚Äôre comparing against (LangSmith? Phoenix? Helicone? Weave?) and whether you care more about **dev/eval** or **production monitoring**, I can give a more pointed comparison.\n",
      "\n",
      "[Turn 5]\n",
      "User: What is the difference between Tracing and Tracking?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/07 15:25:48 WARNING mlflow.tracing.fluent: Found non-string values in metadata. Non-string values in metadata will automatically be stringified when the trace is logged. Non-string items: {'turn_number': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: **Tracking** and **Tracing** solve different problems in MLflow:\n",
      "\n",
      "## MLflow Tracking (Experiment Tracking)\n",
      "**Purpose:** Record and compare *experiments/versions* of a model or GenAI app.\n",
      "\n",
      "**What you log:**\n",
      "- **Parameters** (e.g., temperature, top_p, prompt version, retriever settings)\n",
      "- **Metrics** (e.g., accuracy, groundedness score, latency aggregates)\n",
      "- **Artifacts** (prompt templates, evaluation datasets, configs, model/app code)\n",
      "- **Run metadata** (tags like git SHA, data version)\n",
      "\n",
      "**Granularity:** Typically **one run = one configuration** (one experiment setting), often evaluated over many examples.\n",
      "\n",
      "**Primary use cases:**\n",
      "- Compare prompt/model variants\n",
      "- Reproduce results\n",
      "- Track offline evaluation outcomes\n",
      "- Register/promote a specific version to production\n",
      "\n",
      "## MLflow Tracing\n",
      "**Purpose:** Observe *individual executions/requests* of a GenAI app to understand behavior and performance.\n",
      "\n",
      "**What it records:**\n",
      "- A **trace** per request (or session), made of **spans** (steps)\n",
      "  - retrieval ‚Üí rerank ‚Üí prompt construction ‚Üí LLM call ‚Üí tool call ‚Üí postprocess\n",
      "- **Inputs/outputs at each step**, timing, errors, intermediate context (e.g., retrieved docs)\n",
      "\n",
      "**Granularity:** **Per request**, step-by-step.\n",
      "\n",
      "**Primary use cases:**\n",
      "- Debug why a specific answer was wrong\n",
      "- Find which step is slow or failing\n",
      "- Understand tool-call behavior and RAG context assembly\n",
      "- Production troubleshooting (often with sampling/redaction)\n",
      "\n",
      "## How they work together\n",
      "- Use **Tracking** to answer: *‚ÄúWhich prompt/model configuration is best overall?‚Äù*\n",
      "- Use **Tracing** to answer: *‚ÄúWhy did this specific request behave this way?‚Äù*\n",
      "\n",
      "A common pattern is:\n",
      "1) Track a run for a given app/prompt version and evaluate it on a dataset (Tracking + Evaluation)  \n",
      "2) Use traces from failing/interesting examples to diagnose issues (Tracing)  \n",
      "3) Iterate and track the next run\n",
      "\n",
      "If you share whether you mean **online production requests** or **offline evaluation runs**, I can suggest how to structure runs and traces so they link cleanly.\n",
      "\n",
      "[Turn 6]\n",
      "User: How do I get started with MLflow for GenAI?\n",
      "Agent: ## 1) Pick your setup\n",
      "- **Databricks**: MLflow is built-in; easiest path for GenAI evaluation + tracing + registry.\n",
      "- **Open-source MLflow**: install MLflow and configure a tracking server + artifact store.\n",
      "\n",
      "## 2) Start with a minimal ‚ÄúLLM app‚Äù and log it\n",
      "1. **Create an experiment**\n",
      "2. **Start a run**\n",
      "3. **Log**:\n",
      "   - Prompt template (or messages)\n",
      "   - Model/provider + generation params (temperature, max_tokens, etc.)\n",
      "   - Example inputs/outputs\n",
      "   - Any retrieval settings (top_k, embedding model, index name)\n",
      "\n",
      "This gives you reproducibility and a baseline to compare against.\n",
      "\n",
      "## 3) Add evaluation (offline, dataset-based)\n",
      "- Create a small evaluation dataset (e.g., 50‚Äì200 examples) with:\n",
      "  - input question / instruction\n",
      "  - optional reference answer\n",
      "  - optional context docs (for RAG)\n",
      "- Run MLflow‚Äôs **LLM evaluation** to compute quality metrics and compare prompt/model variants.\n",
      "- Log evaluation results to the same run(s) so you can rank variants.\n",
      "\n",
      "## 4) Add tracing (request-level observability)\n",
      "- Instrument your app so each request produces a **trace** with spans like:\n",
      "  - `retrieval` ‚Üí `prompt_build` ‚Üí `llm_call` ‚Üí `postprocess`\n",
      "- Use traces to debug failures found in evaluation (or from real traffic), and to measure latency per step.\n",
      "- Apply **redaction/sampling** if you‚Äôre tracing production inputs.\n",
      "\n",
      "## 5) Package and version the app\n",
      "- Package the GenAI app (prompt + code + dependencies + config) so it‚Äôs reproducible.\n",
      "- **Register** the app/version in the **MLflow Model Registry**.\n",
      "- Promote through stages (Staging ‚Üí Production) with lineage back to runs/evals/traces.\n",
      "\n",
      "## 6) Deploy and monitor\n",
      "- Deploy via your serving approach (MLflow serving / platform serving).\n",
      "- Keep tracing on (often sampled) and periodically re-run evaluation on fresh datasets.\n",
      "\n",
      "---\n",
      "\n",
      "### What I need from you to give a concrete starter template\n",
      "1) Are you on **Databricks** or **open-source MLflow**?  \n",
      "2) What are you building: **prompt-only**, **RAG**, or **agent/tools**?  \n",
      "3) Which model/provider (OpenAI/Anthropic/Azure/local)?  \n",
      "4) Do you want a **Python notebook example** or a **production service** pattern (FastAPI, etc.)?\n",
      "\n",
      "Reply with those and I‚Äôll give you a minimal working skeleton (tracking + eval + tracing) tailored to your stack.\n",
      "\n",
      "============================================================\n",
      "\n",
      "‚úÖ Conversation complete (Session: 75f8c0e9...)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-f35210baeedf8aba0a2373bfe99cd56a&amp;experiment_id=1&amp;trace_id=tr-0b65bf5bd3c54dbab7143f1685e9848e&amp;experiment_id=1&amp;trace_id=tr-e14969aac57c155bd41f04ff69cf4f10&amp;experiment_id=1&amp;trace_id=tr-72cd982ce24dc6ead4a0b3ae8b5cb4e2&amp;experiment_id=1&amp;trace_id=tr-10f1c7570925c87bd8354d0eb570a231&amp;experiment_id=1&amp;trace_id=tr-f253cffed9d473c80dbd7cd25f0c9f8c&amp;experiment_id=1&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-f35210baeedf8aba0a2373bfe99cd56a), Trace(trace_id=tr-0b65bf5bd3c54dbab7143f1685e9848e), Trace(trace_id=tr-e14969aac57c155bd41f04ff69cf4f10), Trace(trace_id=tr-72cd982ce24dc6ead4a0b3ae8b5cb4e2), Trace(trace_id=tr-10f1c7570925c87bd8354d0eb570a231), Trace(trace_id=tr-f253cffed9d473c80dbd7cd25f0c9f8c)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simulate a multi-turn conversation\n",
    "conv_agent = ConversationalAgent(client=client, model=AGENT_MODEL)\n",
    "\n",
    "conversation_turns = [\n",
    "    \"What is MLflow for GenAI?\",\n",
    "    \"What are its main GenAI main components?\",\n",
    "    \"Tell me more about the Tracing component.\",\n",
    "    \"How does it compare to other tools?\",\n",
    "    \"What is the difference between Tracing and Tracking?\",\n",
    "    \"How do I get started with MLflow for GenAI?\",\n",
    "]\n",
    "\n",
    "print(\"üó£Ô∏è Multi-Turn Conversation\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, user_msg in enumerate(conversation_turns, 1):\n",
    "    print(f\"\\n[Turn {i}]\")\n",
    "    print(f\"User: {user_msg}\")\n",
    "    response = conv_agent.chat(user_msg)\n",
    "    print(f\"Agent: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"\\n‚úÖ Conversation complete (Session: {conv_agent.session_id[:8]}...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: Session-Level Evaluation with DeepEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Found 6 traces for session 75f8c0e9...\n"
     ]
    }
   ],
   "source": [
    "# Search for traces from this session\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "session_traces = mlflow.search_traces(\n",
    "    locations=[experiment.experiment_id],\n",
    "    filter_string=f\"metadata.`mlflow.trace.session` = '{conv_agent.session_id}'\"\n",
    ")\n",
    "\n",
    "print(f\"üìä Found {len(session_traces)} traces for session {conv_agent.session_id[:8]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f031914b13334578a51afd6105d0e6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/7 [Elapsed: 00:00, Remaining: ?] "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <title>Evaluation output</title>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <style>\n",
       "        body {\n",
       "            font-family: Arial, sans-serif;\n",
       "        }\n",
       "\n",
       "        .header {\n",
       "            a.button {\n",
       "                padding: 4px 8px;\n",
       "                line-height: 20px;\n",
       "                box-shadow: none;\n",
       "                height: 20px;\n",
       "                display: inline-flex;\n",
       "                align-items: center;\n",
       "                justify-content: center;\n",
       "                vertical-align: middle;\n",
       "                background-color: rgb(34, 114, 180);\n",
       "                color: rgb(255, 255, 255);\n",
       "                text-decoration: none;\n",
       "                animation-duration: 0s;\n",
       "                transition: none 0s ease 0s;\n",
       "                position: relative;\n",
       "                white-space: nowrap;\n",
       "                text-align: center;\n",
       "                border: 1px solid rgb(192, 205, 216);\n",
       "                cursor: pointer;\n",
       "                user-select: none;\n",
       "                touch-action: manipulation;\n",
       "                border-radius: 4px;\n",
       "                gap: 6px;\n",
       "            }\n",
       "\n",
       "            a.button:hover {\n",
       "                background-color: rgb(14, 83, 139) !important;\n",
       "                border-color: transparent !important;\n",
       "                color: rgb(255, 255, 255) !important;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .warnings-section {\n",
       "            margin-top: 8px;\n",
       "\n",
       "            ul {\n",
       "                list-style-type: none;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .instructions-section {\n",
       "            margin-top: 16px;\n",
       "            font-size: 14px;\n",
       "\n",
       "            ul {\n",
       "                margin-top: 0;\n",
       "                margin-bottom: 0;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        code {\n",
       "            font-family: monospace;\n",
       "        }\n",
       "\n",
       "        .note {\n",
       "            color: #666;\n",
       "        }\n",
       "\n",
       "        a {\n",
       "            color: #2272B4;\n",
       "            text-decoration: none;\n",
       "        }\n",
       "\n",
       "        a:hover {\n",
       "            color: #005580;\n",
       "        }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "    <div class=\"header\">\n",
       "        <a href=\"http://localhost:5000/#/experiments/1/evaluation-runs?selectedRunUuid=1655086ca4e545de829edb54c4d112b0\" class=\"button\">\n",
       "            View evaluation results in MLflow\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1em\" height=\"1em\" fill=\"none\" viewBox=\"0 0 16 16\" aria-hidden=\"true\" focusable=\"false\" class=\"\">\n",
       "                <path fill=\"currentColor\" d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"></path>\n",
       "                <path fill=\"currentColor\" d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"></path>\n",
       "            </svg>\n",
       "        </a>\n",
       "    </div>\n",
       "</div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ DeepEval session evaluation complete!\n",
      "\n",
      "DeepEval Metrics:\n",
      "----------------------------------------\n",
      "  Toxicity/mean: 1.000\n",
      "  ConversationCompleteness/mean: 1.000\n",
      "  KnowledgeRetention/mean: 1.000\n",
      "  TopicAdherence/mean: 1.000\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"DeepEval\") as run:\n",
    "    deepeval_results = mlflow.genai.evaluate(\n",
    "        data= session_traces,\n",
    "        scorers=[\n",
    "            completeness_scorer,\n",
    "            retention_scorer,\n",
    "            topic_scorer,\n",
    "            toxicity_scorer\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(\"\\n‚úÖ DeepEval session evaluation complete!\")\n",
    "print(\"\\nDeepEval Metrics:\")\n",
    "print(\"-\" * 40)\n",
    "for metric_name, value in deepeval_results.metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric_name}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {metric_name}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11: Viewing Evaluation Results in MLflow UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
      "‚ïë         Viewing Evaluation Results in MLflow UI              ‚ïë\n",
      "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
      "\n",
      "üîç EXPERIMENTS VIEW:\n",
      "   Navigate to: http://localhost:5000\n",
      "   Select: \"11-agent-evaluation\" experiment\n",
      "\n",
      "   You'll see:\n",
      "   - Session-level and Evaluation runs with scorer metrics\n",
      "   - Pass/fail rates for each scorer\n",
      "   - Detailed reasoning from LLM judges\n",
      "\n",
      "üìä EVALUATION RESULTS:\n",
      "   Each run includes:\n",
      "   - Metric values (0-1 scores or boolean)\n",
      "   - Judge reasoning explanations\n",
      "   - Input/output pairs\n",
      "   - Artifacts with detailed results\n",
      "\n",
      "üéØ KEY METRICS TO MONITOR:\n",
      "\n",
      "   Built-in Scorers:\n",
      "   - relevance_to_query/score: Response relevance\n",
      "   - correctness/score: Factual accuracy\n",
      "   - safety/score: Safety compliance\n",
      "   - guidelines/score: Guideline adherence\n",
      "\n",
      "   Custom Scorers:\n",
      "   - response_length_check: Length validation\n",
      "   - contains_keywords: Keyword presence\n",
      "   - no_hallucination_markers: Confidence check\n",
      "\n",
      "   DeepEval Scorers:\n",
      "   - conversation_completeness/score: Goal achievement\n",
      "   - knowledge_retention/score: Context memory\n",
      "   - topic_adherence/score: Topic focus\n",
      "\n",
      "üí° TIPS:\n",
      "   1. Compare multiple evaluation runs\n",
      "   2. Filter by scorer to find failures\n",
      "   3. Read judge reasoning for insights\n",
      "   4. Track metrics over time for regression detection\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë         Viewing Evaluation Results in MLflow UI              ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "üîç EXPERIMENTS VIEW:\n",
    "   Navigate to: http://localhost:5000\n",
    "   Select: \"11-agent-evaluation\" experiment\n",
    "   \n",
    "   You'll see:\n",
    "   - Session-level and Evaluation runs with scorer metrics\n",
    "   - Pass/fail rates for each scorer\n",
    "   - Detailed reasoning from LLM judges\n",
    "\n",
    "üìä EVALUATION RESULTS:\n",
    "   Each run includes:\n",
    "   - Metric values (0-1 scores or boolean)\n",
    "   - Judge reasoning explanations\n",
    "   - Input/output pairs\n",
    "   - Artifacts with detailed results\n",
    "\n",
    "üéØ KEY METRICS TO MONITOR:\n",
    "   \n",
    "   Built-in Scorers:\n",
    "   - relevance_to_query/score: Response relevance\n",
    "   - correctness/score: Factual accuracy\n",
    "   - safety/score: Safety compliance\n",
    "   - guidelines/score: Guideline adherence\n",
    "   \n",
    "   Custom Scorers:\n",
    "   - response_length_check: Length validation\n",
    "   - contains_keywords: Keyword presence\n",
    "   - no_hallucination_markers: Confidence check\n",
    "   \n",
    "   DeepEval Scorers:\n",
    "   - conversation_completeness/score: Goal achievement\n",
    "   - knowledge_retention/score: Context memory\n",
    "   - topic_adherence/score: Topic focus\n",
    "\n",
    "üí° TIPS:\n",
    "   1. Compare multiple evaluation runs\n",
    "   2. Filter by scorer to find failures\n",
    "   3. Read judge reasoning for insights\n",
    "   4. Track metrics over time for regression detection\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to evaluate agents and LLM applications using MLflow's evaluation framework.\n",
    "\n",
    "### ‚úÖ What You Learned\n",
    "\n",
    "**Built-in Scorers:**\n",
    "- `RelevanceToQuery` - Check if responses are relevant\n",
    "- `Correctness` - Verify factual accuracy\n",
    "- `Guidelines` - Enforce custom guidelines\n",
    "- `Safety` - Ensure safe outputs\n",
    "\n",
    "**Custom Scorers:**\n",
    "- Use `@scorer` decorator for custom logic\n",
    "- Access `outputs` and `expectations` in scorers\n",
    "- Return boolean or numeric scores\n",
    "\n",
    "**DeepEval Integration:**\n",
    "- `ConversationCompleteness` - Goal achievement\n",
    "- `KnowledgeRetention` - Context memory\n",
    "- `TopicAdherence` - Topic focus\n",
    "\n",
    "**Session-Level Evaluation:**\n",
    "- Tag traces with session IDs\n",
    "- Search traces by session\n",
    "- Evaluate multi-turn conversations\n",
    "\n",
    "### üîë Key Patterns\n",
    "\n",
    "```python\n",
    "# Built-in scorer\n",
    "from mlflow.genai.scorers import RelevanceToQuery\n",
    "scorer = RelevanceToQuery(model=\"openai:/gpt-4o-mini\")\n",
    "\n",
    "# Custom scorer\n",
    "from mlflow.genai import scorer\n",
    "@scorer\n",
    "def my_scorer(outputs) -> bool:\n",
    "    return condition\n",
    "\n",
    "# Run evaluation\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=session_tracew,\n",
    "    scorers=[scorer1, scorer2]\n",
    ")\n",
    "```\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "Continue to **Tutorial 1.8: Complete RAG Application** to build a full RAG system with RAGAS evaluation metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
