{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1.7: Evaluating Agents with MLflow\n",
    "\n",
    "## LLM-as-Judge Evaluation for GenAI Applications\n",
    "\n",
    "This notebook teaches you how to systematically evaluate agents and LLM applications using MLflow's evaluation framework. You'll learn to use built-in judges, create custom scorers, and integrate third-party evaluation libraries like DeepEval.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- âœ… Why agent evaluation matters\n",
    "- âœ… MLflow built-in scorers (RelevanceToQuery, Correctness, Guidelines, Safety)\n",
    "- âœ… Creating custom scorers with the `@scorer` decorator\n",
    "- âœ… DeepEval integration for conversational evaluation\n",
    "- âœ… Session-level multi-turn evaluation\n",
    "\n",
    "### Prerequisites\n",
    "- Completed notebooks 1.1-1.6\n",
    "- Understanding of MLflow tracing\n",
    "\n",
    "### Estimated Time: 25-30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Why Evaluate Agents?\n",
    "\n",
    "Agent evaluation is critical for:\n",
    "\n",
    "| Challenge | Solution |\n",
    "|-----------|----------|\n",
    "| Non-deterministic outputs | Statistical evaluation over multiple runs |\n",
    "| Complex reasoning chains | Step-by-step quality assessment |\n",
    "| Multi-turn conversations | Session-level coherence metrics |\n",
    "| Safety and compliance | Automated guardrail checking |\n",
    "| Regression detection | Baseline comparisons |\n",
    "\n",
    "### LLM-as-Judge Pattern\n",
    "\n",
    "Instead of brittle string matching, we use LLMs to evaluate LLM outputs:\n",
    "\n",
    "```\n",
    "Agent Output â†’ Judge LLM â†’ Score + Reasoning\n",
    "```\n",
    "\n",
    "MLflow provides:\n",
    "1. **Built-in scorers** - Pre-configured judges for common metrics\n",
    "2. **Custom scorers** - Define your own evaluation logic\n",
    "3. **Third-party integrations** - DeepEval, RAGAS, and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri(\"./mlruns\")\n",
    "mlflow.set_experiment(\"11-agent-evaluation\")\n",
    "\n",
    "# Enable autologging\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Judge model for evaluation\n",
    "JUDGE_MODEL = \"openai:/gpt-4o-mini\"\n",
    "\n",
    "print(\"âœ… Environment configured for agent evaluation\")\n",
    "print(f\"   MLflow tracking: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"   Experiment: 11-agent-evaluation\")\n",
    "print(f\"   Judge model: {JUDGE_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Create a Simple Agent for Evaluation\n",
    "\n",
    "Let's create a simple Q&A agent that we'll evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleQAAgent:\n",
    "    \"\"\"\n",
    "    A simple Q&A agent for demonstration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-4o-mini\"):\n",
    "        self.model = model\n",
    "        self.client = OpenAI()\n",
    "    \n",
    "    @mlflow.trace(name=\"qa_agent\", span_type=\"AGENT\")\n",
    "    def answer(self, question: str) -> str:\n",
    "        \"\"\"Answer a question.\"\"\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant. Provide concise, accurate answers.\"},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "# Initialize agent\n",
    "agent = SimpleQAAgent()\n",
    "\n",
    "# Test the agent\n",
    "test_response = agent.answer(\"What is MLflow?\")\n",
    "print(f\"Test response: {test_response[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: MLflow Built-in Scorers\n",
    "\n",
    "MLflow provides pre-configured scorers for common evaluation needs:\n",
    "\n",
    "| Scorer | Purpose | Inputs |\n",
    "|--------|---------|--------|\n",
    "| `RelevanceToQuery` | Is the response relevant to the question? | inputs, outputs |\n",
    "| `Correctness` | Is the response factually correct? | outputs, expectations |\n",
    "| `Guidelines` | Does the response follow specific guidelines? | outputs |\n",
    "| `Safety` | Is the response safe and appropriate? | outputs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import (\n",
    "    RelevanceToQuery,\n",
    "    Correctness,\n",
    "    Guidelines,\n",
    "    Safety\n",
    ")\n",
    "\n",
    "# Initialize built-in scorers\n",
    "relevance_scorer = RelevanceToQuery(model=JUDGE_MODEL)\n",
    "correctness_scorer = Correctness(model=JUDGE_MODEL)\n",
    "safety_scorer = Safety(model=JUDGE_MODEL)\n",
    "\n",
    "# Guidelines scorer with custom guidelines\n",
    "guidelines_scorer = Guidelines(\n",
    "    model=JUDGE_MODEL,\n",
    "    guidelines=[\n",
    "        \"Responses should be concise (under 100 words)\",\n",
    "        \"Responses should not include harmful content\",\n",
    "        \"Responses should be helpful and informative\"\n",
    "    ],\n",
    "    name=\"custom_guidelines\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Built-in scorers initialized:\")\n",
    "print(\"   - RelevanceToQuery\")\n",
    "print(\"   - Correctness\")\n",
    "print(\"   - Safety\")\n",
    "print(\"   - Guidelines (custom)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Create Evaluation Dataset\n",
    "\n",
    "An evaluation dataset contains:\n",
    "- `inputs`: The questions/prompts sent to the agent\n",
    "- `expectations` (optional): Expected answers for correctness checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation dataset\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is MLflow?\"},\n",
    "        \"expectations\": {\"expected_answer\": \"MLflow is an open source platform for managing the machine learning lifecycle, including experiment tracking, model packaging, and deployment.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What are the main components of MLflow?\"},\n",
    "        \"expectations\": {\"expected_answer\": \"MLflow has four main components: Tracking, Projects, Models, and Model Registry.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How does MLflow help with experiment tracking?\"},\n",
    "        \"expectations\": {\"expected_answer\": \"MLflow Tracking allows you to log parameters, metrics, artifacts, and models during ML experiments, enabling comparison and reproducibility.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is MLflow Tracing used for?\"},\n",
    "        \"expectations\": {\"expected_answer\": \"MLflow Tracing provides observability for GenAI applications by capturing LLM calls, retrieval steps, and agent reasoning with full visibility.\"}\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"âœ… Evaluation dataset created with {len(eval_dataset)} examples\")\n",
    "for i, item in enumerate(eval_dataset, 1):\n",
    "    print(f\"   {i}. {item['inputs']['question'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Prediction function wrapper for evaluation.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Dictionary with 'question' key\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with 'response' key\n",
    "    \"\"\"\n",
    "    question = inputs[\"question\"]\n",
    "    response = agent.answer(question)\n",
    "    return {\"response\": response}\n",
    "\n",
    "print(\"âœ… Prediction function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Run Evaluation with Built-in Scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "print(\"ðŸ”„ Running evaluation with built-in scorers...\\n\")\n",
    "\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=predict_fn,\n",
    "    scorers=[\n",
    "        relevance_scorer,\n",
    "        correctness_scorer,\n",
    "        safety_scorer,\n",
    "        guidelines_scorer\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Evaluation complete!\")\n",
    "print(f\"\\nMetrics Summary:\")\n",
    "print(\"-\" * 40)\n",
    "for metric_name, value in results.metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric_name}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {metric_name}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View detailed results\n",
    "print(\"\\nðŸ“Š Detailed Results:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display the results table\n",
    "results.tables[\"eval_results\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Custom Scorers with @scorer Decorator\n",
    "\n",
    "Create your own evaluation logic using the `@scorer` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai import scorer\n",
    "from mlflow.genai.scorers import Scorer\n",
    "\n",
    "@scorer\n",
    "def response_length_check(outputs: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Check if response is within acceptable length.\n",
    "    Returns True if response is between 20 and 500 characters.\n",
    "    \"\"\"\n",
    "    response = outputs.get(\"response\", \"\")\n",
    "    length = len(response)\n",
    "    return 20 <= length <= 500\n",
    "\n",
    "@scorer\n",
    "def contains_keywords(outputs: dict, expectations: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Check if response contains key terms from expected answer.\n",
    "    \"\"\"\n",
    "    response = outputs.get(\"response\", \"\").lower()\n",
    "    expected = expectations.get(\"expected_answer\", \"\").lower()\n",
    "    \n",
    "    # Extract key words (simple approach)\n",
    "    key_words = [word for word in expected.split() if len(word) > 4]\n",
    "    \n",
    "    # Check if at least 30% of key words are present\n",
    "    if not key_words:\n",
    "        return True\n",
    "    \n",
    "    matches = sum(1 for word in key_words if word in response)\n",
    "    return matches / len(key_words) >= 0.3\n",
    "\n",
    "@scorer\n",
    "def no_hallucination_markers(outputs: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Check for common hallucination markers.\n",
    "    \"\"\"\n",
    "    response = outputs.get(\"response\", \"\").lower()\n",
    "    \n",
    "    hallucination_markers = [\n",
    "        \"i think\",\n",
    "        \"i believe\",\n",
    "        \"probably\",\n",
    "        \"might be\",\n",
    "        \"i'm not sure\",\n",
    "        \"as far as i know\"\n",
    "    ]\n",
    "    \n",
    "    return not any(marker in response for marker in hallucination_markers)\n",
    "\n",
    "print(\"âœ… Custom scorers defined:\")\n",
    "print(\"   - response_length_check\")\n",
    "print(\"   - contains_keywords\")\n",
    "print(\"   - no_hallucination_markers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation with custom scorers\n",
    "print(\"ðŸ”„ Running evaluation with custom scorers...\\n\")\n",
    "\n",
    "custom_results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=predict_fn,\n",
    "    scorers=[\n",
    "        response_length_check,\n",
    "        contains_keywords,\n",
    "        no_hallucination_markers\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Custom evaluation complete!\")\n",
    "print(f\"\\nCustom Metrics Summary:\")\n",
    "print(\"-\" * 40)\n",
    "for metric_name, value in custom_results.metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric_name}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {metric_name}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: DeepEval Integration\n",
    "\n",
    "MLflow integrates with DeepEval for advanced conversational AI evaluation metrics.\n",
    "\n",
    "| DeepEval Scorer | Purpose |\n",
    "|-----------------|----------|\n",
    "| `ConversationCompleteness` | Did the conversation achieve its goal? |\n",
    "| `KnowledgeRetention` | Does the agent remember context? |\n",
    "| `TopicAdherence` | Does the agent stay on topic? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers.deepeval import (\n",
    "    ConversationCompleteness,\n",
    "    KnowledgeRetention,\n",
    "    TopicAdherence\n",
    ")\n",
    "\n",
    "# Initialize DeepEval scorers\n",
    "completeness_scorer = ConversationCompleteness(model=JUDGE_MODEL)\n",
    "retention_scorer = KnowledgeRetention(model=JUDGE_MODEL)\n",
    "topic_scorer = TopicAdherence(\n",
    "    model=JUDGE_MODEL,\n",
    "    allowed_topics=[\"MLflow\", \"machine learning\", \"AI\", \"data science\", \"experiment tracking\"]\n",
    ")\n",
    "\n",
    "print(\"âœ… DeepEval scorers initialized:\")\n",
    "print(\"   - ConversationCompleteness\")\n",
    "print(\"   - KnowledgeRetention\")\n",
    "print(\"   - TopicAdherence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Multi-Turn Conversation Agent\n",
    "\n",
    "For session-level evaluation, we need an agent that handles conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "class ConversationalAgent:\n",
    "    \"\"\"\n",
    "    An agent that maintains conversation history for multi-turn interactions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-4o-mini\"):\n",
    "        self.model = model\n",
    "        self.client = OpenAI()\n",
    "        self.conversation_history = []\n",
    "        self.session_id = str(uuid.uuid4())\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset conversation history and start new session.\"\"\"\n",
    "        self.conversation_history = []\n",
    "        self.session_id = str(uuid.uuid4())\n",
    "    \n",
    "    @mlflow.trace(name=\"conversational_agent\", span_type=\"AGENT\")\n",
    "    def chat(self, user_message: str) -> str:\n",
    "        \"\"\"\n",
    "        Send a message and get a response, maintaining history.\n",
    "        \"\"\"\n",
    "        # Tag trace with session ID for grouping\n",
    "        mlflow.update_current_trace(metadata={\n",
    "            \"mlflow.trace.session\": self.session_id,\n",
    "            \"turn_number\": len(self.conversation_history) // 2 + 1\n",
    "        })\n",
    "        \n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message\n",
    "        })\n",
    "        \n",
    "        # Prepare messages with system prompt\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful MLflow expert assistant. Answer questions about MLflow clearly and concisely. Remember previous context in the conversation.\"}\n",
    "        ] + self.conversation_history\n",
    "        \n",
    "        # Get response\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        assistant_message = response.choices[0].message.content\n",
    "        \n",
    "        # Add to history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": assistant_message\n",
    "        })\n",
    "        \n",
    "        return assistant_message\n",
    "\n",
    "print(\"âœ… ConversationalAgent defined with session tracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a multi-turn conversation\n",
    "conv_agent = ConversationalAgent()\n",
    "\n",
    "conversation_turns = [\n",
    "    \"What is MLflow?\",\n",
    "    \"What are its main components?\",\n",
    "    \"Tell me more about the Tracking component.\",\n",
    "    \"How does it compare to other tools?\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ—£ï¸ Multi-Turn Conversation\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, user_msg in enumerate(conversation_turns, 1):\n",
    "    print(f\"\\n[Turn {i}]\")\n",
    "    print(f\"User: {user_msg}\")\n",
    "    response = conv_agent.chat(user_msg)\n",
    "    print(f\"Agent: {response[:200]}...\" if len(response) > 200 else f\"Agent: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"\\nâœ… Conversation complete (Session: {conv_agent.session_id[:8]}...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: Session-Level Evaluation with DeepEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for traces from this session\n",
    "session_traces = mlflow.search_traces(\n",
    "    filter_string=f\"metadata.`mlflow.trace.session` = '{conv_agent.session_id}'\"\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“Š Found {len(session_traces)} traces for session {conv_agent.session_id[:8]}...\")\n",
    "\n",
    "# Display trace info\n",
    "for i, trace in enumerate(session_traces.itertuples(), 1):\n",
    "    print(f\"   Turn {i}: {trace.request_id[:8]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare conversation data for DeepEval evaluation\n",
    "conversation_eval_data = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"messages\": conv_agent.conversation_history\n",
    "        },\n",
    "        \"expectations\": {\n",
    "            \"goal\": \"Help the user understand MLflow and its components\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define a predict function that returns the conversation\n",
    "def conversation_predict_fn(inputs: dict) -> dict:\n",
    "    \"\"\"Return the conversation as-is for evaluation.\"\"\"\n",
    "    return {\"messages\": inputs[\"messages\"]}\n",
    "\n",
    "print(\"ðŸ”„ Running session-level evaluation with DeepEval scorers...\\n\")\n",
    "\n",
    "deepeval_results = mlflow.genai.evaluate(\n",
    "    data=conversation_eval_data,\n",
    "    predict_fn=conversation_predict_fn,\n",
    "    scorers=[\n",
    "        completeness_scorer,\n",
    "        retention_scorer,\n",
    "        topic_scorer\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… DeepEval session evaluation complete!\")\n",
    "print(f\"\\nDeepEval Metrics:\")\n",
    "print(\"-\" * 40)\n",
    "for metric_name, value in deepeval_results.metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric_name}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {metric_name}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11: Viewing Evaluation Results in MLflow UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘         Viewing Evaluation Results in MLflow UI              â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ðŸ” EXPERIMENTS VIEW:\n",
    "   Navigate to: http://localhost:5000\n",
    "   Select: \"11-agent-evaluation\" experiment\n",
    "   \n",
    "   You'll see:\n",
    "   - Evaluation runs with scorer metrics\n",
    "   - Pass/fail rates for each scorer\n",
    "   - Detailed reasoning from LLM judges\n",
    "\n",
    "ðŸ“Š EVALUATION RESULTS:\n",
    "   Each run includes:\n",
    "   - Metric values (0-1 scores or boolean)\n",
    "   - Judge reasoning explanations\n",
    "   - Input/output pairs\n",
    "   - Artifacts with detailed results\n",
    "\n",
    "ðŸŽ¯ KEY METRICS TO MONITOR:\n",
    "   \n",
    "   Built-in Scorers:\n",
    "   - relevance_to_query/score: Response relevance\n",
    "   - correctness/score: Factual accuracy\n",
    "   - safety/score: Safety compliance\n",
    "   - guidelines/score: Guideline adherence\n",
    "   \n",
    "   Custom Scorers:\n",
    "   - response_length_check: Length validation\n",
    "   - contains_keywords: Keyword presence\n",
    "   - no_hallucination_markers: Confidence check\n",
    "   \n",
    "   DeepEval Scorers:\n",
    "   - conversation_completeness/score: Goal achievement\n",
    "   - knowledge_retention/score: Context memory\n",
    "   - topic_adherence/score: Topic focus\n",
    "\n",
    "ðŸ’¡ TIPS:\n",
    "   1. Compare multiple evaluation runs\n",
    "   2. Filter by scorer to find failures\n",
    "   3. Read judge reasoning for insights\n",
    "   4. Track metrics over time for regression detection\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to evaluate agents and LLM applications using MLflow's evaluation framework.\n",
    "\n",
    "### âœ… What You Learned\n",
    "\n",
    "**Built-in Scorers:**\n",
    "- `RelevanceToQuery` - Check if responses are relevant\n",
    "- `Correctness` - Verify factual accuracy\n",
    "- `Guidelines` - Enforce custom guidelines\n",
    "- `Safety` - Ensure safe outputs\n",
    "\n",
    "**Custom Scorers:**\n",
    "- Use `@scorer` decorator for custom logic\n",
    "- Access `outputs` and `expectations` in scorers\n",
    "- Return boolean or numeric scores\n",
    "\n",
    "**DeepEval Integration:**\n",
    "- `ConversationCompleteness` - Goal achievement\n",
    "- `KnowledgeRetention` - Context memory\n",
    "- `TopicAdherence` - Topic focus\n",
    "\n",
    "**Session-Level Evaluation:**\n",
    "- Tag traces with session IDs\n",
    "- Search traces by session\n",
    "- Evaluate multi-turn conversations\n",
    "\n",
    "### ðŸ”‘ Key Patterns\n",
    "\n",
    "```python\n",
    "# Built-in scorer\n",
    "from mlflow.genai.scorers import RelevanceToQuery\n",
    "scorer = RelevanceToQuery(model=\"openai:/gpt-4o-mini\")\n",
    "\n",
    "# Custom scorer\n",
    "from mlflow.genai import scorer\n",
    "@scorer\n",
    "def my_scorer(outputs) -> bool:\n",
    "    return condition\n",
    "\n",
    "# Run evaluation\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=predict_fn,\n",
    "    scorers=[scorer1, scorer2]\n",
    ")\n",
    "```\n",
    "\n",
    "### ðŸ“š Next Steps\n",
    "\n",
    "Continue to **Tutorial 1.8: Complete RAG Application** to build a full RAG system with RAGAS evaluation metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
