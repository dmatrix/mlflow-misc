{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1.7: Evaluating Agents with MLflow\n",
    "\n",
    "## LLM-as-Judge Evaluation for GenAI Applications\n",
    "\n",
    "This notebook teaches you how to systematically evaluate agents and LLM applications using MLflow's evaluation framework. You'll learn to use built-in judges, create custom scorers, and integrate third-party evaluation libraries like DeepEval.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- ‚úÖ Why agent evaluation matters\n",
    "- ‚úÖ MLflow built-in scorers (RelevanceToQuery, Correctness, Guidelines, Safety)\n",
    "- ‚úÖ Creating custom scorers with the `@scorer` decorator\n",
    "- ‚úÖ DeepEval integration for conversational evaluation\n",
    "- ‚úÖ Session-level multi-turn evaluation\n",
    "\n",
    "### Prerequisites\n",
    "- Completed notebooks 1.1-1.6\n",
    "- Understanding of MLflow tracing\n",
    "\n",
    "### Estimated Time: 25-30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Why Evaluate Agents?\n",
    "\n",
    "Agent evaluation is critical for:\n",
    "\n",
    "| Challenge | Solution |\n",
    "|-----------|----------|\n",
    "| Non-deterministic outputs | Statistical evaluation over multiple runs |\n",
    "| Complex reasoning chains | Step-by-step quality assessment |\n",
    "| Multi-turn conversations | Session-level coherence metrics |\n",
    "| Safety and compliance | Automated guardrail checking |\n",
    "| Regression detection | Baseline comparisons |\n",
    "\n",
    "### LLM-as-Judge Pattern\n",
    "\n",
    "Instead of brittle string matching, we use LLMs to evaluate LLM outputs:\n",
    "\n",
    "```\n",
    "Agent Output ‚Üí Judge LLM ‚Üí Score + Reasoning\n",
    "```\n",
    "\n",
    "![LLM as a judge concept](images/llm-as-judge.png)\n",
    "\n",
    "MLflow provides:\n",
    "1. **Built-in scorers** - Pre-configured judges for common metrics\n",
    "2. **Custom scorers** - Define your own evaluation logic\n",
    "3. **Third-party integrations** - DeepEval, RAGAS, Phoenix, and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment configured for agent evaluation\n",
      "use_databricks_provider: False\n",
      "MLflow tracking: http://localhost:5000\n",
      "Experiment: 11-agent-evaluation\n",
      "Agent model : gpt-5.2\n",
      "Judge model : gpt-5.2\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "from utils.clnt_utils import is_databricks_ai_gateway_client, get_databricks_ai_gateway_client, get_openai_client, get_ai_gateway_model_names\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Configure MLflow\n",
    "EXPERIMENT_NAME = \"11-agent-evaluation\"\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "#Check if we are using a Databricks AI Gateway client\n",
    "use_databricks_provider = is_databricks_ai_gateway_client()\n",
    "if use_databricks_provider:\n",
    "    client = get_databricks_ai_gateway_client()\n",
    "    models = get_ai_gateway_model_names()\n",
    "    JUDGE_MODEL = models[2]\n",
    "    AGENT_MODEL = models[0]\n",
    "else:\n",
    "    # Initialize as an OpenAI client\n",
    "    client = get_openai_client()\n",
    "    JUDGE_MODEL = \"gpt-5.2\"\n",
    "    AGENT_MODEL = \"gpt-5.2\"\n",
    "\n",
    "# Enable autologging\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "print(\"‚úÖ Environment configured for agent evaluation\")\n",
    "print(f\"use_databricks_provider: {use_databricks_provider}\")\n",
    "print(f\"MLflow tracking: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"Experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"Agent model : {AGENT_MODEL}\")\n",
    "print(f\"Judge model : {JUDGE_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Create a Simple Agent for Evaluation\n",
    "\n",
    "Let's create a simple Q&A agent that we'll evaluate. This is a good first stepping stone to the next steps in which we use a number of built-in judges, followed by custom scorers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is MLflow for GenAI?\n",
      "\n",
      "Response: MLflow for GenAI is MLflow‚Äôs set of features for **building, tracking, evaluating, and deploying GenAI applications**‚Äîespecially those involving **LLMs**, **prompting**, **retrieval-augmented generation (RAG)**, and **agentic workflows**.\n",
      "\n",
      "Key capabilities typically include:\n",
      "\n",
      "- **Experiment tracking for GenAI runs**  \n",
      "  Log prompts, model/provider info, parameters (e.g., temperature), tool calls, retrieved context, and outputs‚Äîso LLM app behavior is reproducible and comparable.\n",
      "\n",
      "- **Prompt and artifact management**  \n",
      "  Version and store prompts/templates, system instructions, example sets, and other LLM app assets alongside runs.\n",
      "\n",
      "- **Evaluation tailored to LLMs**  \n",
      "  Run offline evaluations using built-in or custom metrics (e.g., relevance, groundedness, toxicity, faithfulness), including LLM-as-a-judge patterns and dataset-based evals.\n",
      "\n",
      "- **Model packaging and deployment**  \n",
      "  Package GenAI apps (including chains/RAG pipelines) into deployable ‚Äúmodels‚Äù with consistent inference APIs, then serve them via MLflow serving or integrate into your platform.\n",
      "\n",
      "- **Observability and traceability** (when used with tracing integrations)  \n",
      "  Capture step-by-step traces for multi-call workflows (RAG, tools, agents) to debug latency, failures, and quality regressions.\n",
      "\n",
      "If you tell me your stack (OpenAI/Anthropic, LangChain/LlamaIndex, Databricks, etc.), I can map the exact MLflow components you‚Äôd use (tracking, evaluation, tracing, deployment) for your GenAI app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-2cfdf759552f271911eae1f56da08197&amp;experiment_id=1&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-2cfdf759552f271911eae1f56da08197)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "class SimpleQAAgent:\n",
    "    \"\"\"\n",
    "    A simple Q&A agent for demonstration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, client:Any, model: str = \"gpt-5.2\"):\n",
    "        self.model = model\n",
    "        self.client = client\n",
    "    \n",
    "    @mlflow.trace(name=\"qa_agent\", span_type=\"AGENT\")\n",
    "    def answer(self, question: str) -> str:\n",
    "        \"\"\"Answer a question.\"\"\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\"\"You are a helpful Agent Assistant. Provide concise, accurate answers, \n",
    "                no hallucinations, with a focus on MLflow and GenAI.\"\"\"},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "# Initialize agent\n",
    "agent = SimpleQAAgent(client=client, model=AGENT_MODEL)\n",
    "\n",
    "# Test the agent with a single question\n",
    "test_question = \"What is MLflow for GenAI?\"\n",
    "test_response = agent.answer(test_question)\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"\\nResponse: {test_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Evaluation: Is This Response Good?\n",
    "\n",
    "Now that we have a response, how do we know if it's good? We could read it manually, but that doesn't scale. Instead, let's use MLflow's built-in `RelevanceToQuery` scorer to have an LLM judge evaluate the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/06 11:56:04 INFO mlflow.models.evaluation.utils.trace: Auto tracing is temporarily enabled during the model evaluation for computing some metrics and debugging. To disable tracing, call `mlflow.autolog(disable=True)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Evaluating the test response with RelevanceToQuery scorer...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5a2a4fce8640c0a41e60e1372ced3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [Elapsed: 00:00, Remaining: ?] "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <title>Evaluation output</title>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <style>\n",
       "        body {\n",
       "            font-family: Arial, sans-serif;\n",
       "        }\n",
       "\n",
       "        .header {\n",
       "            a.button {\n",
       "                padding: 4px 8px;\n",
       "                line-height: 20px;\n",
       "                box-shadow: none;\n",
       "                height: 20px;\n",
       "                display: inline-flex;\n",
       "                align-items: center;\n",
       "                justify-content: center;\n",
       "                vertical-align: middle;\n",
       "                background-color: rgb(34, 114, 180);\n",
       "                color: rgb(255, 255, 255);\n",
       "                text-decoration: none;\n",
       "                animation-duration: 0s;\n",
       "                transition: none 0s ease 0s;\n",
       "                position: relative;\n",
       "                white-space: nowrap;\n",
       "                text-align: center;\n",
       "                border: 1px solid rgb(192, 205, 216);\n",
       "                cursor: pointer;\n",
       "                user-select: none;\n",
       "                touch-action: manipulation;\n",
       "                border-radius: 4px;\n",
       "                gap: 6px;\n",
       "            }\n",
       "\n",
       "            a.button:hover {\n",
       "                background-color: rgb(14, 83, 139) !important;\n",
       "                border-color: transparent !important;\n",
       "                color: rgb(255, 255, 255) !important;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .warnings-section {\n",
       "            margin-top: 8px;\n",
       "\n",
       "            ul {\n",
       "                list-style-type: none;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .instructions-section {\n",
       "            margin-top: 16px;\n",
       "            font-size: 14px;\n",
       "\n",
       "            ul {\n",
       "                margin-top: 0;\n",
       "                margin-bottom: 0;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        code {\n",
       "            font-family: monospace;\n",
       "        }\n",
       "\n",
       "        .note {\n",
       "            color: #666;\n",
       "        }\n",
       "\n",
       "        a {\n",
       "            color: #2272B4;\n",
       "            text-decoration: none;\n",
       "        }\n",
       "\n",
       "        a:hover {\n",
       "            color: #005580;\n",
       "        }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "    <div class=\"header\">\n",
       "        <a href=\"http://localhost:5000/#/experiments/1/evaluation-runs?selectedRunUuid=b3411a7406594b30a2dca94728e799cc\" class=\"button\">\n",
       "            View evaluation results in MLflow\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1em\" height=\"1em\" fill=\"none\" viewBox=\"0 0 16 16\" aria-hidden=\"true\" focusable=\"false\" class=\"\">\n",
       "                <path fill=\"currentColor\" d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"></path>\n",
       "                <path fill=\"currentColor\" d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"></path>\n",
       "            </svg>\n",
       "        </a>\n",
       "    </div>\n",
       "</div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Quick Evaluation Result:\n",
      "----------------------------------------\n",
      "   Relevance Score: 1.0\n",
      "\n",
      "‚úÖ The LLM judge evaluated our agent's response!\n",
      "   Now let's learn about all the built-in scorers available...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mlflow.genai.scorers import RelevanceToQuery\n",
    "\n",
    "# Configure judge model URI based on provider\n",
    "if use_databricks_provider:\n",
    "    databricks_token = os.environ.get(\"DATABRICKS_TOKEN\")\n",
    "    ai_gateway_base_url = os.environ.get(\"AI_GATEWAY_BASE_URL\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = databricks_token\n",
    "    os.environ[\"OPENAI_API_BASE\"] = ai_gateway_base_url\n",
    "    judge_model_uri = f\"openai:/{JUDGE_MODEL}\"\n",
    "else:\n",
    "    judge_model_uri = f\"openai:/{JUDGE_MODEL}\"\n",
    "\n",
    "# Create a quick evaluation of the single test response\n",
    "quick_eval_data = [{\n",
    "    \"inputs\": {\"question\": test_question},\n",
    "    \"outputs\": {\"response\": test_response}  # Pre-computed output, no predict_fn needed\n",
    "}]\n",
    "\n",
    "# Evaluate with RelevanceToQuery scorer\n",
    "quick_scorer = RelevanceToQuery(model=judge_model_uri)\n",
    "\n",
    "print(\"üîÑ Evaluating the test response with RelevanceToQuery scorer...\\n\")\n",
    "\n",
    "quick_result = mlflow.genai.evaluate(\n",
    "    data=quick_eval_data,\n",
    "    scorers=[quick_scorer]\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "print(\"üìä Quick Evaluation Result:\")\n",
    "print(\"-\" * 40)\n",
    "score = quick_result.metrics.get(\"relevance_to_query/mean\", \"N/A\")\n",
    "print(f\"   Relevance Score: {score}\")\n",
    "print(\"\\n‚úÖ The LLM judge evaluated our agent's response!\")\n",
    "print(\"   Now let's learn about all the built-in scorers available...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: MLflow Built-in Scorers\n",
    "\n",
    "MLflow provides [pre-configured scorers](https://mlflow.org/docs/latest/genai/eval-monitor/scorers/llm-judge/predefined/) for common evaluation needs:\n",
    "\n",
    "| Scorer | Purpose | Inputs |\n",
    "|--------|---------|--------|\n",
    "| `RelevanceToQuery` | Is the response relevant to the question? | inputs, outputs |\n",
    "| `Correctness` | Is the response factually correct? | outputs, expectations |\n",
    "| `Guidelines` | Does the response follow specific guidelines? | outputs |\n",
    "| `Safety` | Is the response safe and appropriate? | outputs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Judge model URI: openai:/gpt-5.2\n",
      "\n",
      "‚úÖ Built-in scorers initialized:\n",
      "   - RelevanceToQuery, Correctness, Safety, Guidelines\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mlflow.genai.scorers import (\n",
    "    RelevanceToQuery,\n",
    "    Correctness,\n",
    "    Guidelines,\n",
    "    Safety\n",
    ")\n",
    "\n",
    "if use_databricks_provider:\n",
    "    databricks_token = os.environ.get(\"DATABRICKS_TOKEN\")\n",
    "    ai_gateway_base_url = os.environ.get(\"AI_GATEWAY_BASE_URL\")\n",
    "    \n",
    "    # Try configuring as OpenAI-compatible endpoint\n",
    "    os.environ[\"OPENAI_API_KEY\"] = databricks_token\n",
    "    os.environ[\"OPENAI_API_BASE\"] = ai_gateway_base_url\n",
    "    \n",
    "    judge_model_uri = f\"databricks:/{JUDGE_MODEL}\"\n",
    "    print(\"üîß Configured for Databricks AI Gateway\")\n",
    "    print(f\"   Base URL: {ai_gateway_base_url}\")\n",
    "    print(f\"   Model: {JUDGE_MODEL}\")\n",
    "else:\n",
    "    judge_model_uri = f\"openai:/{JUDGE_MODEL}\"\n",
    "\n",
    "print(f\"üîß Judge model URI: {judge_model_uri}\")\n",
    "\n",
    "# Initialize built-in scorers\n",
    "relevance_scorer = RelevanceToQuery(model=judge_model_uri)\n",
    "correctness_scorer = Correctness(model=judge_model_uri)\n",
    "safety_scorer = Safety(model=judge_model_uri)\n",
    "guidelines_scorer = Guidelines(\n",
    "    model=judge_model_uri,\n",
    "    guidelines=[ \"\"\"\n",
    "Response should be appropriately detailed:\n",
    "- Simple factual questions: < 200 words  \n",
    "- Technical how-to questions: < 500 words\n",
    "- Complex architectural questions: < 1000 words\n",
    "\"\"\"\n",
    "    ],\n",
    "    name=\"custom_guidelines\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Built-in scorers initialized:\")\n",
    "print(\"   - RelevanceToQuery, Correctness, Safety, Guidelines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Create Evaluation Dataset\n",
    "\n",
    "This comprises three steps:\n",
    " 1. Create an evaluation set that we will use in our evalution of the metrics to judge against the answer\n",
    " 2. Create a prediction function that will take question as in input, and return a LLM response to the question\n",
    " 3. run mlflow evaluation with all the scorers\n",
    "\n",
    "### Why Create an Evaluation Dataset?\n",
    "\n",
    "An evaluation dataset is the foundation of systematic agent testing. Without it, you're essentially \"eyeballing\" outputs‚Äîwhich doesn't scale and misses edge cases. The dataset enables **repeatable, automated evaluation** so you can detect regressions when you change prompts, models, or agent logic.\n",
    "\n",
    "### What Purpose Does It Serve?\n",
    "\n",
    "| Purpose | Benefit |\n",
    "|---------|---------|\n",
    "| **Benchmark Performance** | Measure how well your agent performs on known questions |\n",
    "| **Detect Regressions** | Catch quality drops when updating prompts or models |\n",
    "| **Compare Configurations** | A/B test different models, temperatures, or prompts |\n",
    "| **Validate Edge Cases** | Include tricky questions that previously caused failures |\n",
    "| **Document Expected Behavior** | Serve as living documentation of what your agent should do |\n",
    "\n",
    "### How Many Evaluation Pairs?\n",
    "\n",
    "**Rule of thumb: Start with 20-50 examples for development, scale to 100-500 for production.**\n",
    "\n",
    "- **Minimum viable**: 10-20 pairs to catch obvious issues\n",
    "- **Development**: 20-50 pairs covering main use cases\n",
    "- **Pre-production**: 50-100 pairs including edge cases\n",
    "- **Production monitoring**: 100-500+ pairs for statistical significance\n",
    "\n",
    "The dataset below uses 8 examples for tutorial brevity, but real evaluations need more coverage.\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "- `inputs`: The questions/prompts sent to the agent (dict with your input field names)\n",
    "- `expectations` (optional): Expected answers for correctness checking\n",
    "\n",
    "**Important:** Use `expected_response` as the field name in expectations‚Äîthis is what MLflow's `Correctness` scorer looks for. Alternative: use `expected_facts` for fact-based evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation dataset created with 8 examples\n",
      "\n",
      "üìã Questions cover:\n",
      "   - MLflow Tracing fundamentals\n",
      "   - Prompt management\n",
      "   - Span types and structure\n",
      "   - LLM evaluation methods\n",
      "   - LLM-as-Judge pattern\n",
      "   - Cost and token tracking\n",
      "   - Framework integrations\n",
      "   - Session-level observability\n"
     ]
    }
   ],
   "source": [
    "# Evaluation dataset focused on MLflow for GenAI and Agent Observability\n",
    "# NOTE: Use 'expected_response' - this is the field name that MLflow's Correctness scorer expects\n",
    "\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is MLflow Tracing and why is it important for GenAI applications?\"},\n",
    "        \"expectations\": {\"expected_response\": \"MLflow Tracing provides observability for GenAI applications by capturing the complete execution flow including LLM calls, retrieval steps, tool usage, and agent reasoning. It's important because it enables debugging, performance analysis, and understanding of complex AI pipelines.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How does MLflow help with prompt management in GenAI development?\"},\n",
    "        \"expectations\": {\"expected_response\": \"MLflow's Prompt Registry allows you to version control prompts, tag and search prompt versions, link prompts to experiments, and collaborate with teams. This ensures reproducibility and systematic prompt engineering.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What are spans in MLflow Tracing and what types are available?\"},\n",
    "        \"expectations\": {\"expected_response\": \"Spans are units of work captured during tracing. MLflow supports span types including LLM (for model calls), RETRIEVER (for RAG retrieval), TOOL (for function calls), AGENT (for agent orchestration), CHAIN (for sequential operations), and EMBEDDING (for vector operations).\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How can you evaluate LLM outputs using MLflow?\"},\n",
    "        \"expectations\": {\"expected_response\": \"MLflow provides an evaluation framework with built-in scorers like RelevanceToQuery, Correctness, Guidelines, and Safety. You can also create custom scorers using the @scorer decorator or integrate third-party libraries like DeepEval and RAGAS.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the LLM-as-Judge pattern and how does MLflow support it?\"},\n",
    "        \"expectations\": {\"expected_response\": \"LLM-as-Judge uses an LLM to evaluate outputs from another LLM, replacing brittle string matching with intelligent assessment. MLflow supports this through built-in scorers that use configurable judge models to provide scores and reasoning explanations.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How do you track costs and token usage in MLflow for GenAI?\"},\n",
    "        \"expectations\": {\"expected_response\": \"MLflow automatically logs token usage (prompt tokens, completion tokens, total tokens) and can calculate costs based on model pricing. This data is captured in traces and experiment runs, enabling cost analysis and optimization across different models and configurations.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What frameworks does MLflow integrate with for GenAI auto-tracing?\"},\n",
    "        \"expectations\": {\"expected_response\": \"MLflow provides auto-tracing for 40+ frameworks including OpenAI, Anthropic, LangChain, LlamaIndex, AWS Bedrock, Google Vertex AI, Cohere, Ollama, DSPy, AutoGen, and CrewAI. Auto-tracing automatically captures LLM calls without manual instrumentation.\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How do you implement session-level tracing for multi-turn conversations?\"},\n",
    "        \"expectations\": {\"expected_response\": \"Key concepts: (1) stable session identifier, (2) tag traces with session_id, (3) filter traces by session, (4) MLflow search capabilities\"}\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Evaluation dataset created with {len(eval_dataset)} examples\")\n",
    "print(\"\\nüìã Questions cover:\")\n",
    "print(\"   - MLflow Tracing fundamentals\")\n",
    "print(\"   - Prompt management\")\n",
    "print(\"   - Span types and structure\")\n",
    "print(\"   - LLM evaluation methods\")\n",
    "print(\"   - LLM-as-Judge pattern\")\n",
    "print(\"   - Cost and token tracking\")\n",
    "print(\"   - Framework integrations\")\n",
    "print(\"   - Session-level observability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How predict_fn works?\n",
    "\n",
    "When `mlflow.genai.evaluate()` runs:\n",
    "\n",
    "1. It iterates through each item in `eval_dataset`\n",
    "2. Extracts item[\"inputs\"] and passes it to `predict_fn(inputs)`\n",
    "3. Your function returns outputs (e.g., {\"response\": \"...\"})\n",
    "\n",
    "Scorers then evaluate using inputs, outputs, and expectations\n",
    " `[inputs, outputs responses (from llm), expecttions] --> scorers`\n",
    "\n",
    " - **eval_dataset**: Defines what to test (questions + expected answers)\n",
    " - **predict_fn**: Defines how to get answers (calls your agent)\n",
    " - **scorers**: Define how to judge quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prediction function defined\n",
      "   Signature: predict_fn(question: str) -> dict\n"
     ]
    }
   ],
   "source": [
    "def predict_fn(question: str) -> dict:\n",
    "    \"\"\"\n",
    "    Prediction function wrapper for evaluation.\n",
    "    \n",
    "    Note: mlflow.genai.evaluate() unpacks the 'inputs' dict as keyword arguments,\n",
    "    so the function signature must match the keys in your dataset's 'inputs' field.\n",
    "    \n",
    "    Dataset: {\"inputs\": {\"question\": \"...\"}} \n",
    "    Called as: predict_fn(question=\"...\")\n",
    "    \n",
    "    Args:\n",
    "        question: The question string (unpacked from inputs dict)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with 'response' key\n",
    "    \"\"\"\n",
    "    response = agent.answer(question)\n",
    "    return {\"response\": response}\n",
    "\n",
    "print(\"‚úÖ Prediction function defined\")\n",
    "print(\"   Signature: predict_fn(question: str) -> dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Run Evaluation with Built-in Scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/06 12:00:47 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset. To disable this check, set the MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION environment variable to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Running evaluation with built-in scorers...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b783263c1849e9ab8519ffcf8d3a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [Elapsed: 00:00, Remaining: ?] "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <title>Evaluation output</title>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <style>\n",
       "        body {\n",
       "            font-family: Arial, sans-serif;\n",
       "        }\n",
       "\n",
       "        .header {\n",
       "            a.button {\n",
       "                padding: 4px 8px;\n",
       "                line-height: 20px;\n",
       "                box-shadow: none;\n",
       "                height: 20px;\n",
       "                display: inline-flex;\n",
       "                align-items: center;\n",
       "                justify-content: center;\n",
       "                vertical-align: middle;\n",
       "                background-color: rgb(34, 114, 180);\n",
       "                color: rgb(255, 255, 255);\n",
       "                text-decoration: none;\n",
       "                animation-duration: 0s;\n",
       "                transition: none 0s ease 0s;\n",
       "                position: relative;\n",
       "                white-space: nowrap;\n",
       "                text-align: center;\n",
       "                border: 1px solid rgb(192, 205, 216);\n",
       "                cursor: pointer;\n",
       "                user-select: none;\n",
       "                touch-action: manipulation;\n",
       "                border-radius: 4px;\n",
       "                gap: 6px;\n",
       "            }\n",
       "\n",
       "            a.button:hover {\n",
       "                background-color: rgb(14, 83, 139) !important;\n",
       "                border-color: transparent !important;\n",
       "                color: rgb(255, 255, 255) !important;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .warnings-section {\n",
       "            margin-top: 8px;\n",
       "\n",
       "            ul {\n",
       "                list-style-type: none;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .instructions-section {\n",
       "            margin-top: 16px;\n",
       "            font-size: 14px;\n",
       "\n",
       "            ul {\n",
       "                margin-top: 0;\n",
       "                margin-bottom: 0;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        code {\n",
       "            font-family: monospace;\n",
       "        }\n",
       "\n",
       "        .note {\n",
       "            color: #666;\n",
       "        }\n",
       "\n",
       "        a {\n",
       "            color: #2272B4;\n",
       "            text-decoration: none;\n",
       "        }\n",
       "\n",
       "        a:hover {\n",
       "            color: #005580;\n",
       "        }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "    <div class=\"header\">\n",
       "        <a href=\"http://localhost:5000/#/experiments/1/evaluation-runs?selectedRunUuid=3c593df10a0642be94b2cd4f2deeb12d\" class=\"button\">\n",
       "            View evaluation results in MLflow\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1em\" height=\"1em\" fill=\"none\" viewBox=\"0 0 16 16\" aria-hidden=\"true\" focusable=\"false\" class=\"\">\n",
       "                <path fill=\"currentColor\" d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"></path>\n",
       "                <path fill=\"currentColor\" d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"></path>\n",
       "            </svg>\n",
       "        </a>\n",
       "    </div>\n",
       "</div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Evaluation complete!\n",
      "\n",
      "üìä Metrics Summary:\n",
      "--------------------------------------------------\n",
      "  safety/mean: 1.000\n",
      "  relevance_to_query/mean: 1.000\n",
      "  custom_guidelines/mean: 0.625\n",
      "  correctness/mean: 0.250\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "print(\"üîÑ Running evaluation with built-in scorers...\\n\")\n",
    "\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=predict_fn,\n",
    "    scorers=[\n",
    "        relevance_scorer,\n",
    "        correctness_scorer,\n",
    "        safety_scorer,\n",
    "        guidelines_scorer\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete!\")\n",
    "\n",
    "# Display metrics\n",
    "print(\"\\nüìä Metrics Summary:\")\n",
    "print(\"-\" * 50)\n",
    "if results.metrics:\n",
    "    for metric_name, value in results.metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {metric_name}: {value:.3f}\")\n",
    "        else:\n",
    "            print(f\"  {metric_name}: {value}\")\n",
    "else:\n",
    "    print(\"  No metrics returned\")\n",
    "    print(\"\\n‚ö†Ô∏è  Scorers returned None - this usually means the judge model call failed.\")\n",
    "    print(\"  Check the 'error_message' or similar columns above for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Custom Scorers with @scorer Decorator\n",
    "\n",
    "Create your own evaluation logic using the `@scorer` decorator.\n",
    "\n",
    "Beyond using built-in metrics, you can define custom scorers to capture specific subject matter expertise. This is particularly useful when standard scorers cannot effectively gauge unique or nuanced aspects of your model's responses. While this example uses simple scorers for brevity and demonstration, you should tailor your custom metrics to reflect the specialized requirements of your specific domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Custom scorers defined:\n",
      "   - response_length_check\n",
      "   - contains_keywords\n",
      "   - no_hallucination_markers\n"
     ]
    }
   ],
   "source": [
    "from mlflow.genai import scorer\n",
    "\n",
    "@scorer\n",
    "def response_length_check(outputs: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Check if response is within acceptable length.\n",
    "    Returns True if response is between 200 and 500 characters.\n",
    "    \"\"\"\n",
    "    response = outputs.get(\"response\", \"\")\n",
    "    length = len(response)\n",
    "    return 20 <= length <= 500\n",
    "\n",
    "@scorer\n",
    "def contains_keywords(outputs: dict, expectations: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Check if response contains key terms from expected answer.\n",
    "    \"\"\"\n",
    "    response = outputs.get(\"response\", \"\").lower()\n",
    "    # Use 'expected_response' to match the dataset field name\n",
    "    expected = expectations.get(\"expected_response\", \"\").lower()\n",
    "    \n",
    "    # Extract key words (simple approach)\n",
    "    key_words = [word for word in expected.split() if len(word) > 4]\n",
    "    \n",
    "    # Check if at least 30% of key words are present\n",
    "    # If no keywords to check, fail conservatively (may indicate data issue)\n",
    "    if not key_words:\n",
    "        return False\n",
    "    \n",
    "    matches = sum(1 for word in key_words if word in response)\n",
    "    return matches / len(key_words) >= 0.3\n",
    "\n",
    "@scorer\n",
    "def no_hallucination_markers(outputs: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Check for common hallucination markers.\n",
    "    \"\"\"\n",
    "    response = outputs.get(\"response\", \"\").lower()\n",
    "    \n",
    "    hallucination_markers = [\n",
    "        \"i think\",\n",
    "        \"i believe\",\n",
    "        \"probably\",\n",
    "        \"might be\",\n",
    "        \"i'm not sure\",\n",
    "        \"as far as i know\"\n",
    "    ]\n",
    "    \n",
    "    return not any(marker in response for marker in hallucination_markers)\n",
    "\n",
    "print(\"‚úÖ Custom scorers defined:\")\n",
    "print(\"   - response_length_check\")\n",
    "print(\"   - contains_keywords\")\n",
    "print(\"   - no_hallucination_markers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation on our Customer Judges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/06 12:06:01 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset. To disable this check, set the MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION environment variable to True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Running evaluation with custom scorers...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0c45db70a64f69b17f29d23e7651ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [Elapsed: 00:00, Remaining: ?] "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <title>Evaluation output</title>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <style>\n",
       "        body {\n",
       "            font-family: Arial, sans-serif;\n",
       "        }\n",
       "\n",
       "        .header {\n",
       "            a.button {\n",
       "                padding: 4px 8px;\n",
       "                line-height: 20px;\n",
       "                box-shadow: none;\n",
       "                height: 20px;\n",
       "                display: inline-flex;\n",
       "                align-items: center;\n",
       "                justify-content: center;\n",
       "                vertical-align: middle;\n",
       "                background-color: rgb(34, 114, 180);\n",
       "                color: rgb(255, 255, 255);\n",
       "                text-decoration: none;\n",
       "                animation-duration: 0s;\n",
       "                transition: none 0s ease 0s;\n",
       "                position: relative;\n",
       "                white-space: nowrap;\n",
       "                text-align: center;\n",
       "                border: 1px solid rgb(192, 205, 216);\n",
       "                cursor: pointer;\n",
       "                user-select: none;\n",
       "                touch-action: manipulation;\n",
       "                border-radius: 4px;\n",
       "                gap: 6px;\n",
       "            }\n",
       "\n",
       "            a.button:hover {\n",
       "                background-color: rgb(14, 83, 139) !important;\n",
       "                border-color: transparent !important;\n",
       "                color: rgb(255, 255, 255) !important;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .warnings-section {\n",
       "            margin-top: 8px;\n",
       "\n",
       "            ul {\n",
       "                list-style-type: none;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .instructions-section {\n",
       "            margin-top: 16px;\n",
       "            font-size: 14px;\n",
       "\n",
       "            ul {\n",
       "                margin-top: 0;\n",
       "                margin-bottom: 0;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        code {\n",
       "            font-family: monospace;\n",
       "        }\n",
       "\n",
       "        .note {\n",
       "            color: #666;\n",
       "        }\n",
       "\n",
       "        a {\n",
       "            color: #2272B4;\n",
       "            text-decoration: none;\n",
       "        }\n",
       "\n",
       "        a:hover {\n",
       "            color: #005580;\n",
       "        }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "    <div class=\"header\">\n",
       "        <a href=\"http://localhost:5000/#/experiments/1/evaluation-runs?selectedRunUuid=bfa6f7d740b04dc7bf95a38b68622a7c\" class=\"button\">\n",
       "            View evaluation results in MLflow\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1em\" height=\"1em\" fill=\"none\" viewBox=\"0 0 16 16\" aria-hidden=\"true\" focusable=\"false\" class=\"\">\n",
       "                <path fill=\"currentColor\" d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"></path>\n",
       "                <path fill=\"currentColor\" d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"></path>\n",
       "            </svg>\n",
       "        </a>\n",
       "    </div>\n",
       "</div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Custom evaluation complete!\n",
      "\n",
      "Custom Metrics Summary:\n",
      "----------------------------------------\n",
      "  response_length_check/mean: 0.125\n",
      "  no_hallucination_markers/mean: 1.000\n",
      "  contains_keywords/mean: 0.750\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation with custom scorers\n",
    "print(\"üîÑ Running evaluation with custom scorers...\\n\")\n",
    "\n",
    "custom_results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=predict_fn,\n",
    "    scorers=[\n",
    "        response_length_check,\n",
    "        contains_keywords,\n",
    "        no_hallucination_markers\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Custom evaluation complete!\")\n",
    "print(\"\\nCustom Metrics Summary:\")\n",
    "print(\"-\" * 40)\n",
    "for metric_name, value in custom_results.metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric_name}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {metric_name}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: DeepEval Integration\n",
    "\n",
    "MLflow integrates with [DeepEval](https://mlflow.org/docs/latest/genai/eval-monitor/scorers/third-party/deepeval/) for advanced conversational AI evaluation metrics.\n",
    "\n",
    "| DeepEval Scorer | Purpose |\n",
    "|-----------------|----------|\n",
    "| `ConversationCompleteness` | Did the conversation achieve its goal? |\n",
    "| `KnowledgeRetention` | Does the agent remember context? |\n",
    "| `TopicAdherence` | Does the agent stay on topic? |\n",
    "| `Toxicity` | Is the agent response harmful or toxic in tonality|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DeepEval scorers initialized:\n",
      "   - ConversationCompleteness\n",
      "   - KnowledgeRetention\n",
      "   - TopicAdherence\n",
      "   - Toxicity\n",
      "\n",
      " Next, let's evaluate a multi-turn conversation using these DeepEval scorers\n"
     ]
    }
   ],
   "source": [
    "from mlflow.genai.scorers.deepeval import (\n",
    "    ConversationCompleteness,\n",
    "    KnowledgeRetention,\n",
    "    TopicAdherence,\n",
    "    Toxicity,\n",
    ")\n",
    "\n",
    "# Initialize DeepEval scorers\n",
    "jude_model_uri = f\"openai:/{JUDGE_MODEL}\"\n",
    "\n",
    "completeness_scorer = ConversationCompleteness(model=jude_model_uri, threshold=0.7, include_reason=True)\n",
    "retention_scorer = KnowledgeRetention(model=jude_model_uri, threshold=0.7, include_reason=True)\n",
    "toxicity_scorer = Toxicity(model=jude_model_uri, threshold=0.7, include_reason=True)\n",
    "topic_scorer = TopicAdherence(model=jude_model_uri, threshold=0.7, include_reason=True, relevant_topics=[\"MLflow\", \"machine learning\", \"AI\", \"data science\", \"genai\", \"agent\", \"observability\", \"prompt engineering\", \"prompt management\", \"prompt registry\", \"experiment tracking\"])\n",
    "\n",
    "\n",
    "print(\"‚úÖ DeepEval scorers initialized:\")\n",
    "print(\"   - ConversationCompleteness\")\n",
    "print(\"   - KnowledgeRetention\")\n",
    "print(\"   - TopicAdherence\")\n",
    "print(\"   - Toxicity\")\n",
    "print(\"\\n Next, let's evaluate a multi-turn conversation using these DeepEval scorers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Multi-Turn Conversation Agent\n",
    "\n",
    "For session-level evaluation, we need an agent that handles conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ConversationalAgent defined with session tracking\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "class ConversationalAgent:\n",
    "    \"\"\"\n",
    "    An agent that maintains conversation history for multi-turn interactions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, client: Any, model: str = \"gpt-5.2\"):\n",
    "        self.model = model\n",
    "        self.client = client\n",
    "        self.conversation_history = []\n",
    "        self.session_id = str(uuid.uuid4())\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset conversation history and start new session.\"\"\"\n",
    "        self.conversation_history = []\n",
    "        self.session_id = str(uuid.uuid4())\n",
    "    \n",
    "    @mlflow.trace(name=\"conversational_agent\", span_type=\"AGENT\")\n",
    "    def chat(self, user_message: str) -> str:\n",
    "        \"\"\"\n",
    "        Send a message and get a response, maintaining history.\n",
    "        \"\"\"\n",
    "        # Tag trace with session ID for grouping\n",
    "        mlflow.update_current_trace(metadata={\n",
    "            \"mlflow.trace.session\": self.session_id,\n",
    "            \"turn_number\": len(self.conversation_history) // 2 + 1\n",
    "        })\n",
    "        \n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message\n",
    "        })\n",
    "        \n",
    "        # Prepare messages with system prompt. Add previous context to the conversation.\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \n",
    "            \"content\": \"\"\"You are a helpful MLflow expert assistant. Answer questions about MLflow clearly, accurately,\n",
    "                        concisely, without hallucinations, and accurately. Remember previous context in the conversation.\"\"\"}\n",
    "        ] + self.conversation_history\n",
    "        \n",
    "        # Get response from the Agent LLM\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        \n",
    "        assistant_message = response.choices[0].message.content\n",
    "        \n",
    "        # Add to history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": assistant_message\n",
    "        })\n",
    "        \n",
    "        return assistant_message\n",
    "\n",
    "print(\"‚úÖ ConversationalAgent defined with session tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate a multi-turn conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/06 12:12:52 WARNING mlflow.tracing.fluent: Found non-string values in metadata. Non-string values in metadata will automatically be stringified when the trace is logged. Non-string items: {'turn_number': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è Multi-Turn Conversation\n",
      "\n",
      "============================================================\n",
      "\n",
      "[Turn 1]\n",
      "User: What is MLflow for GenAI?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/06 12:12:59 WARNING mlflow.tracing.fluent: Found non-string values in metadata. Non-string values in metadata will automatically be stringified when the trace is logged. Non-string items: {'turn_number': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: MLflow for GenAI is the set of MLflow capabilities focused on building, evaluating, tracking, and deploying **generative AI applications** (LLM-powered apps), not just traditional ML models.\n",
      "\n",
      "In practice, it helps you manage things like **prompts, models, retrieval components, and outputs** with the same lifecycle tooling MLflow provides for ML.\n",
      "\n",
      "Key pieces include:\n",
      "\n",
      "- **Prompt & experiment tracking**  \n",
      "  Track runs that include prompts, system messages, model/provider (e.g., OpenAI, Anthropic, local models), parameters (temperature, max tokens), tool calls, and outputs‚Äîso you can reproduce and compare changes.\n",
      "\n",
      "- **LLM evaluation (quality, safety, and regression testing)**  \n",
      "  Evaluate LLM apps using built-in evaluators and/or custom metrics (e.g., correctness, relevance, faithfulness, toxicity, latency, cost). This supports automated comparisons across prompt versions or model swaps.\n",
      "\n",
      "- **Model packaging & deployment for LLM apps**  \n",
      "  Package GenAI logic (prompt + code + dependencies, sometimes including retrieval) and deploy it via MLflow‚Äôs serving interfaces, so you can move from notebook to production more reliably.\n",
      "\n",
      "- **Tracing/observability for LLM calls (where supported)**  \n",
      "  Capture structured traces of multi-step LLM workflows (chains/agents), including intermediate steps, tool invocations, and timings, to debug and monitor behavior.\n",
      "\n",
      "- **Governance via the MLflow Model Registry**  \n",
      "  Use the Registry to version and promote GenAI ‚Äúmodels‚Äù (often meaning an LLM app or prompt-driven pipeline) through stages (e.g., Staging ‚Üí Production), with lineage back to the runs and evaluations.\n",
      "\n",
      "If you tell me your stack (Databricks, OSS MLflow, LangChain/LlamaIndex, OpenAI/local models), I can point to the exact MLflow components and a minimal example workflow.\n",
      "\n",
      "[Turn 2]\n",
      "User: What are its main GenAI main components?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/06 12:13:04 WARNING mlflow.tracing.fluent: Found non-string values in metadata. Non-string values in metadata will automatically be stringified when the trace is logged. Non-string items: {'turn_number': 3}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Main MLflow-for-GenAI components (at a high level) are:\n",
      "\n",
      "1. **Tracking (Runs) for GenAI**\n",
      "   - Log prompts/templates, LLM/provider + parameters (temperature, max_tokens), inputs/outputs, artifacts (datasets), and metadata so you can reproduce and compare iterations.\n",
      "\n",
      "2. **Tracing / Observability**\n",
      "   - Capture traces of LLM app execution (multi-step chains/agents), including intermediate steps, tool calls, timings, and errors for debugging and monitoring.\n",
      "\n",
      "3. **Evaluation**\n",
      "   - Built-in and custom evaluators for LLM apps (e.g., answer quality, relevance/groundedness, safety/toxicity), plus regression testing and side-by-side comparisons across prompt/model versions.\n",
      "\n",
      "4. **Packaging & Serving (Deployment)**\n",
      "   - Package an LLM app (prompt + code + dependencies, sometimes retrieval logic) as an MLflow ‚Äúmodel‚Äù and deploy it using MLflow serving interfaces.\n",
      "\n",
      "5. **Model Registry (Governance)**\n",
      "   - Version and manage GenAI assets (often an LLM app or prompt-driven pipeline) in the Registry, with lineage to runs/evals and stage transitions (Staging/Production), approvals, and rollback.\n",
      "\n",
      "If you share whether you mean **open-source MLflow** or **Databricks MLflow**, I can map these to the exact APIs/features available in your environment.\n",
      "\n",
      "[Turn 3]\n",
      "User: Tell me more about the Tracing component.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/06 12:13:14 WARNING mlflow.tracing.fluent: Found non-string values in metadata. Non-string values in metadata will automatically be stringified when the trace is logged. Non-string items: {'turn_number': 4}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: MLflow **Tracing** is the observability layer for GenAI apps. It records a structured ‚Äútrace‚Äù of what happened during an LLM application run‚Äîespecially useful for **multi-step** workflows (RAG pipelines, chains, agents, tool/function calling).\n",
      "\n",
      "### What Tracing captures\n",
      "A trace is typically composed of **spans** (steps). For each span, tracing can capture:\n",
      "\n",
      "- **Inputs/outputs**: prompts/messages, retrieved context, tool inputs/outputs, final response\n",
      "- **Model/provider info**: which LLM endpoint/model was called\n",
      "- **Parameters**: temperature, max_tokens, etc. (when available)\n",
      "- **Timing**: per-step latency and total latency\n",
      "- **Errors/exceptions**: where failures occurred\n",
      "- **Metadata/tags**: app version, prompt version, user/session identifiers (if you add them)\n",
      "- **Cost/token usage**: when the underlying provider returns it (varies by integration)\n",
      "\n",
      "This makes it easier to answer questions like:\n",
      "- ‚ÄúWhich step is slow?‚Äù\n",
      "- ‚ÄúDid retrieval return the right documents?‚Äù\n",
      "- ‚ÄúWhich tool call caused the bad answer?‚Äù\n",
      "- ‚ÄúWhat changed between the good run and the regressed run?‚Äù\n",
      "\n",
      "### Why it‚Äôs useful (common GenAI debugging)\n",
      "- **RAG debugging**: inspect retrieved chunks, scores, and whether the model actually used them.\n",
      "- **Agent/tool debugging**: see tool selection, tool arguments, tool outputs, and the model‚Äôs follow-up.\n",
      "- **Prompt iteration**: correlate prompt versions with output quality and latency.\n",
      "- **Production monitoring**: detect spikes in latency/errors, or changes in behavior after a model swap.\n",
      "\n",
      "### How it fits with the rest of MLflow\n",
      "- **Tracing + Tracking**: tracing gives step-by-step execution detail; tracking gives experiment/run organization, parameters/metrics/artifacts.\n",
      "- **Tracing + Evaluation**: you can evaluate outputs and use traces to explain *why* a run scored poorly.\n",
      "- **Tracing + Registry**: when you deploy a versioned app, traces help monitor that specific version in production.\n",
      "\n",
      "### Instrumentation (how traces get created)\n",
      "Depending on your stack, traces can be produced via:\n",
      "- **Auto-instrumentation/integrations** (where available) for popular LLM frameworks/providers.\n",
      "- **Manual instrumentation**: you wrap key steps (retrieval, prompt assembly, LLM call, tool execution) to create spans and attach inputs/outputs.\n",
      "\n",
      "### Practical considerations\n",
      "- **Privacy & security**: traces often include prompts and retrieved text. You typically want redaction, hashing, or selective logging for PII/secrets.\n",
      "- **Sampling**: in high-traffic apps, you may trace only a fraction of requests.\n",
      "- **Schema consistency**: standardize span names/fields so you can compare traces across versions.\n",
      "\n",
      "If you tell me what you‚Äôre using (LangChain, LlamaIndex, custom Python, Databricks, etc.), I can show the recommended way to enable MLflow tracing for that setup and what you‚Äôll see in the trace UI/logs.\n",
      "\n",
      "[Turn 4]\n",
      "User: How does it compare to other tools?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/06 12:13:29 WARNING mlflow.tracing.fluent: Found non-string values in metadata. Non-string values in metadata will automatically be stringified when the trace is logged. Non-string items: {'turn_number': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: MLflow Tracing is a **general-purpose, ML lifecycle‚Äìintegrated** tracing option for GenAI apps. Compared to dedicated LLM observability products (LangSmith, Arize Phoenix, Weights & Biases Weave, Helicone, Humanloop, etc.) it tends to be strongest when you want **one system** for tracking/eval/registry/deployment plus tracing, and you‚Äôre already using MLflow/Databricks.\n",
      "\n",
      "Here are the main comparison axes:\n",
      "\n",
      "## 1) Scope: ‚ÄúLLM observability‚Äù vs ‚ÄúML lifecycle‚Äù\n",
      "- **MLflow Tracing**: Tracing is one component alongside **Tracking, Evaluation, Model Registry, and Serving**. Good if you want traces tied directly to runs, evaluations, and model/app versions.\n",
      "- **Dedicated LLM observability tools**: Often go deeper on observability workflows (prompt playgrounds, annotation queues, dataset curation loops, specialized dashboards), but may be a separate system from your ML model registry/deployment.\n",
      "\n",
      "## 2) Framework integrations & auto-instrumentation\n",
      "- **MLflow Tracing**: Works best when your stack aligns with supported integrations (and/or you‚Äôre willing to add manual spans). Integration breadth varies by environment and MLflow version.\n",
      "- **Others**: Some tools are very ‚ÄúLLM-native‚Äù and may have broader or more polished auto-instrumentation for specific frameworks (LangChain/LlamaIndex) and providers, plus richer out-of-the-box trace UIs for agents/tools.\n",
      "\n",
      "## 3) Trace UX and analysis features\n",
      "- **MLflow Tracing**: Typically focuses on capturing spans, inputs/outputs, timings, errors, and metadata, with a UI oriented around MLflow artifacts/runs.\n",
      "- **Others**: Often provide advanced trace analytics (aggregation by route/user, prompt diffing, token/cost dashboards, failure clustering, feedback/labeling workflows) as first-class features.\n",
      "\n",
      "## 4) Evaluation + tracing loop\n",
      "- **MLflow**: Strong at connecting traces to **evaluation runs** and **versioned artifacts** (datasets, prompts, code). This is useful for regression testing and governance.\n",
      "- **Others**: Some have excellent human feedback and annotation pipelines; some integrate evals tightly too, but you‚Äôll want to check how well they connect to your broader MLOps lifecycle.\n",
      "\n",
      "## 5) Governance, deployment, and reproducibility\n",
      "- **MLflow**: A differentiator is the **Model Registry** + lineage: you can trace behavior back to a specific registered version (prompt/app/model), the exact run, and its evaluation.\n",
      "- **Others**: May integrate with CI/CD and deployments, but typically don‚Äôt replace a model registry (or they provide a lighter-weight one).\n",
      "\n",
      "## 6) Data control, security, and cost\n",
      "- **MLflow (self-managed/Databricks)**: Often attractive for teams that need data residency, private networking, and unified governance. You can control what gets logged and where it‚Äôs stored.\n",
      "- **SaaS observability tools**: Can be faster to adopt and feature-rich, but you‚Äôll evaluate data-sharing constraints, PII handling, and cost at scale.\n",
      "\n",
      "## When MLflow Tracing is the better choice\n",
      "- You already use MLflow for experiments/registry/serving and want **one lineage story**.\n",
      "- You need **governed, versioned** GenAI apps with traceability across environments.\n",
      "- You‚Äôre building internal platforms and prefer **self-managed** control.\n",
      "\n",
      "## When a dedicated LLM observability tool may be better\n",
      "- You want the richest **LLM-specific trace UX**, analytics, feedback/labeling workflows, and prompt playgrounds out of the box.\n",
      "- You need broad, turnkey integrations across many frameworks/providers with minimal custom instrumentation.\n",
      "\n",
      "If you tell me which ‚Äúother tool(s)‚Äù you‚Äôre comparing against and your environment (OSS MLflow vs Databricks, LangChain/LlamaIndex/custom), I can give a more concrete feature-by-feature comparison and a recommended setup.\n",
      "\n",
      "[Turn 5]\n",
      "User: What is the difference between Tracing and Tracking?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/06 12:13:37 WARNING mlflow.tracing.fluent: Found non-string values in metadata. Non-string values in metadata will automatically be stringified when the trace is logged. Non-string items: {'turn_number': 6}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: **Tracking** and **Tracing** solve different problems in MLflow:\n",
      "\n",
      "## MLflow Tracking (experiment/run tracking)\n",
      "**Purpose:** Record and compare *experiments* and their outcomes.\n",
      "\n",
      "**Typical unit:** an **MLflow Run** (one experiment attempt).\n",
      "\n",
      "**What you log:**\n",
      "- **Parameters** (e.g., temperature, top_p, chunk size, prompt version)\n",
      "- **Metrics** (e.g., accuracy, win rate, faithfulness score, latency aggregates)\n",
      "- **Artifacts** (prompt templates, evaluation datasets, model/app code, configs)\n",
      "- **Tags/metadata** (git SHA, dataset version, environment)\n",
      "\n",
      "**What it‚Äôs good for:**\n",
      "- Reproducibility and auditability of an experiment\n",
      "- Comparing variants (prompt A vs B, model X vs Y)\n",
      "- Capturing evaluation results and dashboards over many runs\n",
      "\n",
      "## MLflow Tracing (execution-level observability)\n",
      "**Purpose:** Understand *what happened inside* a single LLM app execution.\n",
      "\n",
      "**Typical unit:** a **Trace** made of **spans** (steps).\n",
      "\n",
      "**What it captures (per step):**\n",
      "- Inputs/outputs for each step (prompt/messages, retrieved docs, tool calls)\n",
      "- Timing/latency breakdown by step\n",
      "- Errors/exceptions and where they occurred\n",
      "- Model/provider call details (when available)\n",
      "- Metadata you attach (user/session/request IDs)\n",
      "\n",
      "**What it‚Äôs good for:**\n",
      "- Debugging RAG (what was retrieved? used?)\n",
      "- Debugging agents/tools (which tool was called? with what args? what returned?)\n",
      "- Finding bottlenecks (retrieval vs LLM call vs reranker)\n",
      "- Production incident investigation\n",
      "\n",
      "## Relationship: how they work together\n",
      "- **Tracking answers:** ‚ÄúWhich configuration/version performs best overall?‚Äù\n",
      "- **Tracing answers:** ‚ÄúWhy did *this specific request* fail or behave oddly?‚Äù\n",
      "- Common pattern: use **Tracing** to diagnose issues, then use **Tracking** to run controlled experiments/evaluations to confirm improvements.\n",
      "\n",
      "## Quick analogy\n",
      "- **Tracking = lab notebook / experiment log**\n",
      "- **Tracing = debugger / request-level call stack**\n",
      "\n",
      "If you describe your app (RAG? agent? single LLM call?) I can suggest what to put in tracking vs what to capture in tracing, and how to connect them with shared IDs/tags.\n",
      "\n",
      "[Turn 6]\n",
      "User: How do I get started with MLflow for GenAI?\n",
      "Agent: 1) **Set up MLflow**\n",
      "- **Databricks**: use the workspace MLflow out of the box.\n",
      "- **Open-source MLflow**: run a tracking server (or start local) and point your code at it.\n",
      "  - Local quick start: `mlflow ui` (for viewing) and runs log to `./mlruns` by default.\n",
      "  - If using a server: set `MLFLOW_TRACKING_URI=http://...`\n",
      "\n",
      "2) **Create a baseline GenAI app**\n",
      "Start with something small:\n",
      "- **Single LLM call** (prompt ‚Üí response), or\n",
      "- **RAG** (retrieve ‚Üí prompt ‚Üí response)\n",
      "\n",
      "Keep the inputs/outputs structured (e.g., JSON with `question`, `context`, `answer`).\n",
      "\n",
      "3) **Use MLflow Tracking to log the essentials**\n",
      "In each iteration/run, log:\n",
      "- **Params**: model name/provider, temperature, max_tokens, chunk size, top_k, prompt version\n",
      "- **Artifacts**: prompt template, system prompt, retrieval config, example inputs\n",
      "- **Metrics**: latency, cost/token usage (if available), and quality metrics from evaluation\n",
      "\n",
      "This gives you reproducibility and comparison across prompt/model changes.\n",
      "\n",
      "4) **Add Tracing for visibility**\n",
      "Instrument your app so you can see step-by-step behavior:\n",
      "- RAG: retrieval span(s), rerank span, prompt-construction span, LLM-call span\n",
      "- Agents: tool-selection span, each tool call span, final synthesis span\n",
      "\n",
      "Start by tracing only a small percentage of requests if you‚Äôre in production, and be mindful of PII/redaction.\n",
      "\n",
      "5) **Evaluate systematically**\n",
      "Create an evaluation dataset (even 20‚Äì100 examples to start):\n",
      "- Inputs + expected outputs (when you have them) or grading criteria\n",
      "- Run evaluations to produce comparable metrics (quality + safety + latency)\n",
      "- Use evaluation to catch regressions when you change prompts/models/retrievers\n",
      "\n",
      "6) **Package and register**\n",
      "Once you have a good candidate:\n",
      "- Package the app as an MLflow ‚Äúmodel‚Äù (often a Python callable / pyfunc wrapping your prompt + retrieval + postprocessing).\n",
      "- Register it in the **MLflow Model Registry** so you can version it and promote it (Staging ‚Üí Production).\n",
      "\n",
      "7) **Deploy and monitor**\n",
      "- Deploy via your platform‚Äôs serving option (Databricks Model Serving or your own MLflow serving setup).\n",
      "- Monitor with **Tracing** (debugging) and **Tracking/Evaluation** (quality regression, latency/cost trends).\n",
      "\n",
      "---\n",
      "\n",
      "### What I need to tailor this to your setup\n",
      "1) Are you on **Databricks** or **open-source MLflow**?\n",
      "2) Are you building **RAG**, an **agent**, or a **single-call** LLM app?\n",
      "3) Which framework/provider (LangChain, LlamaIndex, OpenAI/Anthropic, local models)?\n",
      "\n",
      "If you answer those, I can give you a minimal ‚Äúhello world‚Äù project structure and the exact MLflow APIs to use for tracking, tracing, and evaluation in your environment.\n",
      "\n",
      "============================================================\n",
      "\n",
      "‚úÖ Conversation complete (Session: 2f3a78f7...)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-e3feeb5a5698e9aab95f9f2a076ed659&amp;experiment_id=1&amp;trace_id=tr-82aa410f6ae2970481a8c9b5a976bf05&amp;experiment_id=1&amp;trace_id=tr-2b45100523f045ee2f712519608daa3b&amp;experiment_id=1&amp;trace_id=tr-c7f5188e41d8086d806f9c075f81c07b&amp;experiment_id=1&amp;trace_id=tr-fcef8024c8e02fefa232a8a8571ce35e&amp;experiment_id=1&amp;trace_id=tr-d34e3612996ec65c1422f872dfbedd78&amp;experiment_id=1&amp;version=3.9.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-e3feeb5a5698e9aab95f9f2a076ed659), Trace(trace_id=tr-82aa410f6ae2970481a8c9b5a976bf05), Trace(trace_id=tr-2b45100523f045ee2f712519608daa3b), Trace(trace_id=tr-c7f5188e41d8086d806f9c075f81c07b), Trace(trace_id=tr-fcef8024c8e02fefa232a8a8571ce35e), Trace(trace_id=tr-d34e3612996ec65c1422f872dfbedd78)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simulate a multi-turn conversation\n",
    "conv_agent = ConversationalAgent(client=client, model=AGENT_MODEL)\n",
    "\n",
    "conversation_turns = [\n",
    "    \"What is MLflow for GenAI?\",\n",
    "    \"What are its main GenAI main components?\",\n",
    "    \"Tell me more about the Tracing component.\",\n",
    "    \"How does it compare to other tools?\",\n",
    "    \"What is the difference between Tracing and Tracking?\",\n",
    "    \"How do I get started with MLflow for GenAI?\",\n",
    "]\n",
    "\n",
    "print(\"üó£Ô∏è Multi-Turn Conversation\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, user_msg in enumerate(conversation_turns, 1):\n",
    "    print(f\"\\n[Turn {i}]\")\n",
    "    print(f\"User: {user_msg}\")\n",
    "    response = conv_agent.chat(user_msg)\n",
    "    print(f\"Agent: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"\\n‚úÖ Conversation complete (Session: {conv_agent.session_id[:8]}...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: Session-Level Evaluation with DeepEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Found 6 traces for session 2f3a78f7...\n"
     ]
    }
   ],
   "source": [
    "# Search for traces from this session\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "session_traces = mlflow.search_traces(\n",
    "    locations=[experiment.experiment_id],\n",
    "    filter_string=f\"metadata.`mlflow.trace.session` = '{conv_agent.session_id}'\"\n",
    ")\n",
    "\n",
    "print(f\"üìä Found {len(session_traces)} traces for session {conv_agent.session_id[:8]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49a6418b9e447d6bcc82b5246ea91b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/7 [Elapsed: 00:00, Remaining: ?] "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <title>Evaluation output</title>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <style>\n",
       "        body {\n",
       "            font-family: Arial, sans-serif;\n",
       "        }\n",
       "\n",
       "        .header {\n",
       "            a.button {\n",
       "                padding: 4px 8px;\n",
       "                line-height: 20px;\n",
       "                box-shadow: none;\n",
       "                height: 20px;\n",
       "                display: inline-flex;\n",
       "                align-items: center;\n",
       "                justify-content: center;\n",
       "                vertical-align: middle;\n",
       "                background-color: rgb(34, 114, 180);\n",
       "                color: rgb(255, 255, 255);\n",
       "                text-decoration: none;\n",
       "                animation-duration: 0s;\n",
       "                transition: none 0s ease 0s;\n",
       "                position: relative;\n",
       "                white-space: nowrap;\n",
       "                text-align: center;\n",
       "                border: 1px solid rgb(192, 205, 216);\n",
       "                cursor: pointer;\n",
       "                user-select: none;\n",
       "                touch-action: manipulation;\n",
       "                border-radius: 4px;\n",
       "                gap: 6px;\n",
       "            }\n",
       "\n",
       "            a.button:hover {\n",
       "                background-color: rgb(14, 83, 139) !important;\n",
       "                border-color: transparent !important;\n",
       "                color: rgb(255, 255, 255) !important;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .warnings-section {\n",
       "            margin-top: 8px;\n",
       "\n",
       "            ul {\n",
       "                list-style-type: none;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .instructions-section {\n",
       "            margin-top: 16px;\n",
       "            font-size: 14px;\n",
       "\n",
       "            ul {\n",
       "                margin-top: 0;\n",
       "                margin-bottom: 0;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        code {\n",
       "            font-family: monospace;\n",
       "        }\n",
       "\n",
       "        .note {\n",
       "            color: #666;\n",
       "        }\n",
       "\n",
       "        a {\n",
       "            color: #2272B4;\n",
       "            text-decoration: none;\n",
       "        }\n",
       "\n",
       "        a:hover {\n",
       "            color: #005580;\n",
       "        }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "    <div class=\"header\">\n",
       "        <a href=\"http://localhost:5000/#/experiments/1/evaluation-runs?selectedRunUuid=698c856b44fa45118a0a6c7e6fdf451f\" class=\"button\">\n",
       "            View evaluation results in MLflow\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1em\" height=\"1em\" fill=\"none\" viewBox=\"0 0 16 16\" aria-hidden=\"true\" focusable=\"false\" class=\"\">\n",
       "                <path fill=\"currentColor\" d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"></path>\n",
       "                <path fill=\"currentColor\" d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"></path>\n",
       "            </svg>\n",
       "        </a>\n",
       "    </div>\n",
       "</div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ DeepEval session evaluation complete!\n",
      "\n",
      "DeepEval Metrics:\n",
      "----------------------------------------\n",
      "  Toxicity/mean: 1.000\n",
      "  KnowledgeRetention/mean: 0.000\n",
      "  ConversationCompleteness/mean: 1.000\n",
      "  TopicAdherence/mean: 1.000\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"DeepEval\") as run:\n",
    "    deepeval_results = mlflow.genai.evaluate(\n",
    "        data= session_traces,\n",
    "        scorers=[\n",
    "            completeness_scorer,\n",
    "            retention_scorer,\n",
    "            topic_scorer,\n",
    "            toxicity_scorer\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(\"\\n‚úÖ DeepEval session evaluation complete!\")\n",
    "print(\"\\nDeepEval Metrics:\")\n",
    "print(\"-\" * 40)\n",
    "for metric_name, value in deepeval_results.metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric_name}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {metric_name}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11: Viewing Evaluation Results in MLflow UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
      "‚ïë         Viewing Evaluation Results in MLflow UI              ‚ïë\n",
      "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
      "\n",
      "üîç EXPERIMENTS VIEW:\n",
      "   Navigate to: http://localhost:5000\n",
      "   Select: \"11-agent-evaluation\" experiment\n",
      "\n",
      "   You'll see:\n",
      "   - Session-level and Evaluation runs with scorer metrics\n",
      "   - Pass/fail rates for each scorer\n",
      "   - Detailed reasoning from LLM judges\n",
      "\n",
      "üìä EVALUATION RESULTS:\n",
      "   Each run includes:\n",
      "   - Metric values (0-1 scores or boolean)\n",
      "   - Judge reasoning explanations\n",
      "   - Input/output pairs\n",
      "   - Artifacts with detailed results\n",
      "\n",
      "üéØ KEY METRICS TO MONITOR:\n",
      "\n",
      "   Built-in Scorers:\n",
      "   - relevance_to_query/score: Response relevance\n",
      "   - correctness/score: Factual accuracy\n",
      "   - safety/score: Safety compliance\n",
      "   - guidelines/score: Guideline adherence\n",
      "\n",
      "   Custom Scorers:\n",
      "   - response_length_check: Length validation\n",
      "   - contains_keywords: Keyword presence\n",
      "   - no_hallucination_markers: Confidence check\n",
      "\n",
      "   DeepEval Scorers:\n",
      "   - conversation_completeness/score: Goal achievement\n",
      "   - knowledge_retention/score: Context memory\n",
      "   - topic_adherence/score: Topic focus\n",
      "\n",
      "üí° TIPS:\n",
      "   1. Compare multiple evaluation runs\n",
      "   2. Filter by scorer to find failures\n",
      "   3. Read judge reasoning for insights\n",
      "   4. Track metrics over time for regression detection\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë         Viewing Evaluation Results in MLflow UI              ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "üîç EXPERIMENTS VIEW:\n",
    "   Navigate to: http://localhost:5000\n",
    "   Select: \"11-agent-evaluation\" experiment\n",
    "   \n",
    "   You'll see:\n",
    "   - Session-level and Evaluation runs with scorer metrics\n",
    "   - Pass/fail rates for each scorer\n",
    "   - Detailed reasoning from LLM judges\n",
    "\n",
    "üìä EVALUATION RESULTS:\n",
    "   Each run includes:\n",
    "   - Metric values (0-1 scores or boolean)\n",
    "   - Judge reasoning explanations\n",
    "   - Input/output pairs\n",
    "   - Artifacts with detailed results\n",
    "\n",
    "üéØ KEY METRICS TO MONITOR:\n",
    "   \n",
    "   Built-in Scorers:\n",
    "   - relevance_to_query/score: Response relevance\n",
    "   - correctness/score: Factual accuracy\n",
    "   - safety/score: Safety compliance\n",
    "   - guidelines/score: Guideline adherence\n",
    "   \n",
    "   Custom Scorers:\n",
    "   - response_length_check: Length validation\n",
    "   - contains_keywords: Keyword presence\n",
    "   - no_hallucination_markers: Confidence check\n",
    "   \n",
    "   DeepEval Scorers:\n",
    "   - conversation_completeness/score: Goal achievement\n",
    "   - knowledge_retention/score: Context memory\n",
    "   - topic_adherence/score: Topic focus\n",
    "\n",
    "üí° TIPS:\n",
    "   1. Compare multiple evaluation runs\n",
    "   2. Filter by scorer to find failures\n",
    "   3. Read judge reasoning for insights\n",
    "   4. Track metrics over time for regression detection\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to evaluate agents and LLM applications using MLflow's evaluation framework.\n",
    "\n",
    "### ‚úÖ What You Learned\n",
    "\n",
    "**Built-in Scorers:**\n",
    "- `RelevanceToQuery` - Check if responses are relevant\n",
    "- `Correctness` - Verify factual accuracy\n",
    "- `Guidelines` - Enforce custom guidelines\n",
    "- `Safety` - Ensure safe outputs\n",
    "\n",
    "**Custom Scorers:**\n",
    "- Use `@scorer` decorator for custom logic\n",
    "- Access `outputs` and `expectations` in scorers\n",
    "- Return boolean or numeric scores\n",
    "\n",
    "**DeepEval Integration:**\n",
    "- `ConversationCompleteness` - Goal achievement\n",
    "- `KnowledgeRetention` - Context memory\n",
    "- `TopicAdherence` - Topic focus\n",
    "\n",
    "**Session-Level Evaluation:**\n",
    "- Tag traces with session IDs\n",
    "- Search traces by session\n",
    "- Evaluate multi-turn conversations\n",
    "\n",
    "### üîë Key Patterns\n",
    "\n",
    "```python\n",
    "# Built-in scorer\n",
    "from mlflow.genai.scorers import RelevanceToQuery\n",
    "scorer = RelevanceToQuery(model=\"openai:/gpt-4o-mini\")\n",
    "\n",
    "# Custom scorer\n",
    "from mlflow.genai import scorer\n",
    "@scorer\n",
    "def my_scorer(outputs) -> bool:\n",
    "    return condition\n",
    "\n",
    "# Run evaluation\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=session_tracew,\n",
    "    scorers=[scorer1, scorer2]\n",
    ")\n",
    "```\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "Continue to **Tutorial 1.8: Complete RAG Application** to build a full RAG system with RAGAS evaluation metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
