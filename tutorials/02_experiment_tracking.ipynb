{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1.2: Experiment Tracking for LLMs\n",
    "\n",
    "## Tracking GenAI Experiments with MLflow\n",
    "\n",
    "Welcome to the second notebook! Now that your environment is set up, you'll learn how to track LLM experiments systematically.\n",
    "\n",
    "### What You'll Learn\n",
    "- How to create and organize GenAI experiments\n",
    "- Log LLM parameters (model, temperature, max_tokens, etc.)\n",
    "- Track important metrics (latency, token usage, cost)\n",
    "- Store artifacts (prompts, responses, model configs)\n",
    "- Compare different LLM configurations\n",
    "- Best practices for experiment organization\n",
    "\n",
    "### Prerequisites\n",
    "- Completed Notebook 1.1 (Setup)\n",
    "- MLflow UI running (optional but recommended)\n",
    "\n",
    "### Estimated Time: 25-30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Environment Setup\n",
    "\n",
    "Let's load our environment and imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_databricks: True\n",
      "‚úÖ Environment configured successfully\n",
      "   MLflow Tracking URI: http://localhost:5000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import mlflow\n",
    "from utils import get_databricks_client, get_openai_client\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "use_databricks = os.getenv(\"USE_DATABRICKS_CLIENT\") == \"True\"\n",
    "print(f\"use_databricks: {use_databricks}\")\n",
    "\n",
    "# Verify which client to use\n",
    "if use_databricks:\n",
    "    client = get_databricks_client()\n",
    "else:\n",
    "    client = get_openai_client()\n",
    "\n",
    "# Verify OpenAI key\n",
    "if not use_databricks and not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY not found. Please check your .env file.\")\n",
    "\n",
    "print(\"‚úÖ Environment configured successfully\")\n",
    "print(f\"   MLflow Tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Understanding Experiment Tracking\n",
    "\n",
    "### What is Experiment Tracking?\n",
    "\n",
    "Experiment tracking captures the inputs, outputs, and context of your LLM experiments:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ              EXPERIMENT                          ‚îÇ\n",
    "‚îÇ  Name: \"sentiment-analysis\"                      ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                  ‚îÇ\n",
    "‚îÇ  RUN 1: gpt-5-2, temp=1.0                        ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Parameters: {model, temperature, ...}        ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Metrics: {accuracy, latency, cost}           ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ Artifacts: {prompt.txt, config.json}         ‚îÇ\n",
    "‚îÇ                                                  ‚îÇ\n",
    "‚îÇ  RUN 2: gpt-5-2, temp=1.5                        ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Parameters: {model, temperature, ...}        ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Metrics: {accuracy, latency, cost}           ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ Artifacts: {prompt.txt, config.json}         ‚îÇ\n",
    "‚îÇ                                                  ‚îÇ\n",
    "‚îÇ  RUN 3: gpt-5-2, temp=2.0                        ‚îÇ\n",
    "‚îÇ  ...                                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Parameters**: Configuration values (model name, temperature, max_tokens)\n",
    "- **Metrics**: Numerical measurements (accuracy, latency, token count)\n",
    "- **Artifacts**: Files (prompts, responses, model configs)\n",
    "- **Tags**: Metadata for organizing and filtering runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Your First Tracked LLM Call\n",
    "\n",
    "Let's start with a simple example: making an LLM call and tracking everything about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/27 19:47:58 INFO mlflow.tracking.fluent: Experiment with name '01-basic-llm-calls' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Experiment: 01-basic-llm-calls\n",
      "   View in UI: http://localhost:5000\n"
     ]
    }
   ],
   "source": [
    "# Create an experiment\n",
    "experiment_name = \"01-basic-llm-calls\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"üìä Experiment: {experiment_name}\")\n",
    "print(\"   View in UI: http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RUN COMPLETED\n",
      "============================================================\n",
      "\n",
      "üìù Prompt: Explain MLflow in 2-3 sentences.\n",
      "\n",
      "ü§ñ Response: MLflow is an open-source platform for managing the end-to-end machine learning lifecycle, including experiment tracking, packaging models, and deploying them. It provides four main components‚ÄîTracking (log and compare experiments), Projects (reproducible runs), Models (standardized model format and model registry), and Model Serving/Registry‚Äîto help teams reproduce, share, and deploy ML work.\n",
      "\n",
      "üìä Metrics:\n",
      "   Latency: 2.48s\n",
      "   Tokens: 101 (prompt: 16, completion: 85)\n",
      "\n",
      "üîó Run ID: d09489850e854a1fbfaf2198e717f072\n",
      "   View in UI: http://localhost:5000/#/experiments/2/runs/d09489850e854a1fbfaf2198e717f072\n",
      "üèÉ View run first-tracked-call at: http://localhost:5000/#/experiments/2/runs/d09489850e854a1fbfaf2198e717f072\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-3781a841f8c17fb46c005529ca1aa18d&amp;experiment_id=2&amp;version=3.9.0rc0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-3781a841f8c17fb46c005529ca1aa18d)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a tracked LLM call\n",
    "with mlflow.start_run(run_name=\"first-tracked-call\") as run:\n",
    "    \n",
    "    # 1. Define inputs\n",
    "    if use_databricks:\n",
    "        model_name = \"databricks-gpt-5-mini\"\n",
    "    else:\n",
    "        model_name = \"gpt-5-mini\"\n",
    "    \n",
    "    # some default values\n",
    "    temperature = 1.0\n",
    "    max_tokens = 100\n",
    "    prompt = \"Explain MLflow in 2-3 sentences.\"\n",
    "    \n",
    "    # # 2. Log parameters\n",
    "    mlflow.log_param(\"model\", model_name)\n",
    "    mlflow.log_param(\"temperature\", temperature)\n",
    "    mlflow.log_param(\"max_tokens\", max_tokens)\n",
    "    \n",
    "    # 3. Make the LLM call (with timing)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    \n",
    "    latency = time.time() - start_time\n",
    "    \n",
    "    # 4. Extract response details\n",
    "    answer = response.choices[0].message.content\n",
    "    prompt_tokens = response.usage.prompt_tokens\n",
    "    completion_tokens = response.usage.completion_tokens\n",
    "    total_tokens = response.usage.total_tokens\n",
    "    \n",
    "    # # 5. Log metrics\n",
    "    mlflow.log_metric(\"latency_seconds\", latency)\n",
    "    mlflow.log_metric(\"prompt_tokens\", prompt_tokens)\n",
    "    mlflow.log_metric(\"completion_tokens\", completion_tokens)\n",
    "    mlflow.log_metric(\"total_tokens\", total_tokens)\n",
    "    mlflow.log_metric(\"response_length_chars\", len(answer))\n",
    "    \n",
    "    # # 6. Log artifacts\n",
    "    mlflow.log_text(prompt, \"prompt.txt\")\n",
    "    mlflow.log_text(answer, \"response.txt\")\n",
    "    \n",
    "    # # 7. Log additional metadata as tags\n",
    "    mlflow.set_tag(\"task\", \"explanation\")\n",
    "    mlflow.set_tag(\"framework\", \"openai\")\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RUN COMPLETED\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nüìù Prompt: {prompt}\")\n",
    "    print(f\"\\nü§ñ Response: {answer}\")\n",
    "    print(\"\\nüìä Metrics:\")\n",
    "    print(f\"   Latency: {latency:.2f}s\")\n",
    "    print(f\"   Tokens: {total_tokens} (prompt: {prompt_tokens}, completion: {completion_tokens})\")\n",
    "    print(f\"\\nüîó Run ID: {run.info.run_id}\")\n",
    "    print(f\"   View in UI: http://localhost:5000/#/experiments/{run.info.experiment_id}/runs/{run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ What Just Happened?\n",
    "\n",
    "1. **Created a Run**: MLflow created a unique run with ID\n",
    "2. **Logged Parameters**: Stored configuration (model, temperature, max_tokens)\n",
    "3. **Logged Metrics**: Tracked performance (latency, tokens)\n",
    "4. **Logged Artifacts**: Saved prompt and response as files\n",
    "5. **Added Tags**: Metadata for organizing runs\n",
    "\n",
    "All this data is now stored in SQLite `mlflow.db` and visible in the UI!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Comparing Multiple Configurations\n",
    "\n",
    "Let's run experiments with different LLM configurations to see how they compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper function defined\n"
     ]
    }
   ],
   "source": [
    "# Helper function for tracked LLM calls\n",
    "def tracked_llm_call(prompt, model=\"gpt-5-mini\", temperature=1.0, max_tokens=100, run_name=None):\n",
    "    \"\"\"\n",
    "    Make an LLM call with full MLflow tracking.\n",
    "    \n",
    "    Returns: (response_text, metrics_dict)\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=run_name, nested=True):\n",
    "        # Log parameters\n",
    "        mlflow.log_params({\n",
    "            \"model\": model,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"prompt_length\": len(prompt)\n",
    "        })\n",
    "        \n",
    "        # Make call with timing\n",
    "        start_time = time.time()\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        # Extract response\n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            \"latency_seconds\": latency,\n",
    "            \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "            \"completion_tokens\": response.usage.completion_tokens,\n",
    "            \"total_tokens\": response.usage.total_tokens,\n",
    "            \"response_length\": len(answer)\n",
    "        }\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metrics(metrics)\n",
    "        \n",
    "        # Log artifacts\n",
    "        mlflow.log_text(prompt, \"prompt.txt\")\n",
    "        mlflow.log_text(answer, \"response.txt\")\n",
    "        \n",
    "        return answer, metrics\n",
    "\n",
    "print(\"‚úÖ Helper function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/27 19:53:06 INFO mlflow.tracking.fluent: Experiment with name '02-temperature-comparison' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ Running experiments with different temperatures...\n",
      "\n",
      "Testing temperature=1.0...\n",
      "üèÉ View run temp_1.0 at: http://localhost:5000/#/experiments/3/runs/c2dc809986ae46a6bfa886c955f46b45\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/3\n",
      "  ‚úì Completed in 1.36s\n",
      "\n",
      "Testing temperature=1.5...\n",
      "üèÉ View run temp_1.5 at: http://localhost:5000/#/experiments/3/runs/7443af6e9e5a475da8a0f9884c168fb5\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/3\n",
      "  ‚úì Completed in 0.94s\n",
      "\n",
      "Testing temperature=2.0...\n",
      "üèÉ View run temp_2.0 at: http://localhost:5000/#/experiments/3/runs/cb114dd8a56844ab824b647daf7c0544\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/3\n",
      "  ‚úì Completed in 1.23s\n",
      "\n",
      "\n",
      "============================================================\n",
      "RESULTS COMPARISON\n",
      "============================================================\n",
      "\n",
      "Temperature: 1.0\n",
      "Response: **‚ÄúSee every run. Trust every model.‚Äù**\n",
      "Tokens: 34\n",
      "------------------------------------------------------------\n",
      "\n",
      "Temperature: 1.5\n",
      "Response: **‚ÄúSee every run. Trust every model.‚Äù**\n",
      "Tokens: 34\n",
      "------------------------------------------------------------\n",
      "\n",
      "Temperature: 2.0\n",
      "Response: ‚ÄúSee every experiment, trace every model‚ÄîAI observability powered by MLflow.‚Äù\n",
      "Tokens: 40\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-f5b3d9347986940c2859868cd2306c16&amp;experiment_id=3&amp;trace_id=tr-00b06b19d07e8840cf4fb95a13da2ddf&amp;experiment_id=3&amp;trace_id=tr-513ed1a116249953dc5616d3afc26ad1&amp;experiment_id=3&amp;version=3.9.0rc0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-f5b3d9347986940c2859868cd2306c16), Trace(trace_id=tr-00b06b19d07e8840cf4fb95a13da2ddf), Trace(trace_id=tr-513ed1a116249953dc5616d3afc26ad1)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a new experiment for comparison\n",
    "mlflow.set_experiment(\"02-temperature-comparison\")\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"Write a creative tagline for an AI observability with MLflow platform.\"\n",
    "\n",
    "# Test different temperatures\n",
    "temperatures = [1.0, 1.5, 2.0]\n",
    "\n",
    "print(\"\\nüî¨ Running experiments with different temperatures...\\n\")\n",
    "\n",
    "results = []\n",
    "for temp in temperatures:\n",
    "    print(f\"Testing temperature={temp}...\")\n",
    "    response, metrics = tracked_llm_call(\n",
    "        prompt=test_prompt,\n",
    "        model=\"databricks-gpt-5-2\" if use_databricks else \"gpt-5-2\",\n",
    "        temperature=temp,\n",
    "        max_tokens=50,\n",
    "        run_name=f\"temp_{temp}\"\n",
    "    )\n",
    "    results.append((temp, response, metrics))\n",
    "    print(f\"  ‚úì Completed in {metrics['latency_seconds']:.2f}s\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS COMPARISON\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for temp, response, metrics in results:\n",
    "    print(f\"Temperature: {temp}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(f\"Tokens: {metrics['total_tokens']}\")\n",
    "    print(\"-\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Analysis\n",
    "\n",
    "Notice how temperature affects:\n",
    "- **Creativity**: Higher temperature = more creative responses\n",
    "- **Consistency**: Lower temperature = more deterministic\n",
    "- **Token usage**: May vary with creativity level\n",
    "\n",
    "**üí° Go to the MLflow UI to visualize these differences!**\n",
    "1. Select the \"02-temperature-comparison\" experiment\n",
    "2. Select all three runs\n",
    "3. Click \"Compare\" button\n",
    "4. View side-by-side metrics and artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Tracking Cost Estimates\n",
    "\n",
    "Let's add cost tracking to our experiments. This is crucial for production applications!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cost calculation function defined\n"
     ]
    }
   ],
   "source": [
    "# OpenAI pricing (as of 2026 - verify current rates)\n",
    "PRICING = {\n",
    "    \"gpt-5-2\": {\n",
    "        \"input\": 1.75/ 1_000_000,   # per token\n",
    "        \"output\": 14.00 / 1_000_000   # per token\n",
    "    },\n",
    "    \"gpt-5-2-mini\": {\n",
    "        \"input\": 0.250 / 1_000_000,   # per token\n",
    "        \"output\": 2.000 / 1_000_000   # per token\n",
    "    },\n",
    "    \"databricks-gpt-5-2\": {\n",
    "        \"input\": 1.75/ 1_000_000,   # per token\n",
    "        \"output\": 14.00 / 1_000_000   # per token\n",
    "    },\n",
    "    \"databricks-gpt-5-mini\": {\n",
    "        \"input\": 0.250 / 1_000_000,   # per token\n",
    "        \"output\": 2.000 / 1_000_000   # per token\n",
    "    },\n",
    "}\n",
    "\n",
    "def calculate_cost(model, prompt_tokens, completion_tokens):\n",
    "    \"\"\"\n",
    "    Calculate estimated cost for an LLM call.\n",
    "    \"\"\"\n",
    "    if model not in PRICING:\n",
    "        return 0.0\n",
    "    \n",
    "    input_cost = prompt_tokens * PRICING[model][\"input\"]\n",
    "    output_cost = completion_tokens * PRICING[model][\"output\"]\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    return total_cost\n",
    "\n",
    "print(\"‚úÖ Cost calculation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced tracking function with cost estimation defined\n"
     ]
    }
   ],
   "source": [
    "# Enhanced helper with cost tracking\n",
    "def tracked_llm_call_with_cost(prompt, model=\"gpt-5-mini\", temperature=0.7, max_tokens=100, run_name=None):\n",
    "    \"\"\"\n",
    "    Make an LLM call with full tracking including cost estimation.\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Log parameters\n",
    "        mlflow.log_params({\n",
    "            \"model\": model,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"prompt_length\": len(prompt)\n",
    "        })\n",
    "        \n",
    "        # Make call\n",
    "        start_time = time.time()\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        # Extract details\n",
    "        answer = response.choices[0].message.content\n",
    "        prompt_tokens = response.usage.prompt_tokens\n",
    "        completion_tokens = response.usage.completion_tokens\n",
    "        total_tokens = response.usage.total_tokens\n",
    "        \n",
    "        # Calculate cost\n",
    "        cost = calculate_cost(model, prompt_tokens, completion_tokens)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metrics({\n",
    "            \"latency_seconds\": latency,\n",
    "            \"prompt_tokens\": prompt_tokens,\n",
    "            \"completion_tokens\": completion_tokens,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"estimated_cost_usd\": cost,\n",
    "            \"cost_per_1k_tokens\": (cost / total_tokens * 1000) if total_tokens > 0 else 0\n",
    "        })\n",
    "        \n",
    "        # Log artifacts\n",
    "        mlflow.log_text(prompt, \"prompt.txt\")\n",
    "        mlflow.log_text(answer, \"response.txt\")\n",
    "        \n",
    "        # Cost summary artifact\n",
    "        cost_summary = f\"\"\"\n",
    "Cost Breakdown\n",
    "==============\n",
    "Model: {model}\n",
    "Input tokens: {prompt_tokens} @ ${PRICING[model]['input']*1_000_000:.2f}/M\n",
    "Output tokens: {completion_tokens} @ ${PRICING[model]['output']*1_000_000:.2f}/M\n",
    "Total cost: ${cost:.6f}\n",
    "\"\"\"\n",
    "        mlflow.log_text(cost_summary, \"cost_breakdown.txt\")\n",
    "        \n",
    "        return answer, cost\n",
    "\n",
    "print(\"‚úÖ Enhanced tracking function with cost estimation defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/27 19:37:46 INFO mlflow.tracking.fluent: Experiment with name '03-model-cost-comparison' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí∞ Comparing costs across models...\n",
      "\n",
      "Testing databricks-gpt-5-2...\n",
      "üèÉ View run model_databricks-gpt-5-2 at: http://localhost:5000/#/experiments/3/runs/2064981454bb4ccf91c4a42df5ddeea9\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/3\n",
      "  Cost: $0.001547\n",
      "  Response length: 560 chars\n",
      "\n",
      "Testing databricks-gpt-5-mini...\n",
      "üèÉ View run model_databricks-gpt-5-mini at: http://localhost:5000/#/experiments/3/runs/fd19f7a1a67f4656867b1e11f6e7e174\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/3\n",
      "  Cost: $0.000211\n",
      "  Response length: 535 chars\n",
      "\n",
      "‚úÖ Cost comparison complete!\n",
      "   View detailed breakdown in MLflow UI\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-eeffa3a66160c3e73a8328f52df8ff7b&amp;experiment_id=3&amp;trace_id=tr-861b670e9dacf7ff5bdd72b603ad1d27&amp;experiment_id=3&amp;version=3.9.0rc0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-eeffa3a66160c3e73a8328f52df8ff7b), Trace(trace_id=tr-861b670e9dacf7ff5bdd72b603ad1d27)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare costs across different models\n",
    "mlflow.set_experiment(\"03-model-cost-comparison\")\n",
    "\n",
    "prompt = \"Summarize the benefits of experiment tracking in 3 bullet points.\"\n",
    "\n",
    "models_to_test = [\"databricks-gpt-5-2\", \"databricks-gpt-5-mini\"] if use_databricks else [\"gpt-5-mini\", \"gpt-5-2\"]\n",
    "\n",
    "print(\"\\nüí∞ Comparing costs across models...\\n\")\n",
    "\n",
    "for model in models_to_test:\n",
    "    print(f\"Testing {model}...\")\n",
    "    response, cost = tracked_llm_call_with_cost(\n",
    "        prompt=prompt,\n",
    "        model=model,\n",
    "        temperature=1.0,\n",
    "        max_tokens=150,\n",
    "        run_name=f\"model_{model}\"\n",
    "    )\n",
    "    print(f\"  Cost: ${cost:.6f}\")\n",
    "    print(f\"  Response length: {len(response)} chars\\n\")\n",
    "\n",
    "print(\"‚úÖ Cost comparison complete!\")\n",
    "print(\"   View detailed breakdown in MLflow UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Cost Analysis Insights\n",
    "\n",
    "By tracking costs, you can:\n",
    "1. **Budget effectively** for production deployments\n",
    "2. **Optimize model selection** (GPT-4o-mini vs GPT-4o)\n",
    "3. **Identify expensive prompts** that need optimization\n",
    "4. **Track spending trends** over time\n",
    "\n",
    "**Pro Tip**: Set up cost alerts in production based on these metrics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Organizing Experiments with Tags and Metadata\n",
    "\n",
    "As your experiments grow, organization becomes critical. Let's learn how to use tags effectively. Effectively, this allows you to search and group by runs with tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/27 19:55:17 INFO mlflow.tracking.fluent: Experiment with name '04-production-candidate-testing' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üè∑Ô∏è  Running experiments with comprehensive tagging...\n",
      "\n",
      "‚úì baseline - 3.17s\n",
      "üèÉ View run baseline at: http://localhost:5000/#/experiments/4/runs/ae24e0c3a74e4b8ba8c9733b43233a26\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/4\n",
      "‚úì creative - 5.48s\n",
      "üèÉ View run creative at: http://localhost:5000/#/experiments/4/runs/cb114792e3a648758c9b4bb8bb461258\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/4\n",
      "‚úì precise - 1.71s\n",
      "üèÉ View run precise at: http://localhost:5000/#/experiments/4/runs/6638470f20d74afda86278dd88995e68\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/4\n",
      "\n",
      "‚úÖ All runs completed with comprehensive tagging!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-32b2e240684805299e2ea14205c578c9&amp;experiment_id=4&amp;trace_id=tr-ca7b471e535e85a85623763ef9a87507&amp;experiment_id=4&amp;trace_id=tr-fcb1dd03129d832731413a8828e9eb5f&amp;experiment_id=4&amp;version=3.9.0rc0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-32b2e240684805299e2ea14205c578c9), Trace(trace_id=tr-ca7b471e535e85a85623763ef9a87507), Trace(trace_id=tr-fcb1dd03129d832731413a8828e9eb5f)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example: Systematic experiment with rich metadata\n",
    "mlflow.set_experiment(\"04-production-candidate-testing\")\n",
    "\n",
    "# Test configurations\n",
    "open_configs = [ \n",
    "    {\n",
    "        \"name\": \"baseline\",\n",
    "        \"model\": \"gpt-5-mini\",\n",
    "        \"temperature\": 1.0,\n",
    "        \"system_prompt\": \"You are a helpful assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"creative\",\n",
    "        \"model\": \"gpt-5-2\",\n",
    "        \"temperature\": 1.5,\n",
    "        \"system_prompt\": \"You are a creative writing assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"precise\",\n",
    "        \"model\": \"gemini-2-5-flash\",\n",
    "        \"temperature\": 1.0,\n",
    "        \"system_prompt\": \"You are a precise, technical assistant.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Databricks hosted foundational models if you want to test them\n",
    "databricks_config = [\n",
    "\n",
    "    {\n",
    "        \"name\": \"baseline\",\n",
    "        \"model\": \"databricks-gpt-5-mini\",\n",
    "        \"temperature\": 1.0,\n",
    "        \"system_prompt\": \"You are a helpful assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"creative\",\n",
    "        \"model\": \"databricks-gpt-5-2\",\n",
    "        \"temperature\": 1.5,\n",
    "        \"system_prompt\": \"You are a creative writing assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"precise\",\n",
    "        \"model\": \"databricks-gemini-2-5-flash\",\n",
    "        \"temperature\": 1.0,\n",
    "        \"system_prompt\": \"You are a precise, technical assistant.\"\n",
    "    }\n",
    "]\n",
    "model_configs = databricks_config if use_databricks else open_configs\n",
    "test_prompt = \"Explain the concept of LLM temperature.\"\n",
    "\n",
    "print(\"\\nüè∑Ô∏è  Running experiments with comprehensive tagging...\\n\")\n",
    "\n",
    "for config in model_configs:\n",
    "    with mlflow.start_run(run_name=config[\"name\"]):\n",
    "        # Log parameters\n",
    "        mlflow.log_params({\n",
    "            \"model\": config[\"model\"],\n",
    "            \"temperature\": config[\"temperature\"],\n",
    "            \"system_prompt\": config[\"system_prompt\"]\n",
    "        })\n",
    "        \n",
    "        # Make call\n",
    "        start_time = time.time()\n",
    "        response = client.chat.completions.create(\n",
    "            model=config[\"model\"],\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": config[\"system_prompt\"]},\n",
    "                {\"role\": \"user\", \"content\": test_prompt}\n",
    "            ],\n",
    "            temperature=config[\"temperature\"],\n",
    "            max_tokens=200\n",
    "        )\n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metrics({\n",
    "            \"latency_seconds\": latency,\n",
    "            \"total_tokens\": response.usage.total_tokens,\n",
    "            \"estimated_cost_usd\": calculate_cost(\n",
    "                config[\"model\"],\n",
    "                response.usage.prompt_tokens,\n",
    "                response.usage.completion_tokens\n",
    "            )\n",
    "        })\n",
    "        \n",
    "        # Rich tagging\n",
    "        mlflow.set_tags({\n",
    "            \"config_name\": config[\"name\"],\n",
    "            \"task\": \"explanation\",\n",
    "            \"stage\": \"testing\",\n",
    "            \"team\": \"ai-research\",\n",
    "            \"version\": \"v1.0\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"production_candidate\": \"true\" if config[\"name\"] == \"baseline\" else \"false\"\n",
    "        })\n",
    "        \n",
    "        # Log artifacts\n",
    "        mlflow.log_text(test_prompt, \"prompt.txt\")\n",
    "        mlflow.log_text(answer, \"response.txt\")\n",
    "        \n",
    "        # Save full config\n",
    "        mlflow.log_dict(config, \"config.json\")\n",
    "        \n",
    "        print(f\"‚úì {config['name']} - {latency:.2f}s\")\n",
    "\n",
    "print(\"\\n‚úÖ All runs completed with comprehensive tagging!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üè∑Ô∏è Tagging Best Practices\n",
    "\n",
    "Use tags for:\n",
    "1. **Environment**: `stage: development/testing/production`\n",
    "2. **Ownership**: `team: ai-research`, `owner: jules`\n",
    "3. **Purpose**: `task: summarization`, `use_case: customer-support`\n",
    "4. **Status**: `production_candidate: true`, `approved: false`\n",
    "5. **Version**: `version: v1.0`, `prompt_version: v2.1`\n",
    "\n",
    "**üí° You can filter and search runs by tags in the MLflow UI!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Advanced - Parent-Child Runs\n",
    "\n",
    "For complex workflows with multiple LLM calls, use nested runs to maintain hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/27 19:55:33 INFO mlflow.tracking.fluent: Experiment with name '05-multi-step-workflow' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run step-1-preprocess at: http://localhost:5000/#/experiments/5/runs/24669fabcd574a1499441a582eb85a3f\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/5\n",
      "üèÉ View run step-2-generate at: http://localhost:5000/#/experiments/5/runs/7de9f2e57cc14fdbb3ec9f1487ccc807\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/5\n",
      "üèÉ View run step-3-quality-check at: http://localhost:5000/#/experiments/5/runs/80ce3f129ce9498fa9724ccdf930da64\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/5\n",
      "\n",
      "‚úÖ Multi-step pipeline completed!\n",
      "   Parent Run ID: 3b0c9b18189042d1999abf726414e717\n",
      "    View hierarchy in UI\n",
      "üèÉ View run question-answering-pipeline at: http://localhost:5000/#/experiments/5/runs/3b0c9b18189042d1999abf726414e717\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-779c4f2d4208984c9c6cb2275e78bbbe&amp;experiment_id=5&amp;trace_id=tr-33123c622e21595582f36a9fa389252d&amp;experiment_id=5&amp;trace_id=tr-4b4bcfa03753cfc0d3491308a8a0d79d&amp;experiment_id=5&amp;version=3.9.0rc0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-779c4f2d4208984c9c6cb2275e78bbbe), Trace(trace_id=tr-33123c622e21595582f36a9fa389252d), Trace(trace_id=tr-4b4bcfa03753cfc0d3491308a8a0d79d)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example: Multi-step workflow with nested runs\n",
    "mlflow.set_experiment(\"05-multi-step-workflow\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"question-answering-pipeline\") as parent_run:\n",
    "    \n",
    "    # Tag parent run\n",
    "    mlflow.set_tag(\"workflow\", \"qa-pipeline\")\n",
    "    mlflow.set_tag(\"num_steps\", \"3\")\n",
    "    \n",
    "    user_question = \"What is machine learning?\"\n",
    "    \n",
    "    # Step 1: Question preprocessing (nested run)\n",
    "    with mlflow.start_run(run_name=\"step-1-preprocess\", nested=True):\n",
    "        mlflow.set_tag(\"step\", \"preprocessing\")\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"databricks-gpt-5-mini\" if use_databricks else \"gpt-5-mini\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Rephrase this question to be more specific: {user_question}\"\n",
    "            }],\n",
    "            temperature=1.0,\n",
    "            max_tokens=50\n",
    "        )\n",
    "        \n",
    "        refined_question = response.choices[0].message.content\n",
    "        mlflow.log_param(\"original_question\", user_question)\n",
    "        mlflow.log_param(\"refined_question\", refined_question)\n",
    "        mlflow.log_metric(\"tokens_used\", response.usage.total_tokens)\n",
    "    \n",
    "    # Step 2: Answer generation (nested run)\n",
    "    with mlflow.start_run(run_name=\"step-2-generate\", nested=True):\n",
    "        mlflow.set_tag(\"step\", \"generation\")\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"databricks-gpt-5-mini\" if use_databricks else \"gpt-5-mini\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Provide a detailed answer: {refined_question}\"\n",
    "            }],\n",
    "            temperature=1.0,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content\n",
    "        mlflow.log_text(answer, \"answer.txt\")\n",
    "        mlflow.log_metric(\"tokens_used\", response.usage.total_tokens)\n",
    "        mlflow.log_metric(\"answer_length\", len(answer))\n",
    "    \n",
    "    # Step 3: Quality check (nested run)\n",
    "    with mlflow.start_run(run_name=\"step-3-quality-check\", nested=True):\n",
    "        mlflow.set_tag(\"step\", \"quality-check\")\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"databricks-gpt-5-mini\" if use_databricks else \"gpt-5-mini\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Rate the quality of this answer (1-10): {answer}\"\n",
    "            }],\n",
    "            temperature=1.0,\n",
    "            max_tokens=10\n",
    "        )\n",
    "        \n",
    "        quality_rating = response.choices[0].message.content\n",
    "        mlflow.log_param(\"quality_rating\", quality_rating)\n",
    "        mlflow.log_metric(\"tokens_used\", response.usage.total_tokens)\n",
    "    \n",
    "    # Log parent run summary\n",
    "    mlflow.log_text(f\"\"\"\n",
    "Pipeline Summary\n",
    "================\n",
    "Original Question: {user_question}\n",
    "Refined Question: {refined_question}\n",
    "Answer Length: {len(answer)} chars\n",
    "Quality Rating: {quality_rating}\n",
    "\"\"\", \"pipeline_summary.txt\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Multi-step pipeline completed!\")\n",
    "    print(f\"   Parent Run ID: {parent_run.info.run_id}\")\n",
    "    print(\"    View hierarchy in UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üå≥ Parent-Child Run Benefits\n",
    "\n",
    "```\n",
    "Parent Run: question-answering-pipeline\n",
    "‚îú‚îÄ‚îÄ Child Run 1: step-1-preprocess\n",
    "‚îú‚îÄ‚îÄ Child Run 2: step-2-generate\n",
    "‚îî‚îÄ‚îÄ Child Run 3: step-3-quality-check\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- Logical organization of complex workflows\n",
    "- Individual step metrics without losing the big picture\n",
    "- Easy to debug specific steps\n",
    "- Aggregate metrics across all steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Querying Experiments Programmatically\n",
    "\n",
    "Let's learn how to retrieve and analyze experiment data using the MLflow API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Experiment: 02-temperature-comparison\n",
      "   ID: 3\n",
      "   Artifact Location: mlflow-artifacts:/3\n",
      "\n",
      "   Found 3 runs:\n",
      "\n",
      "============================================================\n",
      "\n",
      "   Run: temp_1.5\n",
      "   Parameters:\n",
      "      model: databricks-gpt-5-2\n",
      "      temperature: 1.5\n",
      "      max_tokens: 50\n",
      "      prompt_length: 70\n",
      "   Metrics:\n",
      "      latency_seconds: 0.9443578720092773\n",
      "      prompt_tokens: 20.0\n",
      "      completion_tokens: 14.0\n",
      "      total_tokens: 34.0\n",
      "      response_length: 39.0\n",
      "\n",
      "   Run: temp_2.0\n",
      "   Parameters:\n",
      "      model: databricks-gpt-5-2\n",
      "      temperature: 2.0\n",
      "      max_tokens: 50\n",
      "      prompt_length: 70\n",
      "   Metrics:\n",
      "      latency_seconds: 1.2332289218902588\n",
      "      prompt_tokens: 20.0\n",
      "      completion_tokens: 20.0\n",
      "      total_tokens: 40.0\n",
      "      response_length: 77.0\n",
      "\n",
      "   Run: temp_1.0\n",
      "   Parameters:\n",
      "      model: databricks-gpt-5-2\n",
      "      temperature: 1.0\n",
      "      max_tokens: 50\n",
      "      prompt_length: 70\n",
      "   Metrics:\n",
      "      latency_seconds: 1.3623478412628174\n",
      "      prompt_tokens: 20.0\n",
      "      completion_tokens: 14.0\n",
      "      total_tokens: 34.0\n",
      "      response_length: 39.0\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client_mlflow = MlflowClient()\n",
    "\n",
    "# Get experiment by name\n",
    "experiment = client_mlflow.get_experiment_by_name(\"02-temperature-comparison\")\n",
    "\n",
    "if experiment:\n",
    "    print(f\"\\nüìä Experiment: {experiment.name}\")\n",
    "    print(f\"   ID: {experiment.experiment_id}\")\n",
    "    print(f\"   Artifact Location: {experiment.artifact_location}\")\n",
    "    \n",
    "    # Search runs in this experiment\n",
    "    runs = client_mlflow.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        order_by=[\"metrics.latency_seconds ASC\"],\n",
    "        max_results=5\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n   Found {len(runs)} runs:\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    for run in runs:\n",
    "        print(f\"\\n   Run: {run.info.run_name}\")\n",
    "        print(\"   Parameters:\")\n",
    "        for key, value in run.data.params.items():\n",
    "            print(f\"      {key}: {value}\")\n",
    "        print(\"   Metrics:\")\n",
    "        for key, value in run.data.metrics.items():\n",
    "            print(f\"      {key}: {value}\")\n",
    "else:\n",
    "    print(\"Experiment not found. Make sure you ran the temperature comparison section.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Fastest Run:\n",
      "   Name: temp_1.5\n",
      "   Latency: 0.944s\n",
      "   Temperature: 1.5\n",
      "   Run ID: 7443af6e9e5a475da8a0f9884c168fb5\n"
     ]
    }
   ],
   "source": [
    "# Find the best run based on a metric\n",
    "if experiment:\n",
    "    # Find fastest run\n",
    "    fastest_run = client_mlflow.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        order_by=[\"metrics.latency_seconds ASC\"],\n",
    "        max_results=1\n",
    "    )[0]\n",
    "    \n",
    "    print(\"\\nüèÜ Fastest Run:\")\n",
    "    print(f\"   Name: {fastest_run.info.run_name}\")\n",
    "    print(f\"   Latency: {fastest_run.data.metrics['latency_seconds']:.3f}s\")\n",
    "    print(f\"   Temperature: {fastest_run.data.params.get('temperature', 'N/A')}\")\n",
    "    print(f\"   Run ID: {fastest_run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Advanced Queries\n",
    "\n",
    "The MLflow API supports powerful filtering:\n",
    "\n",
    "```python\n",
    "# Filter by metric threshold\n",
    "fast_runs = client_mlflow.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    filter_string=\"metrics.latency_seconds < 1.0\"\n",
    ")\n",
    "\n",
    "# Filter by parameter\n",
    "gpt4_runs = client_mlflow.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    filter_string=\"params.model = 'gpt-4o'\"\n",
    ")\n",
    "\n",
    "# Filter by tag\n",
    "prod_candidates = client_mlflow.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    filter_string=\"tags.production_candidate = 'true'\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. ‚úÖ Core concepts of experiment tracking (parameters, metrics, artifacts)\n",
    "2. ‚úÖ How to log LLM calls with full context\n",
    "3. ‚úÖ Comparing multiple model configurations systematically\n",
    "4. ‚úÖ Tracking costs for budget management\n",
    "5. ‚úÖ Organizing experiments with tags and metadata\n",
    "6. ‚úÖ Using parent-child runs for complex workflows\n",
    "7. ‚úÖ Querying experiment data programmatically\n",
    "\n",
    "### Best Practices Recap\n",
    "\n",
    "- ‚úÖ **Be Consistent**: Use the same parameter/metric names across runs\n",
    "- ‚úÖ **Tag Everything**: Make runs searchable with meaningful tags\n",
    "- ‚úÖ **Track Costs**: Essential for production budgeting\n",
    "- ‚úÖ **Use Nested Runs**: For multi-step workflows\n",
    "- ‚úÖ **Name Runs Meaningfully**: `baseline-v1`, `high-temp-creative`\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Ready to dive deep into observability?\n",
    "\n",
    "**üìì Notebook 1.3: Introduction to Tracing**\n",
    "- Learn automatic tracing with autologging\n",
    "- Understand the trace data model\n",
    "- Visualize LLM execution flows\n",
    "- Integrate with multiple frameworks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
